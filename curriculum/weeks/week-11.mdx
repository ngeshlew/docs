---
title: "Week 11: Advanced Techniques & Innovation"
description: "Explore cutting-edge AI techniques and their UX implications"
---

# Week 11: Advanced Techniques & Innovation

## Why This Week Matters for Designers

This week explores cutting-edge AI techniques and their implications for user experience design. You'll learn about emerging AI capabilities and how they might transform the way we design digital experiences.

**Designer Lens:** Focus on how these advanced techniques can enhance user experiences rather than just the technical implementation.

## Weekly Overview

**Theme:** Advanced Techniques & Innovation  
**Time Investment:** 4 sessions × 30 minutes = 2 hours  
**Key Outcome:** You'll understand emerging AI techniques and their potential impact on UX design.

## Sessions

### Session 41: Advanced Prompting Techniques (30 min)

**What You'll Learn:**
- Sophisticated prompting methods for better AI interactions
- Creating more intelligent and helpful AI experiences
- Balancing complexity with usability

**Materials:**
- [Advanced Prompting](/modules/prompting-advanced/index)

**Activity:** Design advanced prompting interface

**Deliverable:** `week11-session41-advanced-prompting.md` - Your advanced prompting design

<Tip>
Advanced techniques should serve user needs, not just demonstrate technical capability.
</Tip>

### Session 42: Graph Prompting & Relationships (30 min)

**What You'll Learn:**
- Designing for complex AI relationships
- Creating knowledge graph interfaces
- Helping users understand AI connections

**Materials:**
- [Graph Prompting](/modules/prompting-techniques/graph-prompting)

**Activity:** Design graph-based AI interface

**Deliverable:** `week11-session42-graph-interface.md` - Your graph interface design

<Info>
Graph-based AI can help users understand complex relationships and make better decisions.
</Info>

### Session 43: Reflexion & Self-Correction (30 min)

**What You'll Learn:**
- Designing for AI self-improvement
- Creating learning AI experiences
- Building user trust in adaptive systems

**Materials:**
- [Reflexion](/modules/prompting-techniques/reflexion)

**Activity:** Design self-correcting AI feature

**Deliverable:** `week11-session43-self-correction.md` - Your self-correction design

<Warning>
Self-correcting AI must be transparent about its learning process to maintain user trust.
</Warning>

### Session 44: Program-Aided Language Models (30 min)

**What You'll Learn:**
- Designing for AI code generation
- Creating programming assistance UX
- Balancing automation with user control

**Materials:**
- [Program-Aided Language Models](/modules/prompting-techniques/program-aided-language-models)

**Activity:** Design AI programming assistant

**Deliverable:** `week11-session44-programming-assistant.md` - Your programming assistant design

<Check>
By the end of this week, you should understand emerging AI techniques and their potential impact on UX design.
</Check>

## Mini-Activities Between Sessions

**Between Sessions 41-42:** Explore how advanced AI techniques are currently used in products you use.

**Between Sessions 42-43:** Sketch how graph-based AI could improve a current user experience.

**Between Sessions 43-44:** Think about how self-improving AI could enhance your own workflow.

## Weekly Deliverables

1. **Advanced Prompting Design** - Interface for sophisticated AI interactions
2. **Graph Interface Design** - Knowledge graph visualization
3. **Self-Correction Design** - AI learning and improvement interface
4. **Programming Assistant Design** - Code generation UX

## Reflection Questions

- Which advanced AI techniques seem most promising for your work?
- How can you balance innovation with user needs?
- What ethical considerations arise with these advanced techniques?

## Next Week Preview

Week 12 is your **Capstone & Application** - you'll apply all your learning to design a comprehensive AI-powered product feature.

---

*Remember: Innovation should serve users, not just showcase technology. Always ask "How does this make the user's life better?"*

## Sources

> **Note:** The following article is reproduced verbatim from
> Fast Real-time Interactive Emoji, *Google* (2025):
> [Fast Real-time Interactive Emoji](https://design.google/library/fast-real-time-and-fully-interactive-emoji)
> for internal educational use only (non-profit).

# ML and the Evolution of Web-Based Experiences: Fast, Real-Time, and Fully Interactive

The advent of Machine Learning (ML) is clearly a groundbreaking moment in modern computer science. As designers—and as users—we've already seen tangible impacts: ML can [help to transform medical diagnoses](https://ai.google/research/teams/brain/healthcare-biosciences), [improve energy efficiency in data centers](https://blog.google/inside-google/infrastructure/safety-first-ai-autonomous-data-center-cooling-and-industrial-control/), and even [identify bowls of ramen by shop](https://www.blog.google/topics/machine-learning/noodle-machine-learning-can-identify-ramen-shop/).

ML has also enabled the development of new, cutting-edge products and user experiences, creating exciting opportunities for web designers. In March, Google announced [TensorFlow.js](https://js.tensorflow.org/), TensorFlow's open-source framework for ML with JavaScript. [TensorFlow.js](https://js.tensorflow.org/) lets web developers train and deploy ML models right in web browsers like Google Chrome. In other words, ML is now publicly available and accessible to anyone with an Internet connection. But what does this really mean for web designers?

> **Note:** The following article is reproduced verbatim from
> Grokking, *Google PAIR* (2025):
> [Grokking](https://pair.withgoogle.com/explorables/grokking/)
> for internal educational use only (non-profit).

# Do Machine Learning Models Memorize or Generalize?

### Grokking Modular Addition

Do more complex models also suddenly generalize after they're trained longer? Large language models can certainly seem like they have a rich understanding of the world, but they might just be regurgitating memorized bits of the enormous amount of text they've been trained on . How can we tell if they're generalizing or memorizing?

In this article we'll examine the training dynamics of a tiny model and reverse engineer the solution it finds – and in the process provide an illustration of the exciting emerging field of mechanistic interpretability . While it isn't yet clear how to apply these techniques to today's largest models, starting small makes it easier to develop intuitions as we progress towards answering these critical questions about large language models.

Modular addition is essentially the fruit fly of grokking. The above line chart comes from a model trained to predict a+b mod 67a + b \bmod 67a+bmod67. We start by randomly dividing all the a,ba, ba,b pairs into test and training datasets. Over thousands of training steps, the training data is used to adjust the model into outputting correct answers, while the test data is only used to check if the model has learned a general solution.

The model's architecture is similarly simple: ReLU(aone-hotWinput+bone-hotWinput)Woutput\text{ReLU}\left(\mathbf{a}_{\text{one-hot}} \mathbf{W}_{\text{input}} + \mathbf{b}_{\text{one-hot}} \mathbf{W}_{\text{input}}\right) \mathbf{W}_{\text{output}}
ReLU(aone-hot​Winput​+bone-hot​Winput​)Woutput​ — a one-layer MLP with 24 neurons. All the weights of the model are shown in the heatmap below; you can see how they change during training by mousing over the line chart above.

The model makes a prediction by selecting the two columns of Winput\mathbf{W}_{\text{input}}Winput​ corresponding to inputs aaa and bbb then adding them together to create a vector of 24 separate numbers. Next it sets all the negative numbers in the vector to 0 and finally outputs the column of Woutput\mathbf{W}_{\text{output}}Woutput​ that's closest to the updated vector.

The weights of the model are initially quite noisy but start to exhibit periodic patterns as accuracy on the test data increases and the model switches to generalizing. By the end of training, each neuron — each row of the heatmap — cycles through high and low values several times as the input number increases from 0 to 66.

This is easier to see if we group the neurons by how often they cycle at the end of training and chart each of them as a separate line:
