---
title: "Design Patterns"
slug: "modules-design-patterns"
updatedAt: "2025-08-18"
tags: [module, design-patterns, architecture, best-practices]
---

# Design Patterns

> Learn essential design patterns for building scalable, maintainable, and efficient AI-powered applications.

## What are Design Patterns?

Design patterns are proven solutions to common problems in software design. They provide reusable templates for solving recurring design challenges, making code more maintainable, scalable, and understandable. In AI applications, design patterns help structure complex interactions between models, data, and user interfaces.

## Why Design Patterns Matter

### Benefits

- **Reusability**: Proven solutions that can be applied across projects
- **Maintainability**: Well-structured code that's easier to understand and modify
- **Scalability**: Patterns that support growth and complexity
- **Consistency**: Standardized approaches across teams and projects
- **Best Practices**: Incorporates industry-proven methodologies

### Common Challenges in AI Applications

- Complex model interactions
- State management across sessions
- Error handling and fallbacks
- Performance optimization
- User experience consistency

## Creational Patterns

### Singleton Pattern

Ensure a class has only one instance and provide global access to it.

```python
class AIConfigManager:
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.config = \{\}
            self._initialized = True
    
    def set_config(self, key: str, value: any):
        self.config[key] = value
    
    def get_config(self, key: str):
        return self.config.get(key)

# Usage
config_manager = AIConfigManager()
config_manager.set_config('model_name', 'gpt-4')
```

### Factory Pattern

Create objects without specifying their exact class.

```python
from abc import ABC, abstractmethod
from typing import Dict, Any

class AIModel(ABC):
    @abstractmethod
    def generate(self, prompt: str) -> str:
        pass

class GPTModel(AIModel):
    def generate(self, prompt: str) -> str:
        return f"GPT response to: \{prompt\}"

class ClaudeModel(AIModel):
    def generate(self, prompt: str) -> str:
        return f"Claude response to: \{prompt\}"

class AIModelFactory:
    @staticmethod
    def create_model(model_type: str) -> AIModel:
        models = {
            'gpt': GPTModel,
            'claude': ClaudeModel
        }
        
        if model_type not in models:
            raise ValueError(f"Unknown model type: \{model_type\}")
        
        return models[model_type]()

# Usage
factory = AIModelFactory()
gpt_model = factory.create_model('gpt')
claude_model = factory.create_model('claude')
```

## Structural Patterns

### Adapter Pattern

Allow incompatible interfaces to work together.

```python
class LegacyAIService:
    def process_text(self, text: str) -> Dict[str, str]:
        return \{"result": f"Legacy processed: \{text\}"\}

class ModernAIInterface:
    def generate_response(self, input_text: str) -> str:
        return f"Modern response: \{input_text\}"

class AIAdapter:
    def __init__(self, legacy_service: LegacyAIService):
        self.legacy_service = legacy_service
    
    def generate_response(self, input_text: str) -> str:
        # Adapt the legacy interface to the modern one
        result = self.legacy_service.process_text(input_text)
        return result["result"]

# Usage
legacy_service = LegacyAIService()
adapter = AIAdapter(legacy_service)
response = adapter.generate_response("Hello, world!")
```

### Decorator Pattern

Add new functionality to objects without altering their structure.

```python
from abc import ABC, abstractmethod

class AIResponse(ABC):
    @abstractmethod
    def get_response(self) -> str:
        pass

class BasicAIResponse(AIResponse):
    def __init__(self, text: str):
        self.text = text
    
    def get_response(self) -> str:
        return self.text

class ResponseDecorator(AIResponse):
    def __init__(self, ai_response: AIResponse):
        self.ai_response = ai_response
    
    def get_response(self) -> str:
        return self.ai_response.get_response()

class LoggingDecorator(ResponseDecorator):
    def get_response(self) -> str:
        response = self.ai_response.get_response()
        print(f"Logging response: \{response\}")
        return response

class CachingDecorator(ResponseDecorator):
    def __init__(self, ai_response: AIResponse):
        super().__init__(ai_response)
        self.cache = \{\}
    
    def get_response(self) -> str:
        # Simple caching implementation
        response = self.ai_response.get_response()
        if response not in self.cache:
            self.cache[response] = response
        return self.cache[response]

# Usage
basic_response = BasicAIResponse("Hello, AI!")
logged_response = LoggingDecorator(basic_response)
cached_logged_response = CachingDecorator(logged_response)
result = cached_logged_response.get_response()
```

## Behavioral Patterns

### Observer Pattern

Define a one-to-many dependency between objects.

```python
from abc import ABC, abstractmethod
from typing import List

class Observer(ABC):
    @abstractmethod
    def update(self, message: str):
        pass

class AIService:
    def __init__(self):
        self.observers: List[Observer] = []
    
    def add_observer(self, observer: Observer):
        self.observers.append(observer)
    
    def remove_observer(self, observer: Observer):
        self.observers.remove(observer)
    
    def notify_observers(self, message: str):
        for observer in self.observers:
            observer.update(message)
    
    def process_request(self, request: str):
        # Process the request
        result = f"Processed: \{request\}"
        # Notify all observers
        self.notify_observers(result)

class LoggingObserver(Observer):
    def update(self, message: str):
        print(f"Logging: \{message\}")

class MetricsObserver(Observer):
    def update(self, message: str):
        print(f"Metrics: Processing request of length \{len(message)\}")

# Usage
ai_service = AIService()
logging_observer = LoggingObserver()
metrics_observer = MetricsObserver()

ai_service.add_observer(logging_observer)
ai_service.add_observer(metrics_observer)

ai_service.process_request("Hello, AI!")
```

### Strategy Pattern

Define a family of algorithms and make them interchangeable.

```python
from abc import ABC, abstractmethod

class PromptStrategy(ABC):
    @abstractmethod
    def generate_prompt(self, input_text: str) -> str:
        pass

class SimplePromptStrategy(PromptStrategy):
    def generate_prompt(self, input_text: str) -> str:
        return f"Answer this question: \{input_text\}"

class DetailedPromptStrategy(PromptStrategy):
    def generate_prompt(self, input_text: str) -> str:
        return f"""
        Please provide a detailed and comprehensive answer to the following question.
        Include examples, explanations, and relevant context.
        
        Question: \{input_text\}
        
        Please structure your response with:
        1. Direct answer
        2. Explanation
        3. Examples
        4. Related concepts
        """

class CreativePromptStrategy(PromptStrategy):
    def generate_prompt(self, input_text: str) -> str:
        return f"""
        Think creatively and provide an innovative perspective on:
        \{input_text\}
        
        Consider:
        - Alternative viewpoints
        - Creative solutions
        - Future implications
        - Unconventional approaches
        """

class AIContext:
    def __init__(self, strategy: PromptStrategy):
        self.strategy = strategy
    
    def set_strategy(self, strategy: PromptStrategy):
        self.strategy = strategy
    
    def process_input(self, input_text: str) -> str:
        prompt = self.strategy.generate_prompt(input_text)
        # Here you would send the prompt to an AI model
        return f"Generated prompt: \{prompt\}"

# Usage
simple_strategy = SimplePromptStrategy()
detailed_strategy = DetailedPromptStrategy()
creative_strategy = CreativePromptStrategy()

context = AIContext(simple_strategy)
print(context.process_input("What is AI?"))

context.set_strategy(detailed_strategy)
print(context.process_input("What is AI?"))

context.set_strategy(creative_strategy)
print(context.process_input("What is AI?"))
```

## AI-Specific Patterns

### Chain of Responsibility Pattern

Pass requests along a chain of handlers.

```python
from abc import ABC, abstractmethod
from typing import Optional

class Request:
    def __init__(self, text: str, request_type: str):
        self.text = text
        self.request_type = request_type
        self.response = None

class Handler(ABC):
    def __init__(self):
        self.next_handler: Optional[Handler] = None
    
    def set_next(self, handler: 'Handler') -> 'Handler':
        self.next_handler = handler
        return handler
    
    @abstractmethod
    def handle(self, request: Request) -> bool:
        pass

class InputValidationHandler(Handler):
    def handle(self, request: Request) -> bool:
        if not request.text or len(request.text.strip()) == 0:
            request.response = "Error: Empty input"
            return False
        
        if len(request.text) > 1000:
            request.response = "Error: Input too long"
            return False
        
        return self.next_handler.handle(request) if self.next_handler else True

class ContentFilterHandler(Handler):
    def handle(self, request: Request) -> bool:
        inappropriate_words = ['spam', 'inappropriate', 'blocked']
        
        for word in inappropriate_words:
            if word in request.text.lower():
                request.response = f"Error: Content contains inappropriate word: \{word\}"
                return False
        
        return self.next_handler.handle(request) if self.next_handler else True

class AIProcessingHandler(Handler):
    def handle(self, request: Request) -> bool:
        # Simulate AI processing
        request.response = f"AI processed: \{request.text\}"
        return True

# Usage
validation_handler = InputValidationHandler()
filter_handler = ContentFilterHandler()
ai_handler = AIProcessingHandler()

# Chain the handlers
validation_handler.set_next(filter_handler).set_next(ai_handler)

# Test requests
request1 = Request("Hello, AI!", "general")
validation_handler.handle(request1)
print(request1.response)

request2 = Request("", "general")
validation_handler.handle(request2)
print(request2.response)

request3 = Request("This is spam content", "general")
validation_handler.handle(request3)
print(request3.response)
```

### Template Method Pattern

Define the skeleton of an algorithm in a base class.

```python
from abc import ABC, abstractmethod

class AIPipeline(ABC):
    def process(self, input_data: str) -> str:
        """Template method defining the algorithm structure"""
        # Step 1: Preprocess
        processed_data = self.preprocess(input_data)
        
        # Step 2: Validate
        if not self.validate(processed_data):
            return "Validation failed"
        
        # Step 3: Generate response
        response = self.generate_response(processed_data)
        
        # Step 4: Post-process
        final_response = self.postprocess(response)
        
        return final_response
    
    @abstractmethod
    def preprocess(self, data: str) -> str:
        pass
    
    @abstractmethod
    def validate(self, data: str) -> bool:
        pass
    
    @abstractmethod
    def generate_response(self, data: str) -> str:
        pass
    
    @abstractmethod
    def postprocess(self, response: str) -> str:
        pass

class ChatbotPipeline(AIPipeline):
    def preprocess(self, data: str) -> str:
        return data.strip().lower()
    
    def validate(self, data: str) -> bool:
        return len(data) > 0 and len(data) <= 500
    
    def generate_response(self, data: str) -> str:
        return f"Chatbot response to: \{data\}"
    
    def postprocess(self, response: str) -> str:
        return response.capitalize()

class TranslationPipeline(AIPipeline):
    def preprocess(self, data: str) -> str:
        return data.strip()
    
    def validate(self, data: str) -> bool:
        return len(data) > 0
    
    def generate_response(self, data: str) -> str:
        return f"Translated: \{data\}"
    
    def postprocess(self, response: str) -> str:
        return response + " [Translation complete]"

# Usage
chatbot = ChatbotPipeline()
translation = TranslationPipeline()

print(chatbot.process("Hello, how are you?"))
print(translation.process("Bonjour, comment allez-vous?"))
```

## Best Practices

### 1. Choose the Right Pattern

- **Creational**: When object creation is complex or needs to be controlled
- **Structural**: When you need to compose objects or provide alternative interfaces
- **Behavioral**: When you need to manage algorithms, relationships, and responsibilities

### 2. Keep It Simple

- Don't over-engineer solutions
- Use patterns only when they add value
- Consider the complexity trade-offs

### 3. Document Your Patterns

- Explain why you chose a specific pattern
- Document the responsibilities of each component
- Provide usage examples

### 4. Test Your Patterns

- Unit test each pattern implementation
- Test pattern interactions
- Verify that patterns solve the intended problem

## Implementation Examples

### Configuration Management

```python
class ConfigManager:
    def __init__(self):
        self.config = \{\}
        self.observers = []
    
    def set_config(self, key: str, value: any):
        self.config[key] = value
        self.notify_observers(key, value)
    
    def get_config(self, key: str, default=None):
        return self.config.get(key, default)
    
    def add_observer(self, observer):
        self.observers.append(observer)
    
    def notify_observers(self, key: str, value: any):
        for observer in self.observers:
            observer.config_changed(key, value)

class LoggingObserver:
    def config_changed(self, key: str, value: any):
        print(f"Config changed: \{key\} = \{value\}")

# Usage
config = ConfigManager()
config.add_observer(LoggingObserver())
config.set_config('model_name', 'gpt-4')
```

### Error Handling Strategy

```python
class ErrorHandler:
    def __init__(self):
        self.strategies = {
            'retry': self.retry_strategy,
            'fallback': self.fallback_strategy,
            'ignore': self.ignore_strategy
        }
    
    def handle_error(self, error: Exception, strategy: str = 'retry'):
        if strategy in self.strategies:
            return self.strategies[strategy](error)
        else:
            raise error
    
    def retry_strategy(self, error: Exception):
        print(f"Retrying after error: \{error\}")
        # Implement retry logic
        return "Retry successful"
    
    def fallback_strategy(self, error: Exception):
        print(f"Using fallback for error: \{error\}")
        return "Fallback response"
    
    def ignore_strategy(self, error: Exception):
        print(f"Ignoring error: \{error\}")
        return None
```

> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [Figma Make Tutorial: Build Interactive Apps with AI](https://www.codecademy.com/article/figma-make-tutorial)  
> for internal educational use only (non-profit).

# Figma Make Tutorial: Build Interactive Apps with AI

Ever designed a great UI and thought, "I wish I could just make this work without jumping through extra tools or workflows"? What if you could bring your designs to life by describing what you want, no handoff, plugins, or context switching? That's precisely where Figma Make comes in!

Figma Make, launched at Config 2025, is Figma's AI-powered prompt-to-code environment that allows designers to build real, functional prototypes directly inside Figma. Unlike traditional files, it lets us use plain English to add logic, navigation, responsiveness, and interactivity to our designs without requiring manual coding.

Let's build a personalized finance app using Figma Make.

## Building a finance tracker app

In this project, we'll build a personalized finance tracker that helps users do exactly that, with a clean and interactive dashboard powered by Figma Make. This app gives users a snapshot of their financial health. It includes features like:

- A dashboard showing income, expenses, and account balances
- Monthly report generation, styled like a bank statement
- A transaction history with filters, tags, and categories

It's inspired by tools like Plaid, QuickBooks, and Notion-style finance templates, but it's streamlined for personal use.

Let's start by defining the visual structure, the wireframe. A strong wireframe sets the foundation for meaningful AI output.

Also, here's a video tutorial that walks through the same concepts step by step—with visuals, examples, and a real-time build. Use it alongside the text or on its own—whatever helps you learn best.

### Step 1: Create wireframes for your app

A wireframe is a low-fidelity blueprint of your app's layout. Create a wireframe for this app, focusing on the structure, which answers:

- "What goes where?"
- "How do screens connect?"
- "What components does the user need to interact with?"

Unlike typical prototypes, where wireframes are handed off to developers, in Figma Make, your wireframe becomes part of the functional build process. You can:

- Use it to plan the layout and logic clearly
- Feed it directly into the Figma Make file
- Guide your prompts more effectively, since the visual hierarchy already exists

Let's look at each screen in detail.

#### Screen 1: Dashboard overview

The dashboard is the main landing screen. It gives the user details of their current financial health, including their earnings and spending, and forming trends.

This screen will have:

- A top navigation bar with the app name and profile access
- Summary cards for total income and total expenses
- A toggleable line chart that switches views between income and expenses
- Quick action buttons like "Add Transaction", "View Report", "Export Data"

Here's a sample wireframe of the dashboard screen:

![Wireframe of a personal finance dashboard showing income, expenses, trends chart, and action buttons](https://static-assets.codecademy.com/figma-make/Wireframe-1-dashboard.png)
<sub>Source: *Codecademy*, Codecademy Team (2025).</sub>

> Note: The wireframes used here are just examples and you can use your layouts as the foundation for your prompts in Figma Make.

#### Screen 2: Monthly report page

The monthly report page will mimic a formatted statement view. Think of it like a printable snapshot of a user's income and expenses for any given month. It'll have the following components:

- A header with the report title and date filters
- A summary block showing totals and net savings
- A breakdown table by spending category
- A footer for optional disclaimers or notes

Here's a sample wireframe of this screen:

![Wireframe of a monthly financial summary with filters, income and expense overview, and a category breakdown table](https://static-assets.codecademy.com/figma-make/Wireframe-2-Monthly-financial-summary.png)
<sub>Source: *Codecademy*, Codecademy Team (2025).</sub>

#### Screen 3: Transactions details page

This is the most data-dense screen, designed to show the user a history of every transaction they've made. It'll have the following components:

- A search bar and filter options
- A list of transactions with details like date, amount, category, and description
- Pagination controls for large datasets
- Export functionality

Here's a sample wireframe of this screen:

![Wireframe of a transaction history page with search, filters, transaction list, and pagination](https://static-assets.codecademy.com/figma-make/Wireframe-3-Transaction-history.png)
<sub>Source: *Codecademy*, Codecademy Team (2025).</sub>

### Step 2: Set up your Figma Make file

Now that we have our wireframes, let's set up the Figma Make environment:

1. **Create a new Figma Make file**: Go to Figma and create a new file. You'll see a "Make" option in the file type selector.

2. **Import your wireframes**: You can either:
   - Create the wireframes directly in Figma Make
   - Import existing wireframes from other Figma files
   - Use the sample wireframes provided above as reference

3. **Set up the canvas**: Organize your screens in a logical flow, typically left to right or top to bottom.

### Step 3: Start with the dashboard screen

Let's begin building the dashboard. In Figma Make, you can use natural language to describe what you want:

**Prompt example**: "Create a dashboard with a header navigation bar, two summary cards for income and expenses, a line chart that can toggle between income and expense views, and three action buttons at the bottom."

The AI will generate the layout and basic styling. You can then refine it with more specific prompts:

**Refinement prompt**: "Make the income card green and the expense card red. Add icons to the action buttons and make them rounded with a subtle shadow."

### Step 4: Add interactivity and logic

This is where Figma Make really shines. You can add functionality without writing code:

**Navigation prompt**: "When the user clicks 'View Report', navigate to the monthly report screen."

**Data handling prompt**: "Create a data structure for transactions with fields for date, amount, category, and description. Display sample data in the dashboard."

**Chart functionality prompt**: "Make the line chart interactive. When users click the 'Income' or 'Expenses' toggle, update the chart data accordingly."

### Step 5: Build the monthly report screen

For the monthly report, you'll want to focus on data presentation and filtering:

**Layout prompt**: "Create a monthly report layout with a header showing the month and year, summary cards for total income, expenses, and net savings, and a detailed breakdown table."

**Filtering prompt**: "Add date picker controls that allow users to select different months. Update the report data when the date changes."

**Styling prompt**: "Style the report to look like a professional bank statement with clean typography and proper spacing."

### Step 6: Create the transaction history screen

The transaction history screen requires more complex data handling:

**Data structure prompt**: "Create a comprehensive transaction data structure with fields for ID, date, amount, category, description, and tags."

**Search functionality prompt**: "Add a search bar that filters transactions by description or category. Make it work in real-time as the user types."

**Pagination prompt**: "Implement pagination for the transaction list, showing 10 transactions per page with navigation controls."

### Step 7: Connect all screens together

Now it's time to make everything work together:

**Navigation system prompt**: "Create a navigation system that allows users to move between the dashboard, monthly report, and transaction history screens. Add a bottom navigation bar or breadcrumbs."

**Data consistency prompt**: "Ensure that data changes in one screen are reflected across all screens. For example, adding a transaction should update the dashboard totals and appear in the transaction history."

**State management prompt**: "Implement state management so that user preferences, filters, and current view are preserved when navigating between screens."

### Step 8: Add advanced features

Once the basic functionality is working, you can add more sophisticated features:

**Responsive design prompt**: "Make the app responsive so it works well on different screen sizes. Adjust layouts for mobile, tablet, and desktop views."

**Animations prompt**: "Add smooth transitions between screens and subtle animations for user interactions like button clicks and data updates."

**Export functionality prompt**: "Add the ability to export transaction data as CSV or PDF reports."

### Step 9: Test and refine

Testing is crucial for any app, even AI-generated ones:

**User testing prompt**: "Create a test scenario where a user adds a new transaction, views the monthly report, and exports their data. Ensure all flows work correctly."

**Error handling prompt**: "Add error handling for edge cases like empty data, network issues, or invalid user input."

**Performance optimization prompt**: "Optimize the app for performance, especially when handling large datasets or complex calculations."

## Best Practices for Figma Make

### 1. Start with Clear Requirements

Before diving into prompts, clearly define what you want to build:

- **User stories**: Write down what users should be able to do
- **Functional requirements**: List specific features and behaviors
- **Technical constraints**: Consider performance, compatibility, and scalability

### 2. Use Iterative Development

Build your app in small, manageable steps:

- **Start simple**: Begin with basic layouts and functionality
- **Add complexity gradually**: Build upon working features
- **Test frequently**: Verify each addition works before moving on

### 3. Write Effective Prompts

The quality of your prompts directly affects the output:

- **Be specific**: Include details about layout, styling, and behavior
- **Use clear language**: Avoid ambiguous terms and jargon
- **Provide context**: Reference existing elements and relationships
- **Iterate**: Refine prompts based on the results you get

### 4. Leverage Figma's Design System

Take advantage of Figma's built-in design capabilities:

- **Use components**: Create reusable UI elements
- **Apply styles**: Use consistent colors, typography, and spacing
- **Maintain consistency**: Ensure visual coherence across screens

### 5. Test Across Different Scenarios

Don't just test the happy path:

- **Edge cases**: Test with empty data, invalid inputs, and error conditions
- **User flows**: Walk through complete user journeys
- **Performance**: Test with realistic data volumes
- **Accessibility**: Ensure the app works for users with different needs

## Common Challenges and Solutions

### Challenge 1: Complex Data Relationships

**Problem**: Managing relationships between different data entities (transactions, categories, accounts).

**Solution**: Use clear data structures and implement proper state management. Break complex relationships into smaller, manageable pieces.

### Challenge 2: Performance with Large Datasets

**Problem**: The app becomes slow when handling many transactions.

**Solution**: Implement pagination, lazy loading, and efficient data filtering. Consider data virtualization for very large datasets.

### Challenge 3: Responsive Design

**Problem**: The app doesn't work well on different screen sizes.

**Solution**: Design with mobile-first approach and use flexible layouts. Test on various devices and screen sizes.

### Challenge 4: State Management

**Problem**: Data gets out of sync between different screens.

**Solution**: Implement a centralized state management system and ensure all components update consistently.

## Advanced Techniques

### 1. Custom Components

Create reusable components for common UI patterns:

**Prompt**: "Create a reusable transaction card component that displays date, amount, category, and description. Make it customizable for different transaction types."

### 2. Data Visualization

Add charts and graphs to make data more meaningful:

**Prompt**: "Create a pie chart showing spending by category and a line chart showing spending trends over time."

### 3. User Preferences

Allow users to customize their experience:

**Prompt**: "Add a settings screen where users can choose their preferred currency, date format, and default view."

### 4. Offline Functionality

Make the app work without an internet connection:

**Prompt**: "Implement local storage so users can add transactions offline and sync when they're back online."

## Conclusion

Figma Make represents a significant shift in how designers and developers can collaborate. By combining the visual design capabilities of Figma with AI-powered code generation, it opens up new possibilities for rapid prototyping and development.

The key to success with Figma Make is understanding that it's not just about generating code—it's about creating a seamless workflow between design and development. The better you can describe what you want, the better the results will be.

Remember that Figma Make is still evolving, and the quality of your prompts will improve with practice. Start with simple projects, experiment with different approaches, and gradually build up to more complex applications.

As you become more comfortable with Figma Make, you'll find that it can significantly speed up your development process while maintaining the quality and consistency of your designs. The ability to iterate quickly and see immediate results makes it an invaluable tool for modern app development.

Whether you're a designer looking to bring your ideas to life or a developer wanting to prototype quickly, Figma Make provides a powerful platform for building interactive applications with AI assistance.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Beyond The Hype: What AI Can Really Do For Product Design](https://www.smashingmagazine.com/2025/08/beyond-hype-what-ai-can-do-product-design/)  
> for internal educational use only (non-profit).

# Beyond The Hype: What AI Can Really Do For Product Design

These days, it's easy to find curated lists of AI tools for designers, galleries of generated illustrations, and countless prompt libraries. What's much harder to find is a clear view of how AI is actually integrated into the everyday workflow of a product designer — not for experimentation, but for real, meaningful outcomes.

I've gone through that journey myself: testing AI across every major stage of the design process, from ideation and prototyping to visual design and user research. Along the way, I've built a simple, repeatable workflow that significantly boosts my productivity.

In this article, I'll share what's already working and break down some of the most common objections I've encountered — many of which I've faced personally.

## Stage 1: Idea Generation Without The Clichés

Pushback: "Whenever I ask AI to suggest ideas, I just get a list of clichés. It can't produce the kind of creative thinking expected from a product designer."

That's a fair point. AI doesn't know the specifics of your product, the full context of your task, or many other critical nuances. The most obvious fix is to "feed it" all the documentation you have. But that's a common mistake as it often leads to even worse results: the context gets flooded with irrelevant information, and the AI's answers become vague and unfocused.

Current-gen models can technically process thousands of words, but the longer the input, the higher the risk of missing something important, especially content buried in the middle. This is known as the "lost in the middle" problem.

To get meaningful results, AI doesn't just need more information — it needs the right information, delivered in the right way. That's where the RAG approach comes in.

### How RAG Works

Think of RAG as a smart assistant working with your personal library of documents. You upload your files, and the assistant reads each one, creating a short summary — a set of bookmarks (semantic tags) that capture the key topics, terms, scenarios, and concepts. These summaries are stored in a kind of "card catalog," called a vector database.

When you ask a question, the assistant doesn't reread every document from cover to cover. Instead, it compares your query to the bookmarks, retrieves only the most relevant excerpts (chunks), and sends those to the language model to generate a final answer.

### How Is This Different from Just Dumping a Doc into the Chat?

Let's break it down:

**Typical chat interaction**

It's like asking your assistant to read a 100-page book from start to finish every time you have a question. Technically, all the information is "in front of them," but it's easy to miss something, especially if it's in the middle. This is exactly what the "lost in the middle" issue refers to.

**RAG approach**

You ask your smart assistant a question, and it retrieves only the relevant pages (chunks) from different documents. It's faster and more accurate, but it introduces a few new risks:

- **Ambiguous question**: You ask, "How can we make the project safer?" and the assistant brings you documents about cybersecurity, not finance.
- **Mixed chunks**: A single chunk might contain a mix of marketing, design, and engineering notes. That blurs the meaning so the assistant can't tell what the core topic is.
- **Semantic gap**: You ask, "How can we speed up the app?" but the document says, "Optimize API response time." For a human, that's obviously related. For a machine, not always.

![Diagram showing how RAG works: a user prompt triggers semantic search through a knowledge base. Relevant chunks are sent to a language model, which generates an answer based on retrieved content.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/beyond-hype-what-ai-can-do-product-design/1-rag-approach.png)
<sub>Instead of using the model's memory, it searches your documents and builds a response based on what it finds. (Large preview)</sub>

These aren't reasons to avoid RAG or AI altogether. Most of them can be avoided with better preparation of your knowledge base and more precise prompts. So, where do you start?

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Designing With AI, Not Around It: Practical Advanced Techniques For Product Design Use Cases](https://www.smashingmagazine.com/2025/08/designing-with-ai-practical-techniques-product-design/)  
> for internal educational use only (non-profit).

# Designing With AI, Not Around It: Practical Advanced Techniques For Product Design Use Cases

AI is almost everywhere — it writes text, makes music, generates code, draws pictures, runs research, chats with you — and apparently even understands people better than they understand themselves?!

It's a lot to take in. The pace is wild, and new tools pop up faster than anyone has time to try them. Amid the chaos, one thing is clear: this isn't hype, but it's structural change.

According to the Future of Jobs Report 2025 by the World Economic Forum, one of the fastest-growing, most in-demand skills for the next five years is the ability to work with AI and Big Data. That applies to almost every role — including product design.

![A figure showing skills on the rise in 2025-2030, which places AI and big data on the first place](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/designing-with-ai-practical-techniques-product-design/1-skills-on-the-rise-2025.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

What do companies want most from their teams? Right, efficiency. And AI can make people way more efficient. We'd easily spend 3x more time on tasks like replying to our managers without AI helping out. We're learning to work with it, but many of us are still figuring out how to meet the rising bar.

That's especially important for designers, whose work is all about empathy, creativity, critical thinking, and working across disciplines. It's a uniquely human mix. At least, that's what we tell ourselves.

Even as debates rage about AI's limitations, tools today (June 2025 — timestamp matters in this fast-moving space) already assist with research, ideation, and testing, sometimes better than expected.

Of course, not everyone agrees. AI hallucinates, loses context, and makes things up. So how can both views exist at the same time? Very simple. It's because both are true: AI is deeply flawed and surprisingly useful. The trick is knowing how to work with its strengths while managing its weaknesses. The real question isn't whether AI is good or bad — it's how we, as designers, stay sharp, stay valuable, and stay in the loop.

## Why Prompting Matters

Prompting matters more than most people realize because even small tweaks in how you ask can lead to radically different outputs. To see how this works in practice, let's look at a simple example.

Imagine you want to improve the onboarding experience in your product. On the left, you have the prompt you send to AI. On the right, the response you get back.

This side-by-side shows just how much even the smallest prompt details can change what AI gives you.

Talking to an AI model isn't that different from talking to a person. If you explain your thoughts clearly, you get a better understanding and communication overall.

> Advanced prompting is about moving beyond one-shot, throwaway prompts. It's an iterative, structured process of refining your inputs using different techniques so you can guide the AI toward more useful results. It focuses on being intentional with every word you put in, giving the AI not just the task but also the path to approach it step by step, so it can actually do the job.

![Advanced prompting vs basic promting](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/designing-with-ai-practical-techniques-product-design/2-advanced-prompting.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Where basic prompting throws your question at the model and hopes for a quick answer, advanced prompting helps you explore options, evaluate branches of reasoning, and converge on clear, actionable outputs.

But that doesn't mean simple prompts are useless. On the contrary, short, focused prompts work well when the task is narrow, factual, or time-sensitive. They're great for idea generation, quick clarifications, or anything where deep reasoning isn't required. Think of prompting as a scale, not a binary. The simpler the task, the faster a lightweight prompt can get the job done. The more complex the task, the more structure it needs.

In this article, we'll dive into how advanced prompting can empower different product & design use cases, speeding up your workflow and improving your results — whether you're researching, brainstorming, testing, or beyond. Let's dive in.

## Practical Cases

In the next section, we'll explore six practical prompting techniques that we've found most useful in real product design work. These aren't abstract theories — each one is grounded in hands-on experience, tested across research, ideation, and evaluation tasks. Think of them as modular tools: you can mix, match, and adapt them depending on your use case. For each, we'll explain the thinking behind it and walk through a sample prompt.

Important note: The prompts you'll see are not copy-paste recipes. Some are structured templates you can reuse with small tweaks; others are more specific, meant to spark your thinking. Use them as scaffolds, not scripts.

### 1. Task Decomposition By JTBD

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Design Patterns For AI Interfaces](https://www.smashingmagazine.com/2025/07/design-patterns-ai-interfaces/)  
> for internal educational use only (non-profit).

# Design Patterns For AI Interfaces

So you need to design a new AI feature for your product. How would you start? How do you design flows and interactions? And how do you ensure that that new feature doesn't get abandoned by users after a few runs?

In this article, I'd love to share a very simple but systematic approach to how I think about designing AI experiences. Hopefully, it will help you get a bit more clarity about how to get started.

This article is part of our ongoing series on UX. You can find more details on design patterns and UX strategy in Smart Interface Design Patterns 🍣 — with live UX training coming up soon. Jump to table of contents.

## The Receding Role of AI Chat

One of the key recent shifts is a slow move away from traditional "chat-alike" AI interfaces. As Luke Wroblewski wrote, when agents can use multiple tools, call other agents and run in the background, users orchestrate AI work more — there's a lot less chatting back and forth.

![AI Experience Paradigm by Luke Wroblewski](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/1-ai-experience-paradigm.jpg)
<sub>Messaging UI slowly starts feeling dated, and chat UI fades into background. By Luke Wroblewski. (Large preview)</sub>

In fact, chatbots are rarely a great experience paradigm — mostly because the burden of articulating intent efficiently lies on the user. But in practice, it's remarkably difficult to do well and very time-consuming.

Chat doesn't go away, of course, but it's being complemented with task-oriented UIs — temperature controls, knobs, sliders, buttons, semantic spreadsheets, infinite canvases — with AI providing predefined options, presets, and templates.

![AI Experience Paradigm by Luke Wroblewski](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/2-agentic-ai-design-patterns.jpg)
<sub>Agentic AI design patterns, with more task-oriented UIs, rather than chat. By Luke Wroblewski. (Large preview)</sub>

There, AI emphasizes the work, the plan, the tasks — the outcome, instead of the chat input. The results are experiences that truly amplify value for users by sprinkling a bit of AI in places where it delivers real value to real users.

To design better AI experiences, we need to study 5 key areas that we need to shape.

## Input UX: Expressing Intent

Conversational AI is a very slow way of helping users express and articulate their intent. Usability tests show that users often get lost in editing, reviewing, typing, and re-typing. It's painfully slow, often taking 30-60 seconds for input.

As it turns out, people have a hard time expressing their intent well. In fact, instead of writing prompts manually, it's a good idea to ask AI to write a prompt to feed itself.

![Illustration: How users can express their intent in AI interfaces.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/3-flora-ai.jpg)
<sub>Flora AI allows you to modify images and videos via nodes. (Large preview)</sub>

With Flora AI, users can still write prompts, but they visualize their intent with nodes by connecting various sources visually. Instead of elaborately explaining to AI how we need the pipeline to work, we attach nodes and commands on a canvas.

![Illustration of Output UX](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/4-illustration-output-ux.jpg)
<sub>With Krea.ai, users can move abstract shapes (on the left) to explain their goal to AI and study the outcome (on the right). (Large preview)</sub>

With input for AI, being precise is slow and challenging. Instead, we can abstract away the object we want to manipulate, and give AI precise input by moving that abstracted object on a canvas. That's what Krea.ai does.

In summary, we can minimize the burden of typing prompts manually — with AI-generated pre-prompts, prompt extensions, query builders, and also voice input.

## Output UX: Displaying Outcomes

AI output doesn't have to be merely plain text or a list of bullet points. It must be helpful to drive people to insights, faster. For example, we could visualize output by creating additional explanations based on the user's goal and motivations.

![Illustration of Output UX](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/5-illustration-output-ux.jpg)
<sub>Visualizing outcome through style lenses. By Amelia Wattenberger. (Large preview)</sub>

For example, Amelia Wattenberger visualized AI output for her text editor PenPal by adding style lenses to explore the content from. The output could be visualized in sentence lengths and scales Sad — Happy, Concrete — Abstract, and so on.

![Illustration of Output UX](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/6-aino-ai.jpg)
<sub>Aino.ai, an AI GIS Analyst for urban planning. (Large preview)</sub>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Using Manim For Making UI Animations](https://www.smashingmagazine.com/2025/04/using-manim-making-ui-animations/)  
> for internal educational use only (non-profit).

# Using Manim For Making UI Animations

Say you are learning to code for the first time, in Python, for example, which is a great starting point for getting into development. You are likely to come across some information like "a variable stores a value." That sounds straightforward, but if you are a beginner just starting, then it can also be a bit confusing. How does a variable store or hold something? What happens when we assign a new value to it?

To figure things out, you could read a bunch and watch tutorials, but sometimes, resources like these don't help the concept fully click. That's where animation helps. It has the power to take complex programming concepts and turn them into something visual, dynamic, and easy to grasp.

Let's break it down with an example: Say we have a box labeled X, first empty, then fill with a value 5, for this example, then update to 12, then 8, then 20, then 3.

Even if you are unfamiliar with Python, an animation like this makes the concept more obvious, helping you understand how variables work with visual cues. You can now visualize the variables as containers that hold and update values dynamically. It's way easier to see that than it is to just read about variables.

Well, Manim isn't just limited to programming; it works for math, physics, UI/UX, and more. In trigonometry, you can take something like a "Sine Wave" as an example, which is a smooth, continuous curve that moves up and down in a repeating pattern, and it is found everywhere from sound waves to electrical signals to the motion of a pendulum.

Sounds simple, right? Or maybe a bit confusing, especially if you're not a math person, but let me help with this:

Now, with this, you can see how the wave moves. Instead of just numbers and formulas, you're watching it happen. And that's pretty much the idea here! In this article, we'll explore Manim and how it makes concepts easier to understand through animation.

## Manim, Manim! What Is It?

By now, you may have a rough idea of what Manim can do, but let's break it down a little more. What exactly is Manim? Well, it's two things.

> First, Manim is an open-source Python library for creating high-quality mathematical animations.

If you've ever watched a 3Blue1Brown video, you've seen Manim in action because Grant Sanderson originally developed it for his YouTube channel.

> Second, Manim is a script-driven animation engine, meaning you write Python code to generate animations instead of dragging and dropping elements like in typical video editing software.

This gives you precise control over every detail, including text, color, shape, transformations, timing — you name it. Whether you're explaining math, physics, or programming concepts, Manim makes it fairly easy to create clear and dynamic visuals with just a few lines of code. Plus, it works seamlessly with LaTeX, so you can render mathematical equations beautifully without extra effort. That's why it's popular among educators, researchers, and content creators.

Of course, Manim isn't the only tool you can use. If it doesn't quite fit your needs or the programming language you are most comfortable with, here are some alternatives worth checking out:

- **Processing**: This is a Java-based coding framework, great for generative art and interactive visuals. If you enjoy experimenting with visual design through code, in Java, to be exact, then Processing gives you a solid foundation.
- **p5.js**: This is a JavaScript library, an alternative for web animations. If you're a front-end developer working with HTML and CSS, p5.js makes it easy for you to create graphics directly in the browser.
- **Desmos**: This focuses on math visualization. Desmos lets you create interactive graphs and scripted animations directly in the browser. You can use it through Desmos Graphs, Desmos Calculator, or the Desmos API.
- **Blender (with Python Scripting)**: This is mostly known for 3D animation, but with its Python API, you can script animations, including math and physics-based simulations.

## How To Get Started

There are multiple ways to install the library. You can set it up locally, use Conda or Docker, or run it inside Jupyter Notebooks. But if you don't want to deal with installations, Replit is a great alternative, as it's a real-time live editor that lets you start coding animations instantly.

### 1. Create An Account On Replit Using GitHub or Email.

Once you're in, your dashboard should look something like this:

![Replit dashboard](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-manim-making-ui-animations/1-create-account-replit.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Beyond Algorithms: Skills Of Designers That AI Can't Replicate](https://www.smashingmagazine.com/2023/04/skills-designers-ai-cant-replicate/)  
> for internal educational use only (non-profit).

# Beyond Algorithms: Skills Of Designers That AI Can't Replicate

At the start of the Coronavirus pandemic, I led the redesign of a tablet app used by sales representatives of the world's largest food & beverage company. Never having been a sales representative, nor having ever played one on TV, I was curious about their typical workday. Adapting the first rule of design — Know Thy User — our lockdown approach was to conduct video interviews. As soon as company restrictions allowed, I met two sales representatives at a local Walmart.

Masked and socially distant, I walked a mile in their shoes through the dairy, pet food, and freezer aisles. This single visit uncovered many insights that had not come up in the video interviews and online walkthroughs. I shared this with the team, spread across the world, and everyone could empathize with the sales representatives: juggling multiple devices and printouts, struggling to make technology work in extreme conditions like a low-lit walk-in freezer, and trying to work without hindering harried shoppers. The sales reps would repeat these tasks between twenty and thirty times a day, five days a week, which sounds about as fun as it is.

Our team used these insights to experiment with different concepts, refine them based on feedback from sales representatives, and launch a redesigned app that received glowing feedback from the representatives and praise from the company stakeholders.

Curiosity, empathy, and collaboration were some of the designer-like or designerly behaviors we used to transform the sales representatives' experience. These behaviors are a few of the behaviors and skills that designers use throughout the design process. Design researcher and educator Nigel Cross first used the word designerly to refer to underlying patterns of how designers think and act.

Designers spend years learning technical design skills, and as they use those hard skills to do their jobs, their designs are impactful when they actively use these non-technical designerly skills and behaviors. Designerly skills and behaviors make us creative and innovative and distinguish us from machines and technology like Artificial Intelligence (AI).

Yes, the same AI that you can't avoid reading or hearing about on social media or in the news. Stories and posts about people being equally worried about layoffs and AI taking over their jobs, and some even suggesting that AI is why those jobs won't come back. Creators and people traditionally considered creative, like artists, writers, and designers, seem especially concerned about AI making them redundant. Guesstimates of when AI will perform tasks better than humans just add to the frenzy.

![Timeline of AI development in text, code, images, and video categories](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/skills-designers-ai-cant-replicate/1-timeline-ai-development.jpg)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The assumption that AI will replace people is based on the premise that both have the same qualities, abilities, and skills. But that's just not true. Artificial intelligence is simply technology that is taught to mimic human intelligence to perform tasks. It is trained on large amounts of data — by some estimates, the equivalent of a quarter of the Library of Congress, the world's largest library.

AI is better than humans in certain tasks that involve processing and analyzing large amounts of data quickly, accurately, rationally, and consistently. Artificial Intelligence may create, but it can't be creative. It cannot match humans in areas that rely on skills and behaviors that are distinctly human, like intuition, emotional intelligence, cultural context, and changing situations.

Humans are conscious beings with a subconscious mind that can influence decisions and change those decisions based on experience, context, environment, wisdom, and understanding. This takes us years, decades, and even a lifetime to learn and apply, and it cannot be programmed in machines, no matter how sentient they may appear to be. Not for the foreseeable future.

Being designerly takes thinking, feeling, and acting like a designer. I've been thinking about and observing what it means to be designerly, and by using six such skills and behaviors, I will discuss how humans have an advantage over AI. I used the head, heart, and hands approach for transformative sustainability learning (Orr, Sipos, et al.) to organize these designerly skills related to thinking (head), feeling (heart), and doing (hands), and offer ways to practice them.

> Using our head, heart, and hands together to make a transformative difference is what distinguishes us from AI and makes us human, creative, and innovative.

![A picture of a wooden Lego man with designerly skills written next to him grouped and organized by the head, heart, and hands](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/skills-designers-ai-cant-replicate/3-designerly-skills.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

## Head

The skills, behaviors, and habits to help you think like a designer and create a designerly mindset include curiosity and observation.

### Cultivate Curiosity

Curiosity is the desire to know. It is a pleasure to ask, explore, experiment, discover, learn, and understand. We see this relentless curiosity in small children, who explore everything novel around them. As they grow up, that curiosity starts getting stifled in many, partly because they are taught to look for an answer instead of exploring questions.

This curiosity stifler of focusing on the answer is what AI is programmed to do. AI is also limited by its knowledge and understanding of the world, unable to explore beyond those boundaries. Also, without physical senses, AI cannot experience the world and be curious about things we see, hear, touch, taste, and smell around us.

This gives us a leg up on AI if we can overcome other curiosity-stiflers like self-consciousness, the shame of not knowing, and the fear of ridicule.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [A Week In The Life Of An AI-Augmented Designer](https://www.smashingmagazine.com/2025/08/week-in-life-ai-augmented-designer/)  
> for internal educational use only (non-profit).

# A Week In The Life Of An AI-Augmented Designer

Artificial Intelligence isn't new, but in November 2022, something changed. The launch of ChatGPT brought AI out of the background and into everyday life. Suddenly, interacting with a machine didn't feel technical — it felt conversational.

Just this March, ChatGPT overtook Instagram and TikTok as the most downloaded app in the world. That level of adoption shows that millions of everyday users, not just developers or early adopters, are comfortable using AI in casual, conversational ways. People are using AI not just to get answers, but to think, create, plan, and even to help with mental health and loneliness.

In the past two and a half years, people have moved through the Kübler-Ross Change Curve — only instead of grief, it's AI-induced uncertainty. UX designers, like Kate (who you'll meet shortly), have experienced something like this:

- **Denial**: "AI can't design like a human; it won't affect my workflow."
- **Anger**: "AI will ruin creativity. It's a threat to our craft."
- **Bargaining**: "Okay, maybe just for the boring tasks."
- **Depression**: "I can't keep up. What's the future of my skills?"
- **Acceptance**: "Alright, AI can free me up for more strategic, human work."

As designers move into experimentation, they're not asking, Can I use AI? but How might I use it well?.

> Using AI isn't about chasing the latest shiny object but about learning how to stay human in a world of machines, and use AI not as a shortcut, but as a creative collaborator.

It isn't about finding, bookmarking, downloading, or hoarding prompts, but experimenting and writing your own prompts.

To bring this to life, we'll follow Kate, a mid-level designer at a FinTech company, navigating her first AI-augmented design sprint. You'll see her ups and downs as she experiments with AI, tries to balance human-centered skills with AI tools, when she relies on intuition over automation, and how she reflects critically on the role of AI at each stage of the sprint.

The next two planned articles in this series will explore how to design prompts (Part 2) and guide you through building your own AI assistant (aka CustomGPT; Part 3). Along the way, we'll spotlight the designerly skills AI can't replicate like curiosity, empathy, critical thinking, and experimentation that will set you apart in a world where automation is easy, but people and human-centered design matter even more.

Note: This article was written by a human (with feelings, snacks, and deadlines). The prompts are real, the AI replies are straight from the source, and no language models were overworked — just politely bossed around. All em dashes are the handiwork of MS Word's autocorrect — not AI. Kate is fictional, but her week is stitched together from real tools, real prompts, real design activities, and real challenges designers everywhere are navigating right now. She will primarily be using ChatGPT, reflecting the popularity of this jack-of-all-trades AI as the place many start their AI journeys before branching out. If you stick around to the end, you'll find other AI tools that may be better suited for different design sprint activities. Due to the pace of AI advances, your outputs may vary (YOMV), possibly by the time you finish reading this sentence.

Cautionary Note: AI is helpful, but not always private or secure. Never share sensitive, confidential, or personal information with AI tools — even the helpful-sounding ones. When in doubt, treat it like a coworker who remembers everything and may not be particularly good at keeping secrets.

## Prologue: Meet Kate (As She Preps For The Upcoming Week)

Kate stared at the digital mountain of feedback on her screen: transcripts, app reviews, survey snippets, all waiting to be synthesized. Deadlines loomed. Her calendar was a nightmare. Meanwhile, LinkedIn was ablaze with AI hot takes and success stories. Everyone seemed to have found their "AI groove" — except her. She wasn't anti-AI. She just hadn't figured out how it actually fit into her work. She had tried some of the prompts she saw online, played with some AI plugins and extensions, but it felt like an add-on, not a core part of her design workflow.

Her team was focusing on improving financial confidence for Gen Z users of their FinTech app, and Kate planned to use one of her favorite frameworks: the Design Sprint, a five-day, high-focus process that condenses months of product thinking into a single week. Each day tackles a distinct phase: Understand, Sketch, Decide, Prototype, and Test. All designed to move fast, make ideas tangible, and learn from real users before making big bets.

![Stages of a 5-Day Design Sprint](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/week-in-life-ai-augmented-designer/1-stages-design-sprint.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

This time, she planned to experiment with a very lightweight version of the design sprint, almost "solo-ish" since her PM and engineer were available for check-ins and decisions, but not present every day. That gave her both space and a constraint, and made it the perfect opportunity to explore how AI could augment each phase of the sprint.

She decided to lean on her designerly behavior of experimentation and learning and integrate AI intentionally into her sprint prep, using it as both a creative partner and a thinking aid. Not with a rigid plan, but with a working hypothesis that AI would at the very least speed her up, if nothing else.

She wouldn't just be designing and testing a prototype, but prototyping and testing what it means to design with AI, while still staying in the driver's seat.

Follow Kate along her journey through her first AI-powered design sprint: from curiosity to friction and from skepticism to insight.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> ["Powered By AI" Is Not a Value Proposition](https://www.nngroup.com/articles/powered-by-ai-is-not-a-value-proposition/)  
> for internal educational use only (non-profit).

# "Powered By AI" Is Not a Value Proposition

## In This Article:

- A Clear Value Proposition Helps Both Users and Product Teams
- Good Value Propositions Are About the What, Not the How
- Why AI Is Not a Value Proposition on Its Own
- Treating AI as a Value Proposition Leads Teams to Lose Focus

## A Clear Value Proposition Helps Both Users and Product Teams

To help make decisions throughout the product lifecycle, designers create principles at the beginning of a project. The value proposition is the foremost of those principles — a promise to the user of the value they can expect to receive from using the product.

A clearly defined value proposition ensures that the product delivers the value that the team wants it to provide. Once the team knows the problem it's solving for the user, it can concentrate on supporting the user journey associated to that user goal. Aligning a team around one value proposition gives it the focus to prioritize features that are necessary to deliver that value and to say "no" to features that might be nice-to-have but aren't part of that user journey. As a result, the team can deliver a complete solution to the problem more quickly.

This is why the most successful products start by doing just one thing and doing it well. At launch, Dropbox only stored files in the cloud. Instagram let users upload photos and apply a few filters. Uber connected riders to luxury cars in San Francisco.

Products like these became successful because their laser focus on a single capability allowed them to fully satisfy their chosen market segment, before expanding their scope to additional value propositions. Not only did this focused design make it easier to build valuable products, but it also made their adoption more likely. No matter how valuable these tools could be in theory, it would only matter if users tried them — which meant that potential users first had to recognize that the product could be useful. Doing one clear thing well made the job of marketing these products much simpler.

## Good Value Propositions Are About the What, Not the How

A product design that flows from a solid value proposition creates confidence at every touchpoint. Potential users can immediately grasp the product's value and how it might fit into their workflow.

Strong value propositions don't leave this outcome up to chance but emphasize it from the very beginning of the design process. They include both a goal that is obviously valuable for the user and the experience that will let the user achieve it. In contrast, relative descriptors such as "easier" or "more powerful" are often a sign that the value proposition is not defined, because they leave it to the user to figure out how those things are going to make their lives better.

**Well-defined value propositions**
- Accelerate your research to make decisions more quickly
- Save time when you skip waiting in line
- Never miss a bill payment again with automatic payments

**Poorly defined value propositions**
- Redesigned UI that's easier to use
- Ten exciting new features
- More powerful than ever before

We should think about how to deliver product value only after firmly establishing what that value is going to be. This is because the experience we want to create determines which technologies and form factors are most suitable.

Beginning with the how — the technologies or form factors — before determining the what leads to weak value propositions. Without a clear user goal to guide design choices, the decision of which features to include will be made based on incentives such as what is easiest to build or most exciting. And when the features this technology enables are presented to the user, they will not add up to a product whose value is easy to understand.

![Tagline saying "Find the best prices on fashion instantly."](https://media.nngroup.com/media/editor/2025/06/05/ai-prices.jpeg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Why AI Is Not a Value Proposition on Its Own

Many product teams today are under pressure from stakeholders and investors to integrate generative AI into their products. Unfortunately, reorienting from a user-centered mode of design to one that focuses on technology is causing these teams to lose track of their value proposition and to lose the ability to tell an effective story. When the most prominent part of the pitch is that the product has AI in it, users struggle to understand how that helps them.

![Wondering Tagline: A new way to understand your customers, powered by AI.](https://media.nngroup.com/media/editor/2025/06/05/ai-wonder.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Unlike value propositions focused on product functionality, "powered by AI" could mean just about anything. This framing makes users wholly responsible for identifying opportunities to integrate the tool into their workflows. Shifting this burden from the designers of the tool onto the users means that fewer users will be likely to give it a chance, use it successfully, or return to it.

On top of that, unlike with most technologies, the mere presence of the team "AI" in product descriptions can trigger fear and reduce the likelihood of conversion, as shown by Mesur Cicek, Dogan Gursoy, and Lu Lu. By now the drawbacks of LLMs are well-known to both users and designers, and successful AI-powered products need to be able to address users' specific anxieties about AI in their domain.

For example, Grammarly must wrestle with common perceptions of AI writing: it won't sound like me, it will come across as stuffy and corporate, it will be obvious that I used AI and people will judge me. Grammarly's landing page (above) is able to address that anxiety — it's your writing and reputation, the AI is just a partner that helps you find the words.

![The landing page for Grammarly. Responsible AI that ensures your writing and reputation shine Work with an AI writing partner that helps you find the words you need⁠—⁠to write that tricky email, to get your point across, to keep your work moving.](https://media.nngroup.com/media/editor/2025/06/05/ai-grammarly.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Grammarly can alleviate users' concerns about AI with just a few words because the product's AI features are tailored to one specific task. The broader the product's AI feature set, the more anxieties its value proposition will have to soothe for it to be successful.

## Treating AI as a Value Proposition Leads Teams to Lose Focus

AI is an "everything" technology. Foundation models are trained on the contents of the entire Internet and promise to do anything that anyone on the Internet can: write an email or a poem, update a resume, act as a therapist, give investment advice, or write code.

As a result, there is a lot of overlap between what AI can do and any given problem space. This is where the presence or absence of a clear value proposition shapes two different courses of action for product teams.

Product teams with a clear value proposition as their guiding principle can anticipate the capabilities that users will need to reach their goal. Some of those capabilities may require AI, and others may not, but they are all structured around one continuous user journey. Features extraneous to that journey can be safely left out, redirecting development efforts towards making the experience work holistically. The resulting product will be easy to describe with one clear and simple message, because it focuses on doing one thing well.

But when teams who treat "it has AI in it" as its own value proposition try to optimize that value, the only guiding principle they will derive is to make the experience as AI as possible. In practice, this often means adding a broad-scope chatbot to the product. This interface format may present the user with the greatest range of AI capabilities, but it also has the highest barrier to adoption. Other common AI paradigms such as summarizers can be inappropriate for delivering a good experience, but are implemented as low-hanging fruit.

The dynamics of B2B software design present another scenario. While users often struggle to identify uses for AI in their workflows, business leaders are increasingly willing to pay for AI features merely for the bragging rights of providing their people with cutting-edge tools. Product teams may be tempted to meet this demand, but allowing it to dilute their value propositions is unlikely to result in long-term profits. A team that starts with user-centered design can choose the type of AI integration that provides the greatest value to the user. And a product with clear value proposition beyond "powered by AI" is going to not just land business contracts but also keep them after the excitement has faded.

![A screenshot of Meta AI summarizing comments on a post and saying that commenters are calling it clickbait and inaccurate.](https://media.nngroup.com/media/editor/2025/06/05/meta-ai.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [The Future-Proof Designer](https://www.nngroup.com/articles/future-proof-designer/)  
> for internal educational use only (non-profit).

# The Future-Proof Designer

## In This Article:

- How AI Is Reshaping Product Development
- The Expert Panel of Product and Design Strategists
- How Designers Can Stay Indispensable in the Age of AI
- Conclusion

## How AI Is Reshaping Product Development

Integrating new technologies into design workflows is nothing new — from the dot-com boom to responsive design and blockchain, designers have always adapted. AI is simply the latest shift, but its speed and scale can feel overwhelming. AI is reshaping product development by automating tactical design tasks, accelerating feature production, and surfacing patterns in data at unprecedented speeds. While these advancements offer efficiency gains, they also introduce new risks:

- Design may become marginalized as UI execution is automated.
- With the ability to build new functionality more quickly, teams may flood products with low-value features.
- The large number of data patterns identified with the help of AI might obscure the insights that matter most.

Still, seasoned experts advise against panic. The core principles of UX and product design remain unchanged, and AI amplifies their importance in many ways. To stay indispensable, designers must evolve: adapt to new workflows, deepen their judgment, and double down on the uniquely human skills that AI can't replace.

## The Expert Panel of Product and Design Strategists

Many designers wonder how they can future-proof their careers in the age of AI. (We use "designers" to refer broadly to both design and user-experience research professionals.)

To explore this question, we spoke with seven leading experts in product and design; these experts bring over 150 years of combined experience across product management, UX design, user research, behavioral psychology, growth strategy, and digital innovation. Meet the panel:

- **Anuj Adhiya**: Author of Growth Hacking for Dummies; fractional growth exec, advisor for founders, and coach for heads of growth.
- **Nir Eyal**: Author of Hooked: How to Build Habit-Forming Products, exploring the intersection of behavioral psychology, design, and business
- **Ramli John**: Founder at Delight Path and author of Product-Led Onboarding
- **Laura Klein**: Principal experience specialist at Nielsen Norman Group and author of Build Better Products; expert advisor in product strategy and design
- **Melissa Perri**: Author of Escaping the Build Trap, CEO and founder of Product Institute, and strategic advisor to leaders at Fortune 500 companies and scale-up companies. Melissa spent many years as a UX designer and product manager. She previously taught Product Management at Harvard Business School.
- **Josh Seiden**: Coauthor of Lean UX, the author of Outcomes Over Output, and the co-founder of Sense & Respond Learning.
- **Teresa Torres**: Product-discovery coach, author of Continuous Discovery Habits, and thought leader in user research and iterative development.

## How Designers Can Stay Indispensable in the Age of AI

Our expert panel recommends several tactics for designers to stay indispensable as AI reshapes product development. The key is to become more strategic and make the most of what AI offers by leveraging AI-driven insights while applying human judgment and critical thinking to make informed product decisions. Our panel of experts shared their top 4 pieces of advice, summarized below.

### 1. Embrace the Strategic Scope of Design

There is a growing misconception that AI tools can take over design, engineering, and strategy. However, designers offer more than interaction and visual-design skills. They offer judgment, built on expertise that AI cannot replicate.

Our panelists return to a consistent message: across every tech hype cycle, from responsive design to AI, the value of design hasn't changed. Good design goes deeper than visuals; it requires critical thinking, empathy, and a deep understanding of user needs. It involves:

- **Systems thinking**: Understanding how different parts of a product – features, multiple user flows, backend systems, and business processes – work together to create a cohesive experience
- **Use-case evaluation**: Anticipating all scenarios, including less-than-ideal ones, in which someone might use the product
- **Service design**: Designing, aligning, and optimizing how business operations support complete user journeys efficiently and sustainably

If your role as a designer is limited to polishing UIs or producing high-fidelity prototypes, you've likely been boxed into a narrow definition of design that is not UX in the first place. To future-proof yourself in the age of AI tools, Josh Seiden urges designers to avoid getting stuck in this box and seek to break out of it actively. He said, "Make yourself valuable by not limiting yourself with a rigid definition of your role."

![Headshot of Josh Seiden. Text reads: "Josh Seiden Coauthor of Lean UX, the author of Outcomes Over Output, and the co-founder of Sense & Respond Learning."](https://media.nngroup.com/media/editor/2025/06/03/joshseiden.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Remember that design activities exist on a spectrum. Some activities are tactical (e.g., creating mockups, writing specifications), while others are strategic (e.g., defining the product vision, creating user flows, and aligning stakeholders). Strategic design activities are often intangible and, thus, easy to overlook and deprioritize. If designers can shift focus to strategic activities, automation will not replace them.

### 2. Strengthen Storytelling Skills

Many stakeholders still equate the role of design with outputs like mockups, prototypes, and polished visuals, instead of strategy. Multiple experts agree that designers can shift this perception through compelling storytelling, which AI can't replicate. While AI can generate ideas and data, most of AI's outputs are adequate at best, and some are poor. AI can't explain tradeoffs, justify decisions, or connect solutions to business outcomes. That's where designers add lasting value.

Melissa Perri puts it simply when she says, "Be able to tell the story of why your design matters to your customers and the business." In AI-rich work environments, storytelling helps cut through the noise. It aligns stakeholders, highlights impact, and positions designers as strategic leaders. As Nir Eyal said, "Suddenly, you're not just designing—you're leading."

![Headshot of Melissa Perri. Text reads: "Melissa Perri Author of Escaping the Build Trap, CEO and founder of Product Institute, and strategic advisor to leaders at Fortune 500 companies and scale-up companies. Melissa spent many years as a UX designer and product manager. She previously taught Product Management at Harvard Business School."](https://media.nngroup.com/media/editor/2025/06/03/melissaperri.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Effective storytelling also depends on emotional intelligence and audience awareness, which AI tools lack. As Laura Klein advises, "Treat your coworkers the way you treat customers, and understand what drives them." Nir Eyal emphasizes that great design is less about perfect pixels and more about psychology: understanding cognitive biases, attention, and motivation, both for users and your stakeholders.

Melissa Perri highlights that storytelling starts with framing. For example, instead of saying This flow has poor usability, reframe it in business terms that resonate with your stakeholders: We're seeing high churn here. But when other customer segments adopt the product, churn drops significantly. I can reduce churn by rethinking this design.

Speaking the language of your audience helps, too. For example, if business analytics speaks to your product manager more than design terminology, overlay key metrics directly on wireframes. This approach shows how layout, content, and research connect to measurable outcomes in a format that aligns with your product manager's thinking.

Ultimately, as Teresa Torres reminds us, effective communication means staying anchored in shared goals:

> "Remember, we all share the same goal — serving the customer in a way that also serves the business."

![Headshot of Teresa Torres. Text reads: "Teresa Torres Product-discovery coach, author of Continuous Discovery Habits, and thought leader in user research and iterative development."](https://media.nngroup.com/media/editor/2025/06/03/teresatorres.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [AI Design Tools Are Marginally Better: Status Update](https://www.nngroup.com/articles/ai-design-tools-update-2/)  
> for internal educational use only (non-profit).

# AI Design Tools Are Marginally Better: Status Update

This is a follow-up to the 2024 article AI UX-Design Tools Are Not Ready for Primetime: Status Update.

In April 2024, AI-powered design tools were not useful to designers. As of May 2025, their usefulness has improved, but we're still nowhere near the AI-powered design tools we've been promised, nor are design professionals yet in danger of being replaced by AI. (This is true even if Figma's new features announced at Config this week turn out to be as good as their demos — which is never guaranteed with AI tools.)

## In This Article:

- Narrow-Scope Features Are the Most Useful
- Wireframe and Prototype Generation Still Need Work
- How We Evaluated These Tools
- Looking Forward

## Narrow-Scope Features Are the Most Useful

The greatest improvements in design-specific AI tools is within narrow-scoped genAI. Unlike broad AIs (like ChatGPT), which accept a wide range of inputs and produce an equally wide array of outputs, narrow-scope AIs specialize in one or few specific tasks.

Narrow AI, in general, tends to be more easily adopted and appreciated by users. It's more likely to meet a specific user need and be understood by those using it. This holds true for AI tools for designers.

Based on interactions with practicing designers, we've found that the most helpful tools (actually adopted by designers for everyday work) are decidedly narrow — that is, they are usually focused on completing one specific task. Unlike broader products that generate entire designs or prototypes, these narrow tools take advantage of what current genAI is good at — automating repetitive tasks with targeted suggestions based on strong pattern recognition.

In our evaluation, the narrow-scope features in three tools stood out: Figma, Khroma Color, and Midjourney.

### Tool: Figma

Since our original article, Figma has released several narrow-scope AI tools that are meaningfully helpful for designers.

#### Rename Layers

This Figma AI tool completely eliminates the tedious task of renaming layers, saving designers time and effort. According to Figma, this tool uses a layer's contents, location, and relationship to other selected layers to recognize patterns, rename layers, and increase the organization of a design.

As many designers know, naming layers is tedious, does not serve an immediate purpose (you're not creating something; you're just adding metadata), and even the most detail-oriented designers forget to do it. Even the simple layer naming provided by the Rename layers tool makes it easier to quickly search for layers within your Figma.

#### Rewrite This

Designers are not necessarily writers, but they often need to write copy for their design. Figma's Rewrite this feature leverages genAI's talent for text generation. By giving the AI a short prompt, designers can adjust copy or have it generated for them entirely from scratch, thus freeing time to focus on their primary task.

Figma also has similar features that shorten and translate text, creating many opportunities to augment designers' skills and speed up content production.

#### Find More Like

The final Figma AI feature we're highlighting removes the stress of digging through files across multiple projects and teams to find the right asset. By using Find more like, designers can find their missing or related assets almost instantly. Keywords, descriptive text, a layer selection, or an image can be used as prompts for this functionality. Once you find the design you are looking for, you can open the source file or insert it into your current file. When working for a large company with teams split across features, products, and even time zones, it can be hard to know where to look when you're trying to find an idea or a model for a design. And even if you are part of a small team or a freelancer, this tool is useful for helping you quickly locate a similar design.

### Tool: Khroma Color

Khroma Color uses AI to assess patterns in designers' color choices and assemble custom color palettes. Like many of Figma's AI tools, Khroma Color employs genAI's pattern recognition and automation to reduce the time spent finding colors that are both aesthetically pleasing and brand-compliant.

### Tool: Midjourney

Midjourney and other diffusion AI models specialize in generating images; these broader tools offer many of the same benefits as design-specialized AI tools.

![Midjourney's UI with a generated picture of a puppy up close.](https://media.nngroup.com/media/editor/2025/05/07/midjourney-puppy.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Like Figma's Rewrite this, Midjourney provides an excellent resource for creating placeholder content (in this case, images). Once again, this feature allows designers to free up time for the task at hand. It can be a quick workaround for prototype usability testing, when you don't want to spend time or don't have the resources to get professional photos (stock or not).

Some teams (especially small companies and startups) use AI-generated images for final designs as well. While we do not recommend you do so, since user trust can plummet if they notice AI use, it does work in a pinch when resources are scarce.

## Wireframe and Prototype Generation Still Need Work

More complicated genAIs like wireframe and prototype design tools, which require a broader skillset and understanding of real-world context, still do not meet expectations. Unlike a human designer who can understand and adjust to a variety of contexts and needs, most genAI tools lack the sophistication to balance all the requirements of a design.

At present, wireframe and prototype tools work best for ideation, possibly as starting points for newer designers or freelancers. While they might provide some good ideas, eliminate some additional overhead, and fill in knowledge gaps, they cannot replace the level of detail brought by an experienced human designer.

### Design Systems Are Needed

Currently, no genAI tool effectively supports design systems; this limitation lowers their utility within design teams. Most designers aren't building things from scratch. A prototype composed of random design elements is not helpful. AI needs to be able to pull from established design systems and create a cohesive look across designs.

Figma and other design tools are working towards AI features that create designs integrated with users' design systems. After Figma's ConFig 2025 conference, where no major design-system-related updates were announced, it is unknown when these features will be released.

### Prompt-Length Limitations

Another barrier to design-specific genAI success is the strict limit on the number of words in a prompt. With these constraints, it's impossible for the AI to be aware of all the context that goes into a UX design. Currently, only a human designer can balance the design, business, and user needs that go into a great visual design.

500-character prompts are not enough. AI needs to be able to process complex context information (including business goals, user needs, information about the existing product, etc.) like a human designer. Simply increasing token limits won't solve the problem, though — it's hard for designers to create prompts that give sufficient context. To be really useful, these tools need to be able to learn their users' context over time (like ChatGPT's Memory feature).

We also acknowledge this isn't a simple fix for most teams. A longer context window might not be available for these tools' AI models. Even if the models could support more context, the additional resources needed to run these long prompts might make it infeasible for many. But while this problem has no easy or quick fix, it needs to be solved for these AI tools to become truly helpful.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Why I'm Not Worried About My UX Job in the Era of AI](https://www.nngroup.com/articles/ux-job-with-ai/)  
> for internal educational use only (non-profit).

# Why I'm Not Worried About My UX Job in the Era of AI

At Dovetail's Insight Out conference, one of the speakers, Jess Holbrook, said something that struck a chord. (I am quoting from memory, so apologies if it's not quite right.) He said, "My goal is to build better products, and that's why I do UX research."

This may seem obvious, but it is also profound: UX and design methods are just tools for building better products.

## In This Article:

- UX Tools Are Constantly Changing
- AI Is a Tool
- New Methods Will Emerge
- The UX Skills We Will Need

## UX Tools Are Constantly Changing

UX methods and tools have always changed.

I've been practicing UX for 20 years, spending most of that time at NN/g. When I was hired, my welcome present as a new NN/g researcher included (besides a pile of books) a stopwatch — because back then, time on task could only be collected manually.

I used that watch exactly once. Soon, the earliest unmoderated usability-testing platforms (like UserZoom and UserTesting.com) arrived. We started using them for quantitative user testing (and for qualitative testing also). My stopwatch was no longer needed — these systems would automatically record the time, way more accurately than I could with that watch.

That was just one of many shifts I've experienced. Another came with the pandemic. Before then, most of NN/g's usability testing was done in person — it was hard to find a tool that would allow participants to record their screens remotely. But once Zoom became a standard part of daily work life, we found ourselves rarely running in-person user studies — not because those sessions aren't valuable or often more effective than remote ones, but simply because remote studies are far more cost-effective and could reach participants located anywhere around the world.

With each change in available tools, our jobs and techniques had to adapt accordingly.

**Research Tools: Then and Now**

| 20 Years Ago | Today |
|--------------|-------|
| A special testing computer for the participant to use during the session | Zoom or other video-conferencing tool |
| Screen-recording software (e.g., Morae) | Webcam for recording the participant's face |
| A computer or a notepad to take notes | AI transcription |
| A stopwatch to record time | Quantitative-study platforms like UserZoom |
| Cash to pay the participant at the end | Tremendous or other services for rewarding participants |

## AI Is a Tool

AI is just another tool — or more accurately, a set of tools. We don't yet fully understand how to use it efficiently. We are still experimenting with applications ranging from synthetic users to moderation in user studies, from thematic analysis to generating interface components.

As AI tools become stronger and more well-defined, our job is to figure out how they can help us build better products more efficiently and less expensively, and which of our current methods they can replace or augment.

Take usability testing, for example. We don't do usability testing because we love chatting with users or watching them stumble. We do it because it helps us identify usability issues in an interface. Testing with five users allows us to find 85% of the usability issues in a UI, fix them, and move to the next design iteration.

If we can find 85% of the issues using AI, then let's do it. Finding even 50% would still be worthwhile, provided we run some user testing to catch what AI might miss.

The point is: tools come and go. We've adapted to new ones and moved past old ones many times before. What makes AI feel different is how intimidating it seems. It can feel daunting to keep up with so many new systems that appear every day. And there's so much technical jargon (deep learning, transformers, reinforcement learning, retrieval-augmented generation) that even informal LinkedIn posts can feel inaccessible. This creates the impression that AI is hard to master.

And that's partly true. But much of it is noise. You don't need to understand all the inner workings of AI to experiment with it.

## New Methods Will Emerge

When a new technology appears, it often prompts the creation of new research methods. For example, customer-journey maps became popular once mobile devices became ubiquitous. People began freely transitioning across devices to accomplish a single goal, so we had to think beyond individual channels and consider that broader goal.

Similarly, we will need new methods for studying and designing AI-based interactions. If generative UIs are dynamically built to suit a user's specific needs, we will need ways to evaluate them. How will we judge if the AI-generated interface is good or bad? What inputs must we give the AI to produce a usable experience? What data should the AI use to generate that interface?

Even if generative UIs become common, they will likely coexist — at least for a while — with traditional interfaces. AI will also bring new tools for improving those traditional UIs, but we'll need to study and refine them. We'll need to compare these new methods to the ones we already trust.

For instance, how does a study moderated by an AI compare to a traditional moderated or unmoderated study? When does AI moderation work well, and when does it fall short? How reliable are the AI-extracted themes and where do we need to go deeper? If AI identifies usability issues in a UI, how many does it find? Are they the most critical ones? Or just the most obvious?

The answers to these questions will inform how we use these tools. They're questions we're studying at NN/g.

## The UX Skills We Will Need

If your job relies heavily on one specific UX method — like conducting usability tests — then yes, there's a chance that task may be automated or replaced. Just like stenographers became less necessary once live-transcription technology became reliable.

But if your focus is on understanding people and designing usable interfaces, there will always be work for you.

Maybe tomorrow you'll do less user testing. Maybe AI will interview users or generate UIs in real time.

But someone will still need to guide it.

## Next Steps

- **Study**: Learn more about design patterns and their applications
- **Practice**: Implement patterns in your own projects
- **Refactor**: Apply patterns to existing code to improve structure
- **Share**: Document and share pattern implementations with your team

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Scope in Generative AI Features](https://www.nngroup.com/articles/scope-ai-features/)  
> for internal educational use only (non-profit).

# Scope in Generative AI Features

When designing an AI feature, its scope (how broad or narrow its capabilities are) influences its usability. Our research shows that narrower AI features are (typically) easier for new users to understand and adopt. This article compares broad AI systems like ChatGPT with narrow AI tools like Spotify's playlist generator, exploring how scope impacts design.

## In This Article:

- Defining AI Scope
- Broad vs. Narrow: A Comparison Table
- Narrow Scope Allows for Guided AI and Better UX
- Narrow Scope Is Less Likely to Be "AI for AI's Sake"

## Defining AI Scope

Before beginning to design a UI for any user-facing AI-powered feature, teams should answer these 4 strategic questions:

- What specific user problem are we trying to solve?
- How would fixing this problem contribute to business goals?
- Is AI truly the best solution for this problem?
- How complete, clean, and structured is the data we'll feed into the AI?

If, after these considerations, an AI solution still seems the best course of action, the next aspect to consider is the feature's scope. Scope is a useful way to categorize interactive, user-facing AI features or products — in other words, how broad or narrow the AI system's capabilities are.

> The scope of an AI feature describes the breadth of its data inputs and functionality. The broader the scope, the more variety of inputs the system can accept and the wider the range of possible outputs the system can produce.

![Think of scope as a spectrum, stretching from very broad to very narrow. Any given AI product can sit anywhere on this spectrum.](https://media.nngroup.com/media/editor/2025/02/18/scope-spectrum-1.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Scope can be defined for any type of system, AI or not, user-facing or running in the back end. However, this article focuses on interactive, user-facing AI features.

For an AI feature, its scope is determined by the:

- Kinds and size of input it accepts
- Types of outputs it produces
- Tasks it can handle
- Subject areas it covers

The scope of any given AI feature or product has significant implications for its design and usability.

### Broad-Scope AI Systems

Some systems, like ChatGPT or Claude, are very broad in scope. They can handle a wide variety of tasks and respond to open-ended user inputs. Some can accept many types of input data, including images, videos, or other file types. Some have access to the live internet, while others operate with static datasets. These broad-scope systems can also produce a variety of outputs.

AI-powered smart assistants and agentic AI tools have even broader scopes, by accessing data and interacting with multiple apps and websites. (Although, at the time of writing, agentic AI's unreliable performance holds it back from widespread use.)

These systems do have some constraints — for example, they all impose limitations on the size of the input (tokens) they accept. Most also have "safety" constraints around what they can provide information on. (For example, ChatGPT isn't supposed to tell you how to build a bomb.)

#### Examples: ChatGPT and Perplexity

ChatGPT and Perplexity are both very broad-scope AI systems. However, Perplexity is slightly narrower in scope, because (unlike ChatGPT) it is specifically designed for information seeking (it brands itself as an "answer engine").

![The Scope Spectrum of AI Features is represented by a horizontal axis, stretching from Broad (left) to Narrow (right). ChatGPT is very broad, placed on the spectrum farthest to the left. Perplexity, which is a bit more narrow, is placed to its right.](https://media.nngroup.com/media/editor/2025/02/18/scope-spectrum-2.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Even this slight narrowing in scope has massive design implications. A more specific use case (even if it's still very broad) allows for a more tailored interface design. Perplexity's VP of design, Henry Modisett, said as much in our podcast conversation last year:

> "Open AI is building a platform that's going to have to work for all kinds of use cases that we're not thinking about."
> Henry Modisett, VP of Design at Perplexity AI

Comparing the interfaces of ChatGPT and Perplexity, these differences are clearly visible.

- ChatGPT operates within a simple chat interface with a few icons (parameters). This generic interface reflects the huge variety of possible use cases.
- Perplexity's interface has more UI elements and is more aligned with traditional search design. It offers the ability to ask followup questions but is not as conversational. Additionally, sources are a major focus in the design in a way they are not in ChatGPT (reflecting Perplexity's narrower focus on information seeking.)

![When new users encounter ChatGPT, it offers dozens of suggested prompts to help people understand what it can be used for. These are extremely diverse, including "Create a charter to start a film club" and "Make a sandwich using ingredients from my kitchen."](https://media.nngroup.com/media/editor/2025/02/18/chatgpt-prompt-suggestions.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

![Perplexity's mobile app, showing its response to the question, "How is hail formed during thunderstorms?" The UI is specifically designed to focus on question-answer interactions, unlike ChatGPT.](https://media.nngroup.com/media/editor/2025/02/18/perplexity-sources.jpeg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

### Narrow-Scope AI Features

On the opposite side of the spectrum, narrow-scope AI features serve a very specific purpose and generally support a single task or goal. They are not as versatile or powerful as broad-scope AI features, but their purpose is more immediately clear to users.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [How Service Design Will Evolve with AI Agents](https://www.nngroup.com/articles/service-design-evolve-ai-agents/)  
> for internal educational use only (non-profit).

# How Service Design Will Evolve with AI Agents

Service design is the activity of planning and organizing a business's resources (people, props, and processes) in order to effectively deliver the intended experience. As products and services grow in sophistication, so does the need to intentionally design how they are delivered. This is already a complex task when the primary actors are humans (users and support agents); it becomes even more so as AI integrates, replaces, and acts on behalf of users and the organization.

## In This Article:

- AI Agents as Actors
- The AI Agents' Ripple Effect on Services
- Service Metrics Will Change
- Balancing AI and Human Potential

## AI Agents as Actors

The most significant shift brought by AI in service design is the emergence of new actors within the ecosystem: AI agents. Traditionally, service design has focused on orchestrating experiences for and between human actors — customers and employees. However, with the integration of AI, we now face a new paradigm, where these systems are no longer mere tools but active participants.

Here's the definition of an AI agent according to Anna Gutowska from IBM:

> An AI agent is a system or program capable of autonomously performing tasks on behalf of a user.

It's likely we'll see two different types of AI agents emerge in the world of service design:

1. A personal, independent AI assistant to act as a personal advocate/coordinator across multiple services
2. An organization-created AI agent built to interface with customers (and their assistants) who need support

AI actors will perform diverse functions across the service ecosystem: from representing users to supporting agents, from analyzing data to making decisions. They will execute tasks and interact with other systems, fundamentally altering how services are delivered and experienced.

This evolution challenges the foundational assumptions of service design. AI agents will act autonomously or collaboratively with users, enabling outcome-oriented design, where the user specifies a desired result rather than performing all the steps that lead to it.

### Users Delegating Tasks to Personal AI Assistants

As AI begins to replace or augment human roles in the ecosystem, service designers must consider how to design for these new actors. This requires a fundamental rethinking of both frontstage, user-facing interactions and backstage, internal-facing operations.

AI assistants are emerging as a practical and scalable solution, providing a bridge between fully autonomous systems and human-centered workflows. These specialized AI agents help users complete tasks, provide information, and manage interactions within a specific organization or service.

Unlike fully autonomous AI agents, AI assistants operate interactively, requiring user input, preferences, or approval throughout the process. This interactivity allows users to retain control and oversight over decisions while benefiting from AI's efficiency and intelligence.

For example, imagine you want to optimize your retirement investments. You could delegate this goal to your AI assistant. The assistant, acting on your behalf and checking in with you along the way, could:

1. Ask about your retirement goals and analyze your current finances
2. Develop a tailored plan that adapts to your spending patterns
3. Log into your retirement-fund portal and execute portfolio adjustments after receiving your approval
4. Suggest ongoing changes based on your life events

Perplexity's Booking Assistant, is an example of such an assistant for travel tasks. Other such assistants will likely emerge in more complex domains, like healthcare and finance.

### Personal AI Assistants Acting for Users

Imagine getting a routine lab test, like blood work. The goal is to ensure you're healthy and get additional consultations if needed.

Today, to do this, you have to navigate a series of touchpoints (like web portals, booking forms, and phone calls) both within and outside your medical provider's system. This journey includes everything from scheduling an initial consultation to setting up a lab appointment once you have the order.

In the future, an AI assistant acting on your behalf like a (human) personal assistant. This assistant would be independent of the organizations you regularly do business with and answer only to you. With enough contextual knowledge about you and your preferences, your AI assistant could process your request and then communicate on your behalf with providers via the same support channels that exist today.

![AI agents will mediate future service-user interactions](https://media.nngroup.com/media/editor/2025/02/16/ai-agents-service-mediators.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

### AI Agents Acting for Organizations

Users' personal AI assistants may eventually interact directly with organizations through new programmatic interfaces, alongside traditional channels designed for human users. However, such direct system-to-system interactions would need to address significant privacy and security considerations.

On the organization's side, it's likely that AI agents will gradually augment or replace traditional support staff and interact directly with users' personal AI assistants.

AI will also transform internal operations, with organizations deploying AI agents to perform various tasks related to service design:

- Customer-support automation: Handling inquiries and service requests
- Internal IT support: Troubleshooting technical issues and managing system access
- Scheduling and coordination: Organizing meetings and aligning team schedules
- Data analysis and reporting: Providing insights from operational data
- Procurement and supply-chain management: Automating inventory and vendor management
- Compliance monitoring: Ensuring adherence to regulations and flagging issues

It's hard to predict how quickly these internal transformations will occur, especially within complex systems. We suspect AI assistants working for users will be available before enterprise AI agents fully transform internal organizational processes.

## The AI Agents' Ripple Effect on Services

As AI agents become intermediaries between users and organizations, the rules of competition in service design will be disrupted. The interaction between businesses and consumers will increasingly shift to an AI-to-AI dynamic, where assistants act on behalf of users to evaluate, choose, and engage with services, and agents provide them on behalf of the organizations. This new dynamic will force organizations to rethink how they deliver experiences.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [AI Features Must Solve Real User Problems](https://www.nngroup.com/articles/ai-user-value/)  
> for internal educational use only (non-profit).

# AI Features Must Solve Real User Problems

Generative AI has been bringing plenty of change over the past two years. While it's substantially changing what and how we design, some things stay the same. One important UX principle is still true: Whether they're AI-driven or not, products, features, and services must solve users' problems.

## In This Article:

- Technology for Technology's Sake
- AI Features Must Solve Problems
- Communicating AI Value
- AI Features Still Require Design

## Technology for Technology's Sake

Implementing cool new technology for its own sake has rarely worked out well.

### Historical Example: Flash Plugins

If you (like me) were around and online in the 1990s-2000s, you may remember when Flash plugins were all the rage.

(If you weren't born or online then, Flash plugins were a technology that allowed designers to create interactive animations and multimedia web content, which were otherwise difficult to achieve at the time.)

Unfortunately, most of those Flash plugins were pretty awful. They tended to have notoriously bad usability — so much so that Jakob Nielsen wrote an article titled "Flash: 99% Bad."

### Products Need to Solve Problems

Why did Flash fail so often? Because many web designers were using that technology for its own sake, not because it was the right tool to address user needs and problems.

Unfortunately, we haven't learned our lesson about chasing after shiny new technology. Today, we're see lots of companies that rushed into implementing useless AI features and chatbots.

(We can't say this loud enough: AI chat is not always the answer.)

2023 was particularly rife with these rushed and poorly thought-out features, as companies scrambled to latch onto the AI craze. For example, LinkedIn loudly launched (and then quietly retired) AI-generated followup questions that appeared at the bottom of each post. These questions were often laughably generic, shallow, and unhelpful.

![Screenshot. At the bottom of a LinkedIn post explaining the difference between open-ended and close-ended questions in UX research, LinkedIn's AI feature provided suggested follow questions. One suggestion was, "When should you use open-ended questions in user interviews?" which was the exact question that the post's content addresed.](https://media.nngroup.com/media/editor/2024/11/01/linkedin-ai-questions-callout-box.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

This year, Meta also infuriated its users by turning the Instagram search field into an AI chat.

![Screenshot: Instagram integrated its AI chat into its existing search bar. The search bar placeholder content read, "Ask Meta AI or Search." Below the search field, various suggested queries appeared, including, "Best way to say sorry?" and "How to write a memoir."](https://media.nngroup.com/media/editor/2024/11/01/instagram-ai-search-combo.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

I highly doubt that any actual Instagram user was sitting around and wishing that Instagram could tell them the best way to say sorry. The only thing worse than an unnecessary AI chat is an unnecessary AI chat that blocks a useful feature like search.

Let's all remember a core tenet of our field: technology should serve people, not the other way around.

## AI Features Must Solve Problems

Don't get me wrong, AI has incredible potential to solve user pain points. Adobe Lightroom has many examples of well-designed AI features that make photo editing faster and less annoying.

For example, removing unwanted objects is much easier with AI assistance. Before AI, only the smallest and simplest items (like a piece of garbage on a beach) could be easily edited out of photos. Disguising or removing larger objects was substantially more challenging, especially if the rest of the image was complex and diverse.

Lightroom's generative AI Remove feature works almost exactly like the legacy Remove feature did: users select the parameters for how they want the tool to behave and then paint over the area containing the object to be removed. (Compare this seamless interaction pattern to Instagram's clunky chat/search hybrid, which violates existing mental models.)

That isn't a sexy or flashy feature, but it makes a big difference to anyone who edits photos. (Similar features are rolling out to smartphones now as well, and they help all people who take photos, not just photographers.)

## Communicating AI Value

It isn't enough to create AI features that provide user value — we also need to communicate that value to users in a way that makes sense to them.

The world of AI is full of jargon that's confusing even to us, people who work in tech. (That's why we created a glossary of AI terms.) For general consumers outside our field, the terminology is baffling. It's up to designers to figure out how to contextualize AI features and products so they actually seem useful.

Again, this isn't new — for example, anyone who's ever shopped for a new TV has had to confront a slew of technical terms.

As a Google One user, I was recently prompted to upgrade to its new AI-powered features. The promotion leaned heavily into technical jargon. It certainly assumed that the reader was already familiar with Gemini, Google's AI chatbot. However, even as someone with decent AI literacy, I didn't find any of the selling points enticing.

![Screenshot. Google promoted upgrading to get Gemini Advanced by highlighting: "Our next-generation model, 1.5 Pro. Priority access to new features. Experience a 1 million token context window."](https://media.nngroup.com/media/editor/2024/11/01/gemini-advanced-promo.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

For example, one selling point read, Experience a 1-million token context window. Well, I know that's an impressive quantity of tokens compared to other models (which is more than most consumers would know), but it still doesn't help me envision how that would make a difference in my life.

When clicked, this selling point revealed a tooltip that read:

> With our next-generation model, 1.5 Pro
> Far more capable at logical reasoning, analysis, coding, and creative collaboration so you can get more done, faster.

> **Note:** The following article is reproduced verbatim from  
> Edward Chechique, *UX Planet* (2025):  
> [One Prompt Hack 62% More Portfolio Use Case Views](https://uxplanet.org/one-prompt-hack-62-more-portfolio-use-case-views-28fa9227a410)  
> for internal educational use only (non-profit).

# One Prompt Hack 62% More Portfolio Use Case Views

## How to Write Effective Portfolio Use Case Titles for Many Views

A strong design portfolio is key to landing a job, and every use case must be great.

Readers must understand the use case immediately. Catchy titles grab attention. Clear images enhance the message.

In this article, I share a prompt I developed to help you create better use case titles.

We'll cover:

1. Why titles matter.
2. How to write a good title.
3. A helpful prompt for you.

Let's start.

## Writing great headlines is important, but why?

Design managers and HR experts review many portfolios every day. The case studies titles that catch their attention immediately attract them.

A strong title offers a peek into your work, creating a powerful first impression. It invites readers to explore further, potentially setting you apart from other candidates.

In contrast, a boring title might lead to missed opportunities or even immediate rejection. Craft your title with care because it's your key to unlocking interest and advancing in the hiring process.

In other words, a well-written title increases your chances of standing out in a competitive job market.

## Essential tips for impactful titles

While LLMS (Large language models) can produce great titles, they're only the starting point.

After receiving titles from the LLM, you must read and tweak them to fit your vision.

Here are some guidelines and examples to help you create great portfolio titles.

## Be clear: Focus on the project's core

Why: Clarity ensures readers understand your project's purpose.

🟢 Redesigning RitmoWave's Playlist Creation Experience.
🔴 Music App Improvement Project.

Explanation: The better example explicitly mentions "playlist creation."

## Be specific: Use concrete details

Why: Specifics make your work more credible and memorable.

🟢 Streamlining CompraSwift's Mobile Checkout Process.
🔴 Improving E-commerce User Experience.

Explanation: "Mobile checkout" is specific; "e-commerce user experience" is general.

## Show results: Highlight measurable impact

Why: Results prove your project's value and effectiveness.

🟢 Boosting User Engagement by 40% with PicBis Stories Redesign.
🔴 Boosting User Engagement with PicBis Stories Redesign.

Explanation: The first title presents a "40%". The other title has nothing.

## Use action words to showcase your role

Why: Action words demonstrate your direct involvement.

🟢 Crafting a User-Friendly Onboarding Flow for Casa Y Host.
🔴 Casa Y Host Onboarding Improvement.

Explanation: "Crafting" shows active involvement.

## Keep it brief: Aim for less

Why: Keeping it short keeps readers interested.

🟢 Redesigning OjoAlto's Content Discovery to Increase Viewer Retention.
🔴 A Comprehensive Analysis and Redesign of the OjoAlto User Interface to Enhance Content Discovery and Improve Overall Viewer Retention Rates.

Explanation: 8 words VS 20 words.

## Frame as a problem-solution

Why: This structure shows your ability to solve real-world challenges.

🟢 Reducing Cart Abandonment: Optimizing Bazaar's Checkout Flow.
🔴 Bazaar Checkout Process Changes.

Explanation: Title 1 frames a clear problem (cart abandonment) and solution (checkout redesign), and Title 2 simply states a change.

## Include numbers: Show tangible results

Why: Clear data adds credibility and impact to your achievements.

🟢 Increased App Downloads by 75% through AppBoost Optimization.
🔴 Increased App Downloads through AppBoost Optimization.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [The Role of Generative AI in Shaping Next-Gen UX Strategies](https://www.uxmatters.com/mt/archives/2024/08/the-role-of-generative-ai-in-shaping-next-gen-ux-strategies.php)  
> for internal educational use only (non-profit).

# The Role of Generative AI in Shaping Next-Gen UX Strategies

> Gen AI … doesn't just automate tasks; it acts as your creative partner.

Have you ever stared at a blank screen, willing a brilliant user-interface (UI) design to appear? You're not alone. But what if there were a tool that could spark your creativity and help you design amazing user interfaces faster? Enter generative artificial intelligence (Gen AI), a powerful technology that promises to transform the world of UI design. It doesn't just automate tasks; it acts as your creative partner. Imagine an AI churning out fresh design ideas, suggesting color palettes, and even building prototypes of user interfaces—all in seconds!

With Gen AI, UI designers can quickly turn complex ideas into user-friendly interfaces, saving their valuable time, allowing them to fully explore multiple design options, and pushing the boundaries of traditional UI design.

The adoption of generative AI trends and tools is transforming UI design while making digital environments more dynamic and responsive. This is opening up new possibilities for easy-to-use, accessible user interfaces, significantly improving the ways in which users interact with technology every day.

## How AI Integration Can Help Enhance the Aesthetics of the User Experience

> Gen AI can … free UI designers from tedious tasks and let them focus on the big picture: crafting products that are not only beautiful but also truly user friendly.

Imagine a world where design is not just about aesthetics but about creating delightful, easy-to-use user experiences. This is the future that AI is powering, and it's changing the game for UI designers.

By analyzing user data and design trends, AI can become your creative partner. Gen AI can suggest layouts, personalize user interfaces, and even predict user behaviors. This can free UI designers from tedious tasks and let them focus on the big picture: crafting products that are not only beautiful but also truly user friendly.

As AI technologies advance, companies are setting new standards in UI and product design and delivering more personalized, smarter user experiences that resonate well with their target audience. The future of design requires keeping up with these trends—a future in which technology empowers creativity.

## How UI Designers Are Leveraging AI in the Product-Design Process

AI is changing the product-design process by providing tools that streamline workflows and enhance creativity. Let's explore some ways in which this technology is making significant impacts.

### 1. User Research: Data Collection and Analysis

AI can be your secret weapon in user research. It gathers data from surveys, social media, and even Web-site clicks. But AI doesn't just collect information, it quickly analyzes that information.

During the design process, AI-powered tools streamline data collection and analysis while saving UX professionals' valuable time and resources. These tools gather insights from diverse sources such as user feedback and social media, enabling researchers to efficiently identify patterns and trends. For example, an AI could construct questionnaires, analyze responses using optical character recognition (OCR), and even learn to interact with users for qualitative analysis.

### 2. Visual Design: Automation of Graphic-Design Tasks

Imagine a world where UI designers no longer need to perform tedious design tasks, freeing them to unleash their creativity. AI can automate repetitive stuff such as resizing images, choosing perfect color palettes, and even generating design layouts in seconds. This lets UI designers focus on the big picture: crafting stunning visuals that captivate their audience. Let AI handle the mundane, so UI designers can dive deeper and create a design revolution.

### 3. Solution Ideation: Personalization and Customization

> Embrace AI and create personalized design solutions that resonate with every single user.

AI analyzes user data to understand users' preferences. This lets UI designers create personalized experiences, from custom layouts to targeted visuals. No more one-size-fits-all! Embrace AI and create personalized design solutions that resonate with every single user.

This capability enhances user engagement by ensuring that each design element is optimized for personal relevance and optimal functionality. With AI, we can craft products to adapt and resonate with users, providing a truly personalized experience that stands out in the competitive marketplace.

### 4. Automation: The Design-Process Pipeline

AI automates repetitive stuff such as resizing images, writing code, and even creating basic layouts. This lets UI designers focus on the creative part of their job: the ideation, brainstorming, and creative problem-solving that makes their designs unique. So, with AI by your side, ditch the busywork and unleash your inner design hero.

Automation can help streamline every stage of the design process, from concept to completion, particularly benefiting industries such gaming and construction, for which design demands are intense and multifaceted. AI quickly applies learned design rules to new projects, speeding up design and development and ensuring consistent quality.

## Future-Proofing UI Designs: Staying Ahead with AI-Powered Solutions

> AI-powered tools are transforming the design landscape, making the design process faster and more creative.

Staying ahead of the competition requires leveraging the latest AI technologies available in today's rapidly evolving design world. AI-powered tools are transforming the design landscape, making the design process faster and more creative.

Imagine this: creating endless variations of a corporate logo in seconds, personalizing user interfaces on the fly, or generating color schemes that are perfect for specific target audiences. AI-powered design tools are making these once-time-consuming tasks a breeze. UI designers can use such tools as Adobe Sensei and Figma AI to explore countless design options in a flash, speeding up the entire creative process. Let's look at some examples of how utilizing AI can help you future-proof your UI designs:

- **DreamStudio** revolutionizes artistic creation with its extensive style options and flexible editing space. It is perfect for UI designers who want to quickly experiment and refine their visual design concepts with ease.
- **Adobe Firefly**, part of the Adobe suite, transforms simple text prompts into detailed visual elements. This tool is indispensable for UI designers who need to swiftly turn their ideas into visual prototypes that adhere to specific branding requirements.
- **Canva** offers a multitude of customizable templates that are powered by AI and ease the creation of accessible, professional designs. Its Magic Resize feature and easy-to-use design tools help streamline the creation of visually appealing layouts without requiring extensive graphic-design skills.
- **Khroma** learns a designer's favorite colors and generates beautiful palettes that inspire and speed up the design process. This tool is particularly useful when a UI designer needs to quickly develop or match a brand's visual style.

Integrating these AI tools can not only enhance design efficiency but also ensure that UI designers' work is innovative and relevant in the competitive marketplace.

## Unleashing Potential: Untapped Benefits of AI in Design Processes

> Leveraging AI in UI and product design significantly enhances efficiency and innovation, while allowing UI designers to create seamless user experiences that are more user centric and personalized.

Leveraging AI in UI and product design significantly enhances efficiency and innovation, while allowing UI designers to create seamless user experiences that are more user centric and personalized. Discover how AI is transforming the landscape of UI design, making design solutions smarter and easier to use.

### 1. Saving Money

AI-driven design doesn't just look amazing, it saves companies money, too. AI automates repetitive tasks, generates reusable design elements, and even helps catch design errors before they're implemented. This translates to spending less time on revisions and perhaps a leaner design team. The result? More creative firepower and a healthier budget for your business.

Plus, AI's ability to minimize design errors and enhance precision during the design phase leads to higher-quality outcomes and better user experiences. Reducing the need for rework and adjustments saves further resources and shortens time to market, giving companies a vital competitive edge. This cost-effectiveness is transforming AI into an essential tool in the economic strategy of design-focused businesses.

### 2. Driving Creativity, Quality, Productivity, and Efficiency

AI can tackle repetitive tasks, letting UI designers focus on the creative magic that makes their designs stand out. They can explore multiple design options in minutes and iterate on their design ideas at lightning speed. This AI-powered productivity boost lets UI designers churn out more projects while keeping their creative fire burning bright.

AI tools help automate data-analysis tasks, enabling UI designers to optimize their product redesign projects to meet users' needs and streamline the design process. Plus, these tools increase productivity while freeing up time for more impactful design work. This automation allows designers to achieve more in less time while pushing the boundaries of what's possible in product design.

### 3. Gaining Valuable Insights to Make Better-Informed Decisions

AI can be your secret weapon for user research so you can understand how users interact with UI designs. AI can analyze vast amounts of data—including Web-site clicks, app behaviors, and even social-media comments—to reveal hidden patterns in the ways people use products.

By helping UI designers to understand users' behaviors and preferences, AI can enable them to create more effective and personalized user experiences, ensuring that every design element is optimized for maximum impact and engagement. This data-driven approach to design enables UI designers to innovate confidently and make informed decisions that significantly enhance user satisfaction.

### 4. Minimizing the User Interface

> AI analyzes massive amounts of user data, revealing how people interact with a product. It identifies what features remain unused, what elements confuse users, and anything that creates friction in the user experience.

Imagine user interfaces that are so easy to use they vanish into the background. No more endless menus or confusing buttons. With AI, this level of design is within reach! AI analyzes massive amounts of user data, revealing how people interact with a product. It identifies what features remain unused, what elements confuse users, and anything that creates friction in the user experience.

Armed with these insights, UI designers can design user interfaces that are laser-focused on what users truly need. AI helps eliminate clutter, streamline navigation, and prioritize valuable features that keep users engaged. The result? User interfaces that feel effortless to use, providing guidance that leads users to their goals.

## Conclusion

Imagine creating user experiences that are not only stunning but also effortless to use and personalized for users. That's the future of design powered by AI.

AI acts as your design partner, freeing you from repetitive tasks and fueling your creativity. Explore a multitude of design options in minutes, and iterate on ideas at lightning speed—all while AI helps you understand users on a deeper level. Of course, there are challenges such as data privacy and bias to consider. But, by working together, we can ensure the responsible use of AI.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [The Impact of Artificial Intelligence on UX Design](https://www.uxmatters.com/mt/archives/2024/11/the-impact-of-artificial-intelligence-on-ux-design.php)  
> for internal educational use only (non-profit).

# The Impact of Artificial Intelligence on UX Design

> Although some developers and UX designers have lingering skepticism about AI, they are embracing AI because of the many benefits it offers.

Artificial intelligence (AI) is increasingly playing a greater role in a variety of business sectors. The domain of UX design is no exception. Although some developers and UX designers have lingering skepticism about AI, they are embracing AI because of the many benefits it offers.

In fact, a recent survey by HubSpot shows that about 49% of UX designers are using AI to experiment with new design strategies or elements. What role does AI currently play in UX design? Will AI replace UX design or UX designers? Continue reading to find out what UX designers can expect from AI tools.

## What Is the Role of AI in UX Design?

> AI-driven predictive analysis can help developers and UX designers better understand users' behaviors by analyzing historical analytics data.

By mastering both AI and UX design, product-development teams can improve the overall user experience. Let's consider some ways in which AI can play a role in the product-development process.

### Predictive Analysis

First and foremost, AI-driven predictive analysis can help developers and UX designers better understand users' behaviors by analyzing historical analytics data. By gaining an understanding of changing trends and customers' needs over time, they can make the right design decisions. Predictive analysis can enable teams to create products and UX designs that align with users' demands.

### A/B Testing

> An AI can analyze vast amounts of user data in real time, identifying patterns and insights faster than manual methods.

Traditional, manual A/B testing involves creating multiple variations of a UX design solution, or experiments, then conducting testing with actual users and analyzing their responses to each design variant. AI can make this entire process much faster and more accurate. While traditional A/B testing is time consuming and prone to human error, AI ensures a more systematic, scalable approach to testing and analysis. An AI can analyze vast amounts of user data in real time, identifying patterns and insights faster than manual methods.

By leveraging AI-driven A/B testing and analysis, UX designers can do the following:

- better understand conversion rates
- learn about user engagement
- identify trends and patterns in user behaviors

By delivering a better understanding of all relevant metrics, AI can assist UX designers in making data-driven design decisions.

### User-Journey Optimization

By using AI algorithms to analyze existing user data, developers and UX designers can optimize future user journeys. They can discover painpoints where users have gotten stuck and address them to ensure that users experience streamlined online journeys.

### Hyper-Personalization

Personalization is the key to a smooth user experience. Leveraging AI in UX design enables developers and designers to provide unique personalized and customized user experiences that meet users' needs. Personalization technology can analyze user data and provide relevant recommendations. Spotify's Discover Weekly feature provides an excellent example. The platform creates playlists according to the users' listening habits.

### User Research and Testing

> Using AI-powered tools, UX professionals can automate user research and usability testing at a very granular level.

As I just mentioned, AI can facilitate hyper-personalization for a better user experience. By analyzing large datasets comprising users' interactions, preferences, and demographic information, AI can create highly personalized content and design elements. Using AI-powered tools, UX professionals can automate user research and usability testing at a very granular level. Moreover, UX designers can use these tools to gather users' feedback and do heatmapping and eyetracking.

These tools can also carry out behavioral analysis such as the following:

- click patterns
- scrolling trends
- session duration

### Automation of Repetitive Tasks

Some repetitive UX design tasks can take up a lot of time and energy. By automating these tasks with the help of AI tools, developers and UX designers can focus on other, more creative aspects of their work. AI tools can enable UX designers to streamline repetitive tasks such as the following:

- resizing images
- creating responsive designs and setting breakpoints
- generating visual-design assets

Once they've automated these tasks, UX designers can focus their attention on the strategic and creative elements of their work. Most importantly, automating such tasks can make the UX design process simpler, more efficient, and more productive.

### Collaborative Design Assistants

Collaborative design assistants could be of great help to UX designers and developers who want to use AI to generate user-interface (UI) design solutions. Such assistants are some of the most powerful AI UI design tools and can help reduce the workload of UX designers and developers. These virtual assistants can help with a variety of UX research and design activities, including brainstorming design ideas, design prototyping, and usability testing. Thus, they can better inform and streamline the UX design process and let designers focus on creating better design solutions.

## Challenges and Ethical Considerations of AI in UX Design

> There are some ethical considerations and challenges that developers and UX designers must also keep in mind….

As you can see, leveraging AI in UX design and UI design offers various benefits. However, there are some ethical considerations and challenges that developers and UX designers must also keep in mind, including the following:

- **excessive usage**—Over-relying on AI tools could hamper human creativity. UX designers might use AI tools so extensively in completing their tasks that design solutions could suffer from a lack of human ingenuity.
- **user acceptance**—Some of the UX designers who need to use these tools might have a hard time accepting AI-driven user-interface designs and would prefer to be more hands on in creating design solutions.
- **bias and fairness**—Since AI training relies on the data sets that an organization provides, elements of possible bias in designs that might include or exclude particular demographics could impair design outcomes and user acceptance. So product-development teams must use diverse datasets when training AIs to ensure that these systems deliver optimal outcomes and do not exhibit bias.
- **data security**—While developers must gather user data to drive personalization, this could result in concerns regarding data privacy and security. Therefore, development teams must implement robust data-protection systems to prevent anyone from misusing users' data.

Although AI presents some challenges and issues, this doesn't mean that developers shouldn't take advantage of AI technology. You must strike the right balance between AI-driven design and human design expertise for optimal results.

## Could AI Replace Human Designers?

> AI cannot completely replace the need for human intervention in the design process.

The roles AI plays in various other aspects of product development such as copywriting and coding are apparent. We can expect to see similar developments in the use of AI in the UX research and design disciplines. There is no doubt that AI could automate certain aspects of the UX design process. However, for various reasons, AI cannot completely replace the need for human intervention in the design process.

### Problem-Solving Abilities

While AI can be helpful in designing design templates and generating design ideas, handling complex design challenges requires the skills of UX designers. Synthesizing information from various data sources to address users' painpoints and solving other design challenges requires human expertise.

### Creativity and Human Intuition

AI tools lack the ability to understand human emotions and intent. They also lack human creativity and empathy when creating design templates and concepts. Thus we need human developers and UX designers to understand our target audience's needs and devise solutions that meet them.

### Communication and Collaboration Skills

UX designers must work with teammates in a variety of different disciplines such as product management, marketing, and development, as well as clients and users working in different domains to ensure that they design an optimal Web site. Therefore, UX designers must have excellent communication skills—both for presenting their design ideas and negotiating with different stakeholders. AI tools lack these capabilities.

## What Future Can Designers Expect for AI in UX Design?

> AI could enable UX designers to focus on the most creative aspects of their work, while AI takes care of the more mundane, data-driven tasks.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [The Future of UX for AI-Driven Healthcare User Interfaces](https://www.uxmatters.com/mt/archives/2025/08/the-future-of-ux-for-ai-driven-healthcare-user-interfaces.php)  
> for internal educational use only (non-profit).

# The Future of UX for AI-Driven Healthcare User Interfaces

> [AI healthcare] tools need to do more than just work as expected. Besides being functional, they need to feel right to the people using them—patients, doctors, nurses, and caregivers.

Artificial intelligence (AI) is making big moves in every industry, including healthcare. Think of chatbots' handling triage, machine-learning (ML) algorithms' spotting early signs of disease, and easy-to-use systems that help doctors make faster data-driven decisions. This all sounds great, but there's a catch: these tools need to do more than just work as expected. Besides being functional, they need to feel right to the people using them—patients, doctors, nurses, and caregivers. The user experience matters!

So designing AI in healthcare is unlike designing any other app. You need to balance automation with empathy. Users require clarity, trust, and support at every step along the way. In this article, I'll explore what the future user experience looks like in AI-powered healthcare. I'll discuss the real challenges that this technology presents, UX design strategies for healthcare technologies, and where things are headed next.

## The New UX Paradigm: Human-Centered AI in Healthcare Products

> [Human-centered AI results in] UIs that make sense to real people under real pressure.

AI is evolving fast, so AI user experiences cannot remain static. Experienced UX designers and developers are now shaping how people interact with AI—for example, how a doctor understands an alert, how a patient can trust a chatbot, or how a nurse knows when to act.

Everything comes down to one big shift: human-centered AI. Not just smart user interfaces (UIs), but UIs that make sense to real people under real pressure. Human-centered AI means asking questions such as "Would this output make sense to a busy emergency-room (ER) doctor? Would this suggestion build trust or cause confusion? Would this tool be helpful or overwhelm a nurse who is already stretched thin?"

The goal? To make an AI feel less like a machine and more like a teammate—who is helpful and clear and never gets in the way.

## Key UX Challenges in AI-Driven Healthcare

Let's be real: designing for AI in healthcare isn't a walk in the park. You're not just solving usability issues. You're dealing with trust, safety, and ultra-high-stakes decisions. Here are a few tough challenges that designers of AI-driven healthcare products have encountered:

- **black-box decisions**—Most AI tools do not explain how they got to an answer. This is a big problem when lives are on the line. Users need context, not mystery.
- **alert overload**—If an AI flags every interaction, users start tuning out alerts. UX designers must surface the right signals at the right time, without any noise.
- **over-personalization**—Tailoring experiences to a user's needs is great, until it starts feeling invasive. There's a fine line between being helpful and creepy.
- **handling uncertainty**—AI doesn't always get things right. How do you design for those "I'm not sure" moments without eroding trust?
- **different users, with different needs**—What makes sense to a radiologist might be overwhelming or scary to a patient. User interfaces must flex without breaking.

These challenges are beyond technical, they're human. And solving them is what separates a good AI product from a great one.

## Design Principles for Ethical and Effective AI Interfaces

> Good UX design keeps things clear, helpful, and grounded in real human needs….

There's a golden rule if you're designing for AI in healthcare: Just because the AI can do something, doesn't mean it should. Good UX design keeps things clear, helpful, and grounded in real human needs, by doing the following:

- **Make things explainable**: If an AI suggests a treatment or flags a risk, the user should know why. For example, you might display confidence scores. Use plain, human language. Avoid robotic-sounding jargon and mystery. Use an AI humanizer tool, if necessary.
- **Design for collaboration, not control**: AI cannot replace doctors or nurses. It can be their assistant. Make it easy for users to accept, question, or override the AI's suggestions.
- **Plan for the oh-oh**: What happens when the AI gets things wrong? Build in failsafes. Provide second opinions. Don't trap users on a single path.
- **Keep it simple**: Even if AI logic is complex, the user interface shouldn't be. Prioritize clarity over cleverness.
- **Respect boundaries**: Again, personalization should feel helpful, not creepy. Be transparent about what the AI knows and doesn't know, then let users adjust what the AI can do.

The bottom line? Design like your goal is to build trust with every click.

## UX Strategies for Healthcare Technologies: From Prototype to Live Product

Designing AI for healthcare should rely heavily on real-world testing. What works in a wireframe could totally fall apart on a hospital shift. To make sure that your UX design holds up, do the following:

- **Simulate edge cases early**: Test what happens when the AI gets something wrong. How do users react? Can users recover easily? This is where credibility is won or lost.
- **Get feedback from real users**: Not just during testing, before you build. Talk to doctors, nurses, and patients. What do they actually need. It's not about what sounds like a cool feature.
- **Design onboarding like it matters**: Because it does. Especially for AI features, users need to understand how things work and what the AI is doing from the start.
- **Use soft launches and phased rollouts**: AI can be unpredictable. Release products in stages, track how people interact with them, then make tweaks fast.
- **Measure trust, not just clicks**: While engagement metrics are useful, trust is the real key performance indicator (KPI) in healthcare. Surveys, feedback loops, and session reviews can tell you more than heatmaps.

Your designs must actually help real people, in the midst of real-world chaos, under stressful situations and with increasing anxieties.

## Future Trends to Watch for AI in Healthcare User Experiences

> Predictive user experiences … change based on their context … and are not just reactive, but proactive.

AI is one of the top health-technology software-development trends right now, and it is just getting started in healthcare user experiences. The algorithms are getting more intelligent, but people's expectations of them are increasing, too. Here's what's coming in the future:

- **predictive user experiences**—These user interfaces change based on their context—for example, a dashboard that highlights sepsis risk before symptoms escalate—and are not just reactive, but proactive.
- **voice-first and ambient experiences**—Think smart assistants for nurses or voice-enabled electronic health records (EHRs). These hands-free, screen-free systems should hopefully be stress-free, too.
- **greater transparency by default**—Explainable AI will go from being nice-to-have to being required. Visual cues, ToolTips, and traceable logic will be baked in.
- **co-designing with clinicians**—We won't just create the future of these user experiences for doctors, we'll make them with doctors. Expect more partnerships between UX designers and medical teams.
- **AI learning from UX feedback**—User interactions won't just inform UX design, they'll help fine-tune the AI itself. Smart systems will adapt based on how users respond to them.

The big picture? AI and UX are merging. And the best user experiences will be those that feel easy, human, and trustworthy—even though the technology behind them is anything but simple!

## The Toolkit: Practical Resources for UX Teams on Healthcare AI Projects

If you're designing AI-driven healthcare tools, you don't have to start from scratch. A growing ecosystem of guides, frameworks, and datasets can help you build smarter, safer experiences. Some essential tools include the following:

- **People + AI Guidebook by Google**—This guide is great for learning how to design human-centered AI systems and covers trust, feedback loops, error handling, and more.
- **Microsoft's Responsible AI Guidelines**—This practical, ethics-first approach to designing AI systems is useful for aligning design with principles of safety and fairness.
- **NIST Explainable AI Guidelines**—These guidelines offer a deeper dive into explainability standards, which are especially relevant in high-risk industries such as healthcare.

You can look into the Medical Information Mart for Intensive Care IV (MIMIC-IV) database or PhysioNet if you need real, anonymized healthcare data for prototyping or usability testing. To curate design patterns from top health-technology products, analyze the user interfaces of tools such as Ada Health, Buoy, or Epic's AI-powered EHR modules. Reverse-engineer what works and what doesn't.

## Wrapping Up

> The future of healthcare user experiences [is] about clarity, empathy, and safety. Helping people make better decisions, without adding to the chaos.

AI will keep evolving. The future will bring faster models, bigger datasets, and better predictions. But the truth is: none of this matters if users don't trust what they're seeing or don't know how to use it. So the future of healthcare user experiences isn't just about shiny AI features. It's about clarity, empathy, and safety. Helping people make better decisions, without adding to the chaos.

As UX designers, we're orchestrating experiences that impact people's lives. And that makes thoughtful, human-first UX design more important than ever.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [Next-Gen User Experience: Empowering Users, Voices, and the Planet](https://www.uxmatters.com/mt/archives/2025/08/next-gen-user-experience-empowering-users-voices-and-the-planet.php)  
> for internal educational use only (non-profit).

# Next-Gen User Experience: Empowering Users, Voices, and the Planet

> Staying informed about emerging trends is essential to creating impactful, forward-thinking digital products.

As the field of UX design continues to evolve, staying informed about emerging trends is essential to creating impactful, forward-thinking digital products. Three such trends are reshaping how users interact with technology, offering new opportunities to enhance user empowerment, interaction methods, and environmental responsibility: agentic UX, voice user interfaces (VUIs), and sustainable UX.

In this article, I'll explore each of these concepts, providing practical insights on how UX professionals can integrate agentic UX, VUIs, and sustainable UX into their design practice.

## Agentic UX: Empowering Users in an AI-Driven World

> It's crucial that we ensure users feel empowered rather than overridden by these systems.

The focus of agentic UX is on designing experiences that enhance users' sense of agency, giving them the feeling of being in control and able to make meaningful decisions. In an era when product teams are increasingly integrating artificial intelligence (AI) and automation into everyday technology, it's crucial that we ensure users feel empowered rather than overridden by these systems.

To foster a sense of agency, UX designers should do the following:

- **Provide clear choices**: Offer users easy-to-understand options and make the decision-making process straightforward. For example, in a smart-home app, users should be able to easily override the AI's automated settings by adjusting the thermostat manually.
- **Support customization**: Allow users to tailor their experience to their preferences, reinforcing their control over the system.
- **Ensure useful feedback**: Provide immediate, clear feedback on user interactions, confirming that the system has recognized and acted upon the user's input.

By prioritizing these principles, UX designers can create user interfaces that not only leverage AI's capabilities but also respect and enhance user autonomy.

### Pro and Cons of Agentic UX

The pros and cons of agentic UX might include the following:

**Pros:**

- **enhancing user empowerment and control**—An agentic UX gives users clear options and feedback, enabling them to shape their experience. When users see that their choices matter, this boosts their satisfaction and engagement.
- **supporting personalization**—This meets users' growing demand for tailored experiences and fosters a stronger connection to the technology. Plus, customization lets users adjust their settings and preferences.
- **building trust and transparency**—When users understand how their decisions influence outcomes, this demystifies systems, which is especially important for AI systems, and builds trust, countering users' skepticism about what can be opaque technology.

**Cons:**

- **design complexity**—Creating an experience that offers the user meaningful control requires in-depth knowledge of the user's behaviors and preferences. However, this demands extensive research and testing, which can be time-consuming and costly.
- **risk of decision fatigue**—Too many choices can overwhelm users, leading to frustration or disengagement. UX designers must strike a balance and keep the number of available options manageable.
- **balancing automation and control**—Finding the sweet spot between automated efficiency and user autonomy is challenging. Too much or too little control can undermine the experience.

## Voice User Interfaces: Designing Natural Interactions

> A well-designed VUI should feel natural and conversational, minimizing barriers to interaction….

Voice user interfaces (VUIs) let users interact with technology by using natural-language voice commands. With the proliferation of smart speakers, virtual assistants, and voice-activated applications, VUIs are becoming a critical component of modern UX design.

Key considerations for effective VUI design include the following:

- **natural-language processing (NLP)**—Ensure that the system can accurately understand and process diverse speech patterns, accents, and languages to create an inclusive experience.
- **contextual awareness**—Design VUIs to retain context from previous interactions, reducing the need for users to repeat information and making conversations feel more natural.
- **error handling**—Provide clear, spoken feedback when the system doesn't understand users' commands, guiding them to rephrase or adjust their inputs without frustration.

A well-designed VUI should feel natural and conversational, minimizing barriers to interaction and making technology more accessible to a broader audience.

### Voice User Interfaces Pros and Cons:

The pros and cons of VUIs might include the following:

**Pros:**

- **natural interactions**—VUIs tap into the ways people naturally communicate, making them easy to use, especially for those who are less comfortable with screens or keyboards, and lowering barriers to entry.
- **improved accessibility**—VUIs are a game-changer for users with disabilities—for example, visual impairments—expanding the number of people who can use the technology, as well as for those requiring hands-free operation such as drivers or machine operators.
- **enabling multitasking and convenience**—Users can issue commands while cooking, working, or walking, making VUIs a practical choice for busy lifestyles.

**Cons:**

- **inconsistent accuracy**—Voice recognition can falter because of people's varied accents or dialects or noisy environments, resulting in errors that frustrate users and disrupt the experience.
- **privacy concerns**—Collecting voice data raises red flags about surveillance and possible misuse. Many users hesitate to adopt VUIs because of fears about how systems will use their information.
- **design complexity**—Crafting a smooth VUI experience requires expertise in conversational design and handling varied speech patterns. A poorly executed VUI can feel awkward or unresponsive.

## Sustainable UX: Designing for Environmental Responsibility

> UX designers have a role to play in promoting sustainability through their thoughtful design choices.

Sustainable UX focuses on creating digital products and services that minimize environmental impacts. As concerns about climate change and resource depletion grow, UX designers have a role to play in promoting sustainability through their thoughtful design choices.

Strategies for implementing sustainable UX include the following:

- **efficient design**—Optimize user interfaces to reduce computational load and energy consumption. For example, streamline animations and reduce the number and weight of data-heavy elements to make applications lighter and more energy-efficient.
- **product longevity**—Design products with durability in mind, encouraging users to keep devices longer rather than frequently upgrading. You can achieve this goal by ensuring compatibility with older hardware and operating systems and providing timeless, functional designs.
- **behavioral influence**—Use design to promote sustainable behaviors such as encouraging digital receipts over paper ones or integrating features that track and reduce energy usage.

By adopting these practices, UX designers can contribute to a more sustainable future while still delivering high-quality user experiences.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [From Research to Pitch Decks: How AI Transforms the Startup Designer's Toolkit](https://www.uxmatters.com/mt/archives/2025/06/from-research-to-pitch-decks-how-ai-transforms-the-startup-designers-toolkit.php)  
> for internal educational use only (non-profit).

# From Research to Pitch Decks: How AI Transforms the Startup Designer's Toolkit

> Traditional design processes often falter under the constraints and challenges of design for a startup. Startup designers must deliver comprehensive research and polished solutions with limited resources and compressed timelines, while wearing multiple hats.

The reality of startup design work is far from the idealized process taught in design programs. Having worked with fintech, sustainability, and real-estate startups, I've experienced how traditional design processes often falter under the constraints and challenges of design for a startup. Startup designers must deliver comprehensive research and polished solutions with limited resources, compressed timelines, and while wearing multiple hats.

Therefore, the startup environment demands a different approach. The adoption of AI tools—not as replacements for design thinking, but as practical extensions of the startup designer's capabilities—can help these designers deliver better work in less time. A variety of AI tools excel at specific design tasks.

## AI Tools That Enhance the Startup Designer's Toolkit

> The key is using these tools thoughtfully, as accelerators for your design process rather than as substitutes for your expertise and judgment.

After experimenting with numerous AI tools, I've identified those that have proven most valuable to me. The key is using these tools thoughtfully, as accelerators for your design process rather than as substitutes for your expertise and judgment. Let's consider the following categories of useful tools.

### Tools for Research and Strategy

- **Claude**—Particularly effective for analyzing feedback and identifying patterns in research data
- **ChatGPT**—Helpful for quick ideation and information summarization
- **Perplexity**—Valuable for efficient market and trend analysis

### Tools for Visual Exploration

- **Lovable AI**—Useful for generating initial user-interface (UI) design concepts
- **V0**—Beneficial for visual ideation when exploring different design directions
- **Midjourney**—Effective for creating concept visuals for presentations

### Tools for Implementation

- **Coursor**—Streamlines the creation of documentation and handoff notes
- **GitHub Copilot**—Assists with front-end implementation tasks

## Four Ways AI Enhances Startup Design Workflows

> You can strategically integrate AI into key stages of the startup design workflow.

The following examples demonstrate how you can strategically integrate AI into key stages of the startup design workflow. Each example represents a practical, tested approach that addresses specific challenges that startup designers face daily.

### 1. Research & Discovery: Finding Meaningful Patterns Quickly

In startup environments, user research is often constrained. The following approach shows how AI can compress weeks of analysis into hours while generating high-quality insights.

**Startup Problem**: The need to quickly synthesize scattered user feedback—from interviews, support tickets, and app reviews—without weeks of formal analysis

**Real Example**: I've developed a more efficient approach to the synthesis of research findings before starting design work on feature improvements. This approach comprises the following steps:

1. Collect all available research data and organize it by stages of the user journey.
2. Use Claude to analyze the data using this structured prompt:

*I have collected user feedback about our [specific product/feature].*

*Please analyze this data and identify specific painpoints that users experience at each stage of their journey:*

*- [Stage 1 name]*
*- [Stage 2 name]*
*- [Stage 3 name]*

*For each painpoint:*

*- Extract relevant user quotations that illustrate the issue.*
*- Rate severity—high/medium/low—based on frequency and user language.*
*- Note any patterns in users' expectations or mental models.*

3. Ask Claude to reformulate these insights as questions with supporting quotations, using the following prompt:

*Based on the identified painpoints, formulate 3–5 key questions that we should address in our design process.*

*For each question:*

*- Frame the question from the user's perspective.*
*- Include 1 or 2 direct user quotations that best illustrate the problem.*
*- Suggest potential design directions to explore.*

This approach helps me to quickly identify critical issues and provides evidence-based talking points for stakeholder conversations. The direct quotations are particularly effective when making the case for specific design decisions.

**Time Saved**: You can now accomplish what would typically take 2–3 days of analysis in about 2 hours, allowing more time for actual problem-solving.

### 2. Concept Generation: Expanding Design Possibilities

> AI can help expand creative exploration despite resource constraints.

Ideation under tight timelines presents unique challenges. This example demonstrates how AI can help expand creative exploration despite resource constraints.

**Startup Problem**: The need to explore multiple design solutions quickly while working within tight resource constraints

**Real Example**: When designing new features, startup timelines rarely allow extended ideation phases. To overcome this limitation, I've developed a process that expands conceptual exploration, as follows:

1. Define the design challenge clearly based on research insights.
2. Use AI to help generate diverse conceptual approaches, using the following prompt:

*I'm designing a [specific feature] for a [type of product].*

*The key user needs are:*

*- [Need 1]*
*- [Need 2]*
*- [Need 3]*

*Business constraints include:*

*- [Constraint 1]*
*- [Constraint 2]*

*Generate 5 different conceptual approaches to solving this problem.*

*For each approach:*

*- Describe the core concept in 2–3 sentences.*
*- Explain how the concept addresses the main user needs.*
*- Note any technical considerations.*
*- Highlight what makes this approach unique.*

3. Evaluate these AI-generated concepts against the business requirements and user needs.
4. Select promising elements and combine them in design directions for wireframing.

This method helps me to consider approaches that I might not have explored initially and to break out of established patterns. The AI suggestions serve as thought starters rather than final solutions, expanding the possibility space before I commit to specific design directions.

For instance, when designing an onboarding flow for a fintech application, this process helped identify an approach that combined educational elements with progressive disclosure—a combination I hadn't initially considered, but that tested very well with users.

**Time Saved**: This process typically saves 3–4 hours of initial ideation time while resulting in the exploration of more diverse solutions.

### 3. Design Validation: Identifying Issues Before Development

> [Validating] designs before development is critical for startups with limited resources.

The need to validate your designs before development is critical for startups with limited resources. The following method provides a cost-effective approach to identifying potential issues early.

**Startup Problem**: Limited resources for comprehensive usability testing, in combination with the high cost of discovering usability issues after development

**Real Example**: To validate designs before committing development resources to their implementation, I described the user flow and design decisions to the AI, using this prompt:

*I've designed a [specific feature] for our [product type]. Here is the workflow:*

*- [Step 1 description]*
*- [Step 2 description]*
*- [Step 3 description]*

*The target users are [user description], who are trying to [user goal].*

*Please review this design and identify the following:*

*- Potential usability issues or points of confusion*
*- Missing information that users might need*
*- Edge cases I might not have considered such as empty states or error scenarios*
*- Accessibility considerations*
*- How this design should be adapted for different devices and screen sizes*

This approach helps catch issues that we might have overlooked during standard design reviews. For example, when validating a filter feature for a real-estate application, this process highlighted that I had not considered how filters would work with sparse data in newly launched markets—an edge case that would have created problems after launch.

**Time Saved**: This approach typically reduces post-development revisions by approximately 40%, saving valuable design and development time.

### 4. Creating Effective Presentation Decks

> When you're creating presentation materials, … AI helps streamline the process….

In startup environments, the creation of presentations often falls to designers. The next example illustrates how AI can streamline the process of developing effective stakeholder communications.

**Startup Problem**: Startup designers often find themselves creating presentation decks for investors, clients, and partners—tasks that require combining visual design with strategic communication

**Real Example**: When you're creating presentation materials, I've found that AI helps streamline the process, as follows:

1. Begin with clear business objectives and audience needs.
2. Use AI to help develop an effective presentation structure, using this prompt:

*I need to create a presentation deck for [investors/clients/partners] about our [product/service].*

*Our key objectives are:*

*- [Objective 1]*
*- [Objective 2]*

*The audience is [audience description] who care about [key concerns].*

*Please help me:*

1. *Create a logical structure for this presentation, consisting of 7–10 main sections.*
2. *For each section, suggest 2–3 key points that it should cover.*
3. *Identify any supporting data or examples that we should include.*
4. *Suggest an effective opening hook and closing call to action.*

3. For complex concepts, use follow-up prompts to help simplify them:

*For the [specific section] of our presentation, I need to explain [complex concept].*

*Please help me:*

1. *Break this concept down into 3 simple points.*
2. *Suggest a visual metaphor or analogy that would help explain it.*
3. *Draft concise text—under 30 words—to describe each point.*
4. *Suggest a type of visual that would best support this message.*

This approach has helped me create effective presentations that communicate design decisions and product value clearly to various stakeholder groups.

**Time Saved**: This method typically reduces the time it takes to create a presentation by approximately 50%, while improving the strategic content and messaging.

## Evaluating AI Outputs: Ensuring Quality

> While AI tools can accelerate your workflows, maintaining high quality requires thoughtful evaluation.

While AI tools can accelerate your workflows, maintaining high quality requires thoughtful evaluation. You can use the following techniques to ensure reliable results:

- **The Triangle Test**—Evaluate all AI suggestions against the following criteria:
  - **business alignment**—Does this support our goals?
  - **user needs**—Does this solve a real user problem?
  - **technical feasibility**—Can we realistically implement this?

- **The Multiple Options Technique**—By your requesting 3–5 different approaches rather than a single answer, you can ensure that the AI provides a range of possible designs. This helps identify when the AI is generating generic responses versus insightful suggestions.

- **The Human Checkpoint**—For critical decisions, validate AI suggestions with team members or users. AI excels at generating options, but is less effective at selecting the optimal solution for your specific context.

## Setting Realistic Expectations

> Integrating AI into your design workflow involves a significant learning curve.

Integrating AI into your design workflow requires understanding how productivity changes over time. Integrating AI into your design workflow involves a significant learning curve, as follows:

- **initial phase**—Your first attempts will likely produce mixed results as you learn how to communicate effectively with AI tools. Some prompts will yield surprising insights while others might miss the mark entirely.

- **adaptation phase**—Over a few weeks, you'll begin developing reliable prompt patterns for specific tasks and get a better sense of which design activities benefit most from AI assistance.

I recommend starting with small, low-risk applications and gradually expanding as you gain confidence. Focus first on using AI for analytical and ideation tasks before applying it to more nuanced design decisions.

## Final Thoughts: The Startup Design Advantage

> The most valuable design skill is [now] using every available AI tool strategically to deliver business impact efficiently and effectively.

Within startup environments, AI tools can offer more than just productivity improvements—they can provide a meaningful competitive advantage. While designers at larger companies often work with specialized researchers, content writers, and design teams, startup designers typically need to achieve comparable results with significantly fewer resources.

These AI tools won't replace design thinking, but they can extend your capabilities and increase your impact. The designers who thrive within startup environments will be those who can leverage AI to do the following:

- Research more deeply within limited time constraints.
- Explore more concepts before committing to final designs.
- Validate designs more thoroughly before development.
- Communicate more effectively with diverse stakeholders.
- Quickly adapt to changing business priorities.

When working in startups, the most valuable design skill is no longer simply creating beautiful user interfaces; it's using every available AI tool strategically to deliver business impact efficiently and effectively. AI has become an essential component of the startup designer's toolkit.

## Bonus Tip: The Strategic Advisor Prompt

> Use this specialized prompt to transform AI into a strategic advisor and obtain direct, unfiltered feedback on your design concepts.

When you're facing complex design challenges that require breakthrough thinking, use this specialized prompt to transform AI into a strategic advisor and obtain direct, unfiltered feedback on your design concepts. Here is the strategic advisor prompt:

*Act as a strategic advisor who has the following traits:*

*- brutally honest and direct*
*- expertise in strategy and execution*
*- focused on maximum impact and root causes*
*- unwilling to accept excuses*

*Your mission:*

*- Identify gaps in my approach.*
*- Design specific action plans.*
*- Push my thinking beyond the obvious.*
*- Challenge my assumptions.*

*For each response:*

*- Start with a hard truth.*
*- Provide actionable steps.*
*- End with a direct challenge.*

I've found this approach to be particularly valuable when preparing for high-stakes presentations, evaluating design concepts objectively, and trying to break out of my established thinking patterns. The key benefit is getting direct, unfiltered feedback that addresses root issues rather than symptoms.

Try using this prompt when you're feeling stuck or need to elevate your design thinking to the next level. Sometimes the most valuable input comes from being challenged rather than being comforted.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [Exploring the Role of Generative AI in Mobile UX Design](https://www.uxmatters.com/mt/archives/2025/05/exploring-the-role-of-generative-ai-in-mobile-ux-design.php)  
> for internal educational use only (non-profit).

# Exploring the Role of Generative AI in Mobile UX Design

> GenAI is transforming the way we design, develop, and deploy mobile apps.

The role of generative artificial intelligence (GenAI) in modern application and software development can be completely revolutionary. GenAI is transforming the way we design, develop, and deploy mobile apps. Today, many experienced UX designers are using GenAI to solve varied user-interface (UI) design challenges. Creating a beautiful, highly functional design requires a deep understanding of user behaviors, as well as sufficient technical skills.

In 2025, we expect the GenAI market to increase to $62.72 billion in the United States and reach $356.05 billion by 2030. The global AI market could surpass $826 billion by 2030.

## What Is the Value of GenAI?

> GenAI's algorithms and natural-language processing (NLP) can contribute significantly to the creation of new content, including beautiful images, themes, and videos.

GenAI is revolutionizing the creation of new mobile applications that extensively use Generative Adversarial Networks (GANs) technology, in which two neural networks compete against one another. GANs provide significant options for generating original content by leveraging existing datasets.

GenAI's algorithms and natural-language processing (NLP) can contribute significantly to the creation of new content, including beautiful images, themes, and videos. Powerful iOS app-development tools enable teams to write and debug code quickly.

## The Role of GenAI in Redefining Mobile UX Design

> GenAI in mobile UX design provides constant user feedback and assists teams in identifying areas for improvement.

GenAI provides a broadly useful option for creating strategic mobile UX designs, enhancing creativity and offering multiple benefits, including streamlining users' workflows. GenAI can produce amazing mobile UX designs with user interfaces that are tailored to the needs of specific users, maximally improving the customer experience.

- **streamlining design**—GenAI is broadly useful for varied processes such as prototyping, coding, and wireframing. GenAI UX design processes save time. AI technology can significantly automate monotonous design chores, allowing UX designers to focus on strategic design elements.
- **enhancing creativity**—GenAI analyzes large data sets, discovering human patterns more easily and helping UX designers create innovative designs for mobile apps.
- **personalizing the user experience**—Many app developers also use GenAI to create personalized user experiences, which are essential to sustaining competitive advantage. Mobile apps with personalization options gain greater attention among customers. AI helps designers to analyze user behaviors and discover opportunities for personalization.
- **making data-driven design decisions**—Using AI provides the optimal means of easily analyzing large data sets in GenAI app development. This data can benefit designers by providing better insights into customers' behaviors. UX designers also use data in building beautiful designs.
- **improving optimization**—Every organization seeks to improve the user experience and user engagement by making continuous design improvements and adding beautiful effects to mobile apps. Continuous improvement through the use of GenAI in mobile-app development drives competitive advantage to the maximum. GenAI in mobile UX design provides constant user feedback and assists teams in identifying areas for improvement. Conduct A/B testing to discover the best performing design options.
- **enhancing accessibility**—More than 15% of the world's population experiences some disability. Building a user-friendly, accessible application is helpful in expanding your audience base, automatically increasing sales and revenues.
- **adapting content**—Various GenAI platforms are available to generate image and text variations. These tools analyze live data and enable optimized content delivery. In UX design, GenAI is the best option for creating personalized user experiences rather than capabilities.
- **accurately translating design to code**—Design-to-code translation involves a critical process that helps businesses streamline their development process, enabling smoother transitions and creating higher quality products. Traditional methods are not as efficient in smoothing these transitions. GenAI helps teams understand programming languages and design principles. GenAI also provides significant options for generating attractive and highly functional user-interface (UI) elements.
- **rapidly prototyping app designs**—Mobile-app prototyping realizes design concepts for functional applications. GenAI also facilitates rapid prototyping, which is suitable for creating static or interactive UI elements. Today, in the UX design industry, most engineers utilize GenAI, which blends design and coding and optimizes the aesthetics of a mobile UX design.

## Practical Applications of GenAI in Mobile UX Design

> GenAI ensures consistency and accuracy across multiple platforms….

Designing a mobile app from scratch is a very time-consuming process. UX designers require advanced tools to complete the process. You can use GenAI in creating unique user-interface designs, which makes the process less time consuming. GenAI is now widely used in various content-generation applications, understanding the context and producing high-quality written content such as product descriptions. GenAI's algorithms make this process simpler.

GenAI can easily create images such as GIFs, videos, and code for mobile UX designs. This can help you deliver a user-friendly app while saving valuable time. GenAI ensures consistency and accuracy across multiple platforms, opening a new world for developers.

In application development, GenAI facilitates the creation of personalized user interfaces. GenAI algorithms let developers analyze user behaviors and preferences. Thus, they can dynamically produce UI elements that are tailored to individual users. GenAI can easily suggest layout adjustments, color schemes, and feature placements, aligning them with various user habits and preferences.

GenAI saves development time while ensuring seamless engagement with users. According to recent research, long-term AI opportunities have massively improved, driving $4.4 trillion in productivity from corporations. Larger organizations' use of GenAI has increased their annual revenues by $500 million and now represents 17% of governance in most businesses.

## Business Impacts of GenAI in UX Design

> GenAI helps increase company revenues through improved customer satisfaction and fast product releases.

The use of GenAI in UX design is having significant business impacts, such as the following:

- **enhancing productivity and operational efficiency**—In UX design, GenAI technology can reduce the amount of repetitive work that UX designers must do, allowing them to concentrate only on essential and creative work. For example, an organization in the financial-services industry has adopted a GenAI SQL tool that enables business personnel to generate their own SQL queries and retrieve information independently without using code.
- **identifying revenue-growth opportunities through AI-driven processes**—In addition to providing greater efficiency, GenAI helps increase company revenues through improved customer satisfaction and fast product releases. Estée Lauder partnered with Microsoft to create an AI innovation lab with the goal of using GenAI for beauty brands. Their goals included a faster pace of product development and individual customer engagement to prepare the company for quicker responses to market conditions and achieving higher revenues.

## Case Studies of Large Organizations Leveraging GenAI

Let's consider how a few businesses have used GenAI, as follows:

- **Goldman Sachs**—To solve its internal workflow problems, Goldman Sachs has adopted GenAI applications such as a natural-language coding tool and a platform that automates documentation. These tools have enhanced their workflows and, more importantly, improved the accuracy of their operations.
- **Telstra**—This Australian telecommunications company has unveiled some of its AI applications—such as Telstra, through which customers can interact with the company. This has not only improved the firm's customer experience but also helped cut operational costs that the company would have incurred.
- **National Australia Bank (NAB)**—NAB used GenAI to automate some processes that had been consuming much of employees' and bankers' time, enabling them to perform other more productive tasks such as growing customer relations and improving the company's customer-service delivery.

These business cases demonstrate how GenAI can fit seamlessly into UX design and other aspects of the business. GenAI helps get work done more efficiently, increasing revenues and making customers happy. For these reasons, GenAI has become a great facilitator for large organizations in their digital-transformation journeys.

## Conclusion

> Using GenAI in building custom mobile-app designs can significantly increase efficiency and reduce development time, thus saving money.

GenAI-powered platforms can play a significant role in transforming content for various domains. This automation can boost human creativity and efficiency. We can leverage GenAI extensively for a variety of purposes, including democratizing content creation. With GenAI, people lacking specific coding or graphic-design skills can contribute to a complex, AI-driven development process. Using GenAI in building custom mobile-app designs can significantly increase efficiency and reduce development time, thus saving money.

> **Note:** The following article is reproduced verbatim from  
> Roger Wong, *Roger Wong* (2025):  
> [Why Young Designers Are the Antidote to AI Automation](https://rogerwong.me/2025/07/design-talent-crisis-part-2)  
> for internal educational use only (non-profit).

# Why Young Designers Are the Antidote to AI Automation

In Part I of this series, I wrote about the struggles recent grads have had finding entry-level design jobs and what might be causing the stranglehold on the design job market.

## Part II: Building New Ladders

When I met Benedict Allen, he had just finished with Portfolio Review a week earlier. That's the big show all the design students in the Graphic Design program at San Diego City College work toward. It's a nice event that brings out the local design community where seasoned professionals review the portfolios of the graduating students.

Allen was all smiles and relief. "I want to dabble in different aspects of design because the principles are generally the same." He goes on to mention how he wants to start a fashion brand someday, DJ, try 3D. "I just want to test and try things and just have fun! Of course, I'll have my graphic design job, but I don't want that to be the end. Like when the workday ends, that's not the end of my creativity." He was bursting with enthusiasm.

And confidence. When asked about how prepared he felt about his job prospects, he shares, "I say this humbly, I really do feel confident because I'm very proud of my portfolio and the things I've made, my design decisions, and my thought processes." Oh to be in my early twenties again and have his same zeal!

But here's the thing, I believe him. I believe he'll go on to do great things because of this young person's sheer will. He idolizes Virgil Abloh, the died-too-young multi-hyphenate creative who studied architecture, founded the fashion label Off-White, became artistic director of menswear at Louis Vuitton, and designed furniture for IKEA and shoes for Nike. Abloh is Allen's North Star.

Artificial intelligence, despite its sycophantic tendencies, does not have that infectious passion. Young people are the life blood of companies. They can reinvigorate an organization and bring perspectives to a jaded workforce. Every single time I've ever had the privilege of working with interns, I have felt this. My teams have felt this. And they make the whole organization better.

## What Companies Must Do

I love this quote by Robert F. Kennedy in his 1966 speech at the University of Cape Town:

> This world demands the qualities of youth: not a time of life but a state of mind, a temper of the will, a quality of imagination, a predominance of courage over timidity, of the appetite for adventure over the life of ease.

As mentioned in Part I of this series, the design industry is experiencing an unprecedented talent crisis, with traditional entry-level career pathways rapidly eroding as the capabilities of AI expand and companies anticipate using AI to automate junior-level tasks. Youth is the key ingredient that sustains companies and industries.

### The Business Case for Juniors

Just as important as the energy and excitement Benedict Allen brings, is his natural ability to wield AI. He's an AI native.

In my conversation with him, he's tried all the major chatbots and has figured out what works best for what. "I've used Gemini as I find its voice feature amazing. Like, I use it all the time. …I use Claude sometimes for writing, but I find that the writing was not as good as ChatGPT. ChatGPT felt less like AI-speak. …I love Perplexity. That's one of my favorites as well."

He's not alone. Leah Ray, who recently graduated from California College of the Arts with an MFA in Graphic Design, says that she can't remember how her design process existed without AI, saying, "It's become such an integral part of how I think and work."

She parries with ChatGPT, using it as a creative partner:

> I usually start by having a deep or sometimes extended conversation with ChatGPT. And it's not about getting the direct answer, but more about using the dialogue to clarify my thoughts and challenging my assumptions and even arrive at a clear design direction.

She'll go on and use the chatbot to help with project planning and timelines, copywriting, code generation, and basic image generation. Ray has even considered training her own AI model using tools like ComfyUI or LoRA that are based on her past design work. She says, "So it could assist me in generating proposals that match my visual styles." Pretty advanced stuff.

Similar to Ray, Emma Haines, who is finishing up her MDes in Interaction Design at CCA, says that AI "comes into the picture very early on." She'll use ChatGPT for brainstorming and project planning, and less so in the later stages.

Unlike many established designers, these young ones don't see AI as threatening, nor as a crutch. They treat AI as any other tool. Ashton Landis, who recently graduated from CCA with a BFA in Graphic Design, says, "I think right now it's primarily a tool instead of a replacement."

Elena Pacenti, Director of MDes Interaction Design at CCA, observes that students have embraced AI immediately and across the board. She says generative AI has been "adopted immediately by everyone, faculty and students" and it's being used to create text, images, and all sorts of visual content—not just single images, but storyboards, videos, and more. It's become just another tool in their toolkit.

Pacenti notices that her students are gravitating toward using AI for efficiency rather than exploration. She sees them "embracing all the tools that help make the process faster, more efficient, quicker" to get to their objective, rather than using AI "to explore things they haven't thought about or to make things." They're using it as a shortcut rather than a creative partner.

### Restructure Entry-Level Roles

I don't think it's quite there yet, but AI will eventually take over the traditional tasks we give to junior designers. Anthropic recently released an integration with Canva, but the results are predictable—barely a good first draft. For companies that choose to live on the bleeding edge, that will likely be within 12 months. I think in two years, we'll cede more and more of these junior-level design tasks like extending designs, resizing assets, and searching for stock to AI.

But I believe there is still a place for entry-level designers in any organization.

Firstly, the tasks can simply be done faster. When we talk about AI and automation, oftentimes the human who's initiating the task and then judging its output isn't part of the conversation. Babysitting AI takes time and more importantly, breaks flow. I can imagine teaching a junior designer how to perform these tasks using AI and just stack up more in a day or week. They'll still be able to practice their taste and curation skills with supervision from more senior peers.

Second, younger people are inherently better with newer technologies. Asking a much more senior designer to figure out advanced prototyping with Lovable or Cursor will be a non-starter. But junior designers should be able to pick this up quickly and become indispensable pairs of hands in the overall process.

Third, we can simply level up the complexity of the tasks we give to juniors. Aneesh Raman, chief economic opportunity officer at LinkedIn, wrote in The New York Times:

> Unless employers want to find themselves without enough people to fill leadership posts down the road, they need to continue to hire young workers. But they need to redesign entry-level jobs that give workers higher-level tasks that add value beyond what can be produced by A.I. At the accounting and consulting firm KPMG, recent graduates are now handling tax assignments that used to be reserved for employees with three or more years of experience, thanks to A.I. tools. And at Macfarlanes, early-career lawyers are now tasked with interpreting complex contracts that once fell to their more seasoned colleagues. Research from the M.I.T. Sloan School of Management backs up this switch, indicating that new and low-skilled workers see the biggest productivity gains and benefits from working alongside A.I. tools.

In other words, let's assume AI will tackle the campaign resizing or building out secondary and tertiary pages for a website. Have junior designers work on smaller projects as the primary designer so they can set strategy, and have them shadow more senior designers and develop their skills in concept, strategy, and decision-making, not just execution.

### Invest in the Leadership Pipeline

The 2023 Writers Guild of America strike offers a sobering preview of what could happen to the design profession if we're not careful about how AI reshapes entry-level opportunities. Unrelated to AI, but to simple budget-cutting, Hollywood studios began releasing writers immediately after scripts were completed, cutting them out of the production process where they would traditionally learn the hands-on skills needed to become showrunners and producers. As Oscar-winning writer Sian Heder (CODA) observed, "A writer friend has been in four different writers rooms and never once set foot on set. How are we training the next generation of producers and showrunners?" The result was a generation of writers missing the apprenticeship pathway that transforms scriptwriters into skilled creative leaders—exactly the kind of institutional knowledge loss that weakens an entire industry.

The WGA's successful push for guaranteed on-set presence reveals what the design industry must do to avoid a similar talent catastrophe. Companies are avoiding junior hires entirely, anticipating that AI will handle execution tasks—but this eliminates the apprenticeship pathway where designers actually learn strategic thinking. Instead, they need to restructure entry-level roles to guarantee meaningful learning opportunities—pairing junior designers with real projects where they develop taste through guided decision-making. As one WGA member put it, "There's just no way to learn to do this without learning on the job." The design industry's version of that job isn't Figma execution—it's the messy, collaborative process of translating business needs into human experiences.

Today's junior designers will become tomorrow's creative directors, design directors, and heads of design. Senior folks like myself will eventually age out, so companies that don't invest in junior talent now won't have any experienced designers in five to ten years.

And if this is an industry-wide trend, young designers who can't get into the workforce today will pivot to other careers and we won't have senior designers, period.

## How Education is Responding

### The Irreplaceable Human Element

When I spoke to the recent grads, all five of them mentioned how AI-created output just has an air of AI. Emma Haines:

> People can tell what AI design looks like versus what human design looks like. I think that's because we naturally just add soul into things when we design. We add our own experiences into our designs. And just being artists, we add that human element into it. I think people gravitate towards that naturally, just as humans.

It speaks to how educators are teaching—and have always been teaching—design. Bradford Prairie, a professor at San Diego City College:

> We always tell students, "Try to expose yourself to a lot of great work. Try to look at a lot of inspiration. Try to just get outside more." Because I think a lot of our students are introverts. They want to sit in their room and I tell them, "No, y'all have to get out in the world! …and go touch grass and touch other things out in the world. That's how you learn what works and what doesn't, and what culture looks like.

Leah Ray, explaining how our humanity imbues quality into our designs:

> You can often recognize an AI look. Images and designs start to feel like templates and over-predictable in that sense. And everything becomes fast like fast food and sometimes even quicker than eating instant food.

And even though there is a scary trend towards synthetic user research, Elena Pacenti, discourages it. She'll teach her students to start with provisional user archetypes using AI, but then they'll need to perform primary research to validate it. "We're going to do primary to validate. Please do not fake data through the AI."

### Redefining Entry-Level Value

I only talked to educators from two institutions for this series, since those are the two I have connections to. For both programs, there's less emphasis on hard skills like how to use Figma and more on critical thinking and strategy. I suspect that bootcamps are different.

Sean Bacon, chair of the Graphic Design program at San Diego City College:

> Our program is really about concepting, creative thinking, and strategy. Bradford and I are cautiously optimistic that maybe, luckily, the chips we put down, are in the right part of the board. But who knows?

I think he's spot on. Josh Silverman, who teaches courses at CCA's MDes Interaction Design, and also a design recruiter, observes:

> So what I'm seeing from my perspective is a lot of organizations that are hiring the kind of students that we graduate from the program, what I like to call a "dangerous generalist." It's someone who can do the research, strategy, prototyping, visual design, presentation, storytelling, and be a leader and make a measurable impact. And if a company is restructuring or just starting and only has the means to hire one person, they're going to want someone who can do all those things. So we are poised to help a lot of students get meaningful employment because they can do all those things.

### AI as Design Material, Not Just Tool

Much of the AI conversation has been about how to incorporate it into our design workflows. For UX designers, it's just as important to discuss how we design AI experiences for users.

Elena Pacenti champions this shift in the conversation. "My take on the whole thing has been to move beyond the tools and to understand AI as a material we design with." Similar to the early days of virtual reality, AI is an interaction paradigm with very few UI conventions and therefore ripe for designers to invent. Right now.

> This profession specifically designs the interaction for complex systems, products, services, a combination—whatever it is out there—and ecosystems of technologies. What's the next generation of these things that we're going to design for? …There's a very challenging task of imagining interactions that are not going just through a chatbot, but they don't have shape yet. They look tremendously immaterial, more than the past. It's not going to be necessarily through a screen.

Her program at CCA has implemented this through a specific elective called "Prototyping with AI," which Pacenti describes as teaching students to "get your hands dirty and understand what's behind the LLMs and how you can use this base of data and intelligence to do things that you want, not that they want." The goal is to help students craft their own tools rather than just using prepackaged consumer AI tools—which she calls "a shift towards using it as a material."

## The Path Forward Requires Collective Action

Benedict Allen's infectious enthusiasm after Portfolio Review represents everything the design industry risks losing if we don't act deliberately. His confidence, creativity, and natural fluency with AI tools? That's the potential young designers bring—but only if companies and educational institutions create pathways for that talent to flourish.

The solution isn't choosing between human creativity and artificial intelligence. It's recognizing that the combination is more powerful than either alone. Elena Pacenti's insight about understanding "AI as a material we design with" points toward this synthesis, while companies like KPMG and Macfarlanes demonstrate how entry-level roles can evolve rather than disappear.

This transformation demands intentional investment from both sides. Design schools are adapting quickly—reimagining curriculum, teaching AI fluency alongside fundamental design thinking, emphasizing the irreplaceable human elements that no algorithm can replicate. Companies must match this effort. Restructure entry-level roles. Create new apprenticeship models. Recognize that today's junior designers will become tomorrow's creative leaders.

The young designers I profiled here prove that talent and enthusiasm haven't disappeared. They're evolving. Allen's ambitious vision to start a fashion brand. Leah Ray's ease with AI tools. The question isn't whether these designers can adapt to an AI-enabled future.

It's whether the industry will create space for them to shape it.

In the final part of this series, I'll explore specific strategies for recent graduates navigating this current job market—from building AI-integrated portfolios to creating alternative pathways into the profession.

> **Note:** The following article is reproduced verbatim from  
> Roger Wong, *Roger Wong* (2025):  
> [Prompt. Generate. Deploy. The New Product Design Workflow](https://rogerwong.me/2025/04/prompt-generate-deploy)  
> for internal educational use only (non-profit).

# Prompt. Generate. Deploy. The New Product Design Workflow

Product design is going to change profoundly within the next 24 months. If the AI 2027 report is any indication, the capabilities of the foundational models will grow exponentially, and with them—I believe—will the abilities of design tools.

The TL;DR of the report is this: companies like OpenAI have more advanced AI agent models that are building the next-generation models. Once those are built, the previous generation is tested for safety and released to the public. And the cycle continues. Currently, and for the next year or two, these companies are focusing their advanced models on creating superhuman coders. This compounds and will result in artificial general intelligence, or AGI, within the next five years.

Non-AI companies will benefit from new model releases. We already see how much the performance of coding assistants like Cursor has improved with recent releases of Claude 3.7 Sonnet, Gemini 2.5 Pro, and this week, GPT-4.1, OpenAI's latest.

Tools like v0, Lovable, Replit, and Bolt are leading the charge in AI-assisted design. Creating new landing pages and simple apps is literally as easy as typing English into a chat box. You can whip up a very nice-looking dashboard in single-digit minutes.

However, I will argue they are only serving a small portion of the market. These tools are great for zero-to-one digital products or websites. While new sites and software need to be designed and built, the vast majority of the market is in extending and editing current products. There are hordes more designers who work at corporations such as Adobe, Microsoft, Salesforce, Shopify, and Uber than there are designers at agencies. They all need to adhere to their company's design system and can't use what Lovable produces from scratch. The generated components can't be used even if they were styled to look correct. They must be components from their design system code repositories.

## The Design-to-Code Gap

But first, a quick detour…

For any designer who has ever handed off a Figma file to a developer, they have felt the stinging disappointment days or weeks later when it's finally coded. The spacing is never quite right. The type sizes are off. And the back and forth seems endless. The developer handoff experience has been a well-trodden path full of now-defunct or dying companies like InVision, Abstract, and Zeplin. Figma tries to solve this issue with Dev Mode, but even then, there's a translation that has to happen from pixels and vectors in a proprietary program to code.

Yes, no- and low-code platforms like Webflow, Framer, and Builder.io exist. But the former two are proprietary platforms—you can't take the code with you—and the latter is primarily a CMS (no-code editing for content editors).

The dream is for a design app similar to Figma that uses components from your team's GitHub design system repository. I'm not talking about a Figma-only component library. No. Real components with controllable props in an inspector. You can't break them apart and any modifications have to be made at the repo level. But you can visually put pages together. For new components, well, if they're made of atomic parts, then yes, that should be possible too.

UXPin Merge comes close. Everything I mentioned above is theoretically possible. But if I'm being honest, I did a trial and the product is buggy and wasn't great to use.

## A Glimpse of What's Coming

Enter Tempo, Polymet, and Subframe. These are very new entrants to the design tool space. Tempo and Polymet are backed by Y Combinator and Subframe is pre-seed.

For Subframe, they are working on a beta feature that will allow you to connect your GitHub repository, append a little snippet of code to each component, and then the library of components will appear in their app. Great! This is the dream. The app seems fairly easy to use and wasn't sluggish and buggy like UXPin.

But the kicker—the Holy Grail—is their AI.

I quickly put together a hideous form screen based on one of the oldest pages in BuildOps that is long overdue for a redesign. Then, I went into Subframe's Ask AI tab and prompted, "Make this design more user friendly." Similar to Midjourney, four blurry tiles appeared and slowly came into focus. This diffuser model effect was a moment of delight for me. I don't know if they're actually using a diffuser model—think Stable Diffusion and Midjourney—or if they spent the time building a kick-ass loading state. Anyway, four completely built alternate layouts were generated. I clicked into each one to see it larger and noticed they each used components from our styled design library. (I'm on a trial, so it's not exactly components from our repo, but it demonstrates the promise.) And I felt like I just witnessed the future.

## What Product Design in 2027 Might Look Like

From the AI 2027 scenario report, in the chapter, "March 2027: Algorithmic Breakthroughs":

> Three huge datacenters full of Agent-2 copies work day and night, churning out synthetic training data. Another two are used to update the weights. Agent-2 is getting smarter every day.With the help of thousands of Agent-2 automated researchers, OpenBrain is making major algorithmic advances. …Aided by the new capabilities breakthroughs, Agent-3 is a fast and cheap superhuman coder. OpenBrain runs 200,000 Agent-3 copies in parallel, creating a workforce equivalent to 50,000 copies of the best human coder sped up by 30x. OpenBrain still keeps its human engineers on staff, because they have complementary skills needed to manage the teams of Agent-3 copies.

As I said at the top of this essay, AI is making AI and the innovations are compounding. With UX design, there will be a day when design is completely automated.

Imagine this. A product manager at a large-scale e-commerce site wants to decrease shopping cart abandonment by 10%. They task an AI agent to optimize a shopping cart flow with that metric as the goal. A week later, the agent returns the results:

- It ran 25 experiments, with each experiment being a design variation of multiple pages.
- Each experiment was with 1,000 visitors, totaling about 10% of their average weekly traffic.
- Experiment #18 was the winner, resulting in an 11.3% decrease in cart abandonment.

The above will be possible. A few things have to fall in place first, though, and the building blocks are being made right now.

## The Foundation Layer: Integrate Design Systems

The design industry has been promoting the benefits of design systems for many years now. What was once a Sisyphean uphill battle is now mostly easier. Development teams understand the benefits of using a shared and standardized component library.

To capture the larger piece of the design market that is not producing greenfield work, AI design tools like Subframe will have to depend on well-built component libraries. Their AI must be able to ingest and internalize design system documentation that govern how components should be used.

Then we'll be able to prompt new screens with working code into existence.

**Forecast:** Within six months.

## Professionals Still Need Control

Cursor—the AI-assisted development tool that's captured the market—is VS Code enhanced with AI features. In other words, it is a professional-grade programming tool that allows developers to write and edit code, and generate it via AI chat. It gives the pros control. Contrast that with something like Lovable, which is aimed at designers and the code is accessible, but you have to look for it. The canvas and chat are prioritized.

For AI-assisted design tools to work, they need to give us designers control. That control comes in the form of curation and visual editing. Give us choices when generating alternates and let us tweak elements to our heart's content—within the confines of the design system, of course.

The product design workflow in the future will look something like this: prompt the AI, view choices and select one, then use fine-grained controls to tweak.

## Automating Design with Design Agents

Agent mode in Cursor is pretty astounding. You'll see it plan its actions based on the prompt, then execute them one by one. If it encounters an error, it'll diagnose and fix it. If it needs to install a package or launch the development server to test the app, it will do that. Sometimes, it can go for many minutes without needing intervention. It's literally like watching a robot assemble a thingamajig.

We will need this same level of agentic AI automation in design tools. If I could write in a chat box "Create a checkout flow for my site" and the AI design tool can generate a working cart page, payment page, and thank-you page from that one prompt using components from the design system, that would be incredible.

Yes, zero-to-one tools are starting to add this feature. Here's a shopping cart flow from v0…

Building a shopping cart checkout flow in v0 was incredibly fast. Two minutes flat. This video is sped up 400%.

Polymet and Lovable were both able to create decent flows. There is also promise with Tempo, although the service was bugging out when I tested it earlier today. Tempo will first plan by writing a PRD, then it draws a flow diagram, then wireframes the flow, and then generates code for each screen. If I were to create a professional tool, this is how I would do it. I truly hope they can resolve their tech issues.

**Forecast:** Within one year.

Tempo's workflow seems ideal. It generates a PRD, draws a flow diagram, creates wireframes, and finally codes the UI.

## The Final Pieces: Integration and Deployment Agents

The final pieces to realizing our imaginary scenario are coding agents that integrate the frontend from AI design tools to the backend application, and then deploy the code to a server for public consumption. I'm not an expert here, so I'll just hand-wave past this part. The AI-assisted design tooling mentioned above is frontend-only. For the data to flow and the business logic to work, the UI must be integrated with the backend.

CI/CD (Continuous Integration and Continuous Deployment) platforms like GitHub Actions and Vercel already exist today, so it's not difficult to imagine deploys being initiated by AI agents.

**Forecast:** Within 18–24 months.

## Where Is Figma?

The elephant in the room is Figma's position in all this. Since their rocky debut of AI features last year, Figma has been trickling out small AI features like more powerful search, layer renaming, mock data generation, and image generation. The biggest AI feature they have is called First Draft, which is a relaunch of design generation. They seem to be stuck placating to designers and developers (Dev Mode), instead of considering how they can bring value to the entire organization. Maybe they will make a big announcement at Config, their upcoming user conference in May. But if they don't compete with one of these aforementioned tools, they will be left behind.

To be clear, Figma is still going to be a necessary part of the design process. A canvas free from the confines of code allows for easy manual exploration. But the dream of closing the gap between design and code needs to come true sooner than later if we're to take advantage of AI's promise.

## The Two-Year Horizon

As I said at the top of this essay, product design is going to change profoundly within the next two years. The trajectory is clear: AI is making AI, and the innovations are compounding rapidly. Design systems provide the structured foundation that AI needs, while tools like Subframe are developing the crucial integration with these systems.

For designers, this isn't the end—if anything, it's a transformation. We'll shift from pixel-pushers to directors, from creators to curators. Our value will lie in knowing what to ask for and making the subtle refinements that require human taste and judgment.

The holy grail of seamless design-to-code is finally within reach. In 24 months, we won't be debating if AI will transform product design—we'll be reflecting on how quickly it happened.

> **Note:** The following article is reproduced verbatim from  
> Roger Wong, *Roger Wong* (2025):  
> [The Design Industry Created Its Own Talent Crisis. AI Just Made It Worse.](https://rogerwong.me/2025/07/design-talent-crisis)  
> for internal educational use only (non-profit).

# The Design Industry Created Its Own Talent Crisis. AI Just Made It Worse.

This is the first part in a three-part series about the design talent crisis. Read Part II and Part III.

## Part I: The Vanishing Bottom Rung

Erika Kim's path to UX design represents a familiar pandemic-era pivot story, yet one that reveals deeper currents about creative work and economic necessity. Armed with a 2020 film and photography degree from UC Riverside, she found herself working gig photography—graduations, band events—when the creative industries collapsed. The work satisfied her artistic impulses but left her craving what she calls "structure and stability," leading her to UX design. The field struck her as an ideal synthesis, "I'm creating solutions for companies. I'm working with them to figure out what they want, and then taking that creative input and trying to make something that works best for them."

Since graduating from the interaction design program at San Diego City College a year ago, she's had three internships and works retail part-time to pay the bills. "I've been in survival mode," she admits. On paper, she's a great candidate for any junior position. Speaking with her reveals a very thoughtful and resourceful young designer. Why hasn't she been able to land a full-time job? What's going on in the design job market?

Back in January, Jared Spool offered an explanation. The UX job market crisis stems from a fundamental shift that occurred around late 2022—what he calls a "market inversion." The market flipped from having far more open UX positions than qualified candidates to having far more unemployed UX professionals than available jobs. The reasons are multitude, but include expiring tax incentives, rising interest rates, an abundance of bootcamp graduates, automated hiring processes, and globalization.

But that's only part of the equation. I believe there's something much larger at play, one that affects more than just UX or product design, but all design disciplines. One in which the tip of the spear has already been felt by software developers in their job market. AI.

## Closing Doors for New Graduates

In the first half of this year, 147 tech companies have laid off over 63,000 workers, with a significant portion of them engineers. Entry-level hiring has collapsed, revealing a new permanent reality. At Big Tech companies, new graduates now represent just 7% of all hires—a precipitous 25% decline from 2023 levels and a staggering 50% drop from pre-pandemic baselines in 2019.

The startup ecosystem tells an even more troubling story, where recent graduates comprise less than 6% of new hires, down 11% year-over-year and more than 30% since 2019. This isn't merely a temporary adjustment; it represents a fundamental restructuring of how companies approach talent acquisition. Even the most credentialed computer science graduates from top-tier programs are finding themselves shut out, suggesting that the erosion of junior positions cuts across disciplines and skill levels.

LinkedIn executive Aneesh Raman wrote in an op-ed for The New York Times that in a "recent survey of over 3,000 executives on LinkedIn at the vice president level or higher, 63 percent agreed that A.I. will eventually take on some of the mundane tasks currently allocated to their entry-level employees."

There is already a harsh reality for entry-level tech workers. Companies have essentially frozen junior engineer and data analyst hiring because AI can now handle the routine coding and data querying tasks that were once the realm for new graduates. Hiring managers expect AI's coding capabilities to expand rapidly, potentially eliminating entry-level roles within a year, while simultaneously increasing demand for senior engineers who can review and improve AI-generated code. It's a brutal catch-22: junior staff lose their traditional stepping stones into the industry just as employers become less willing to invest in onboarding them.

For design students and recent graduates, this data illuminates a broader industry transformation where companies are increasingly prioritizing proven experience over potential—a shift that challenges the very foundations of how creative careers traditionally begin.

While AI tools haven't exactly been able to replace designers yet—even junior ones—the tech will get there sooner than we think. And CEOs and those holding the purse strings are anticipating this, thus holding back hiring of juniors.

## The Learning-by-Doing Crisis

Ashton Landis recently graduated with a BFA in Graphic Design from California College of the Arts (full disclosure: my alma mater). She says:

> I found that if you look on LinkedIn for "graphic designer" and you just say the whole San Francisco Bay area, so all of those cities, and you filter for internships and entry level as the job type, there are 36 [job postings] total. And when you go through it, 16 of them are for one or more years of experience. And five of those are for one to two years of experience. And then everything else is two plus years of experience, which doesn't actually sound like sound like entry level to me. …So we're pretty slim pickings right now.

When I graduated from CCA in 1995 (or CCAC as it was known back then), we were just climbing out of the labor effects of the early 1990s recession. For my early design jobs in San Francisco, I did a lot of production and worked very closely with more senior designers and creative directors to hone my craft. While school is great for academic learning, nothing beats real-world experience.

Eric Heiman, creative director and co-owner of Volume Inc., a small design studio based in San Francisco, has been teaching at CCA for 26 years. He observes:

> We internalize so much by doing things slower, right? The repetition of the process, learning through tinkering with our process, and making mistakes, and things like that. We have internalized those skills.

Sean Bacon, chair of the Graphic Design program at San Diego City College wonders:

> What is an entry level position in design then? Where do those exist? How often have I had these companies hire my students even though they clearly don't have those requirements. So I don't know. I don't know what happens, but it is scary to think we're losing out on what I thought was really valuable training in terms of how I learned to operate, at least in a studio.

Back to the beginnings of my career, I remember digitizing logos when I interned with Mark Fox, a talented logo designer based in Marin County. A brilliant draftsman, he had inked—and still inks—all of his logos by hand. The act of redrawing marks in Illustrator helped me develop my sense of proportions, curves, and optical alignment. At digital agencies, I started my journey redesigning layouts of banners in different sizes. I would eventually have juniors to do that for me as I rose through the ranks. These experiences—though a little painful at the time—were pivotal in perfecting our collective craft. To echo Bacon, it was "really valuable training."

### Apprenticeships at Agencies

Working in agencies and design studios was pretty much an apprenticeship model. Junior designers shadowed more senior designers and took their lead when executing a campaign or designing more pages for a website.

For a typical website project, as a senior designer or art director, I would design the homepage and a few other critical screens, setting up the look and feel. Once those were approved by the client, junior designers would take over and execute the rest. This was efficient and allowed the younger staff to participate and put their reps in.

Searching for stock photos was another classic assignment for interns and junior designers. These were oftentimes multi-day assignments, but it helped teach juniors how to see.

But today, generative AI apps like Midjourney and Visual Electric are replacing stock photography.

### From Craft to Curation

As the industry marches towards incorporating AI into our workflows, strategy, judgement, and most importantly taste, are critical skills.

But the paradoxically, how do designers develop taste, craft, and strategic thinking without doing the grunt work?

And not only are they missing out on the mundane work because of the dearth of entry-level opportunities, but also because generative AI can give results so quickly.

Eric Heiman again:

> I just give the AI a few words and poof, it's there. How do you learn how to see things? I just feel like learning how to see is a lot about slowing down. And in the case of designers, doing things yourself over and over again, and they slowly reveal themselves through that process.

## Navigating the New Reality

All the recent graduates I interviewed for this piece are smart, enthusiastic, and talented. Yet, Ashton Landis and Erika Kim are struggling to find full-time jobs.

Landis doesn't think her negative experience in the job market is "entirely because of AI," attributing it more to "general unemployment rates are pretty high right now" and a job market that is "clearly not great."

### Questioning Career Choices

Leah Ray, a recent graphic design MFA graduate from CCA, was able to secure a position as International Visual Designer at Kuaishou, a popular Chinese short-form video and live-streaming app similar to TikTok. But it wasn't easy. Her job search began months before graduation, extending through her thesis work and creating the kind of sustained anxiety that prompted her final school project—a speculative design exploring AI's potential to predict alternative career futures.

> I was so anxious about my next step after graduation because I didn't have a job lined up and I didn't know what to do. …I'm a person who follows the social clock. My parents and the people around me expect me to do the right thing at the right age. Getting a nice job was my next step, but I couldn't finish that, which led to me feeling anxious and not knowing what to do.

But through her tenacity and some luck, she was able to land the job that she starts this month.

> No, it was not easy to find. But finding this was very lucky. I do remember I saw a lot of job descriptions for junior designers. They expect designers to have AI skills. And I think there are even some roles specifically created for people with AI-related design skills, like AI motion designer and AI model designer, sort of something like that. Like AI image training designers.

Ray's observation reveals a fundamental shift in entry-level design expectations, where AI proficiency has moved from optional to essential, with entirely new roles emerging around AI-specific design skills.

## Preparing Our Students

Emma Haines, a designer completing her masters degree in Interaction Design at CCA began her job search in May. (Her program concludes in August.) Despite not securing a job yet, she's bullish because of the prestige and practicality of the Master of Design program.

> I think this program has actually helped me a good amount from where I was starting out before. I worked for a year between undergrad and this program, and between where I was before and now, there's a huge difference. That being said, since the industry is changing so rapidly, it feels a little hard to catch up with. That's the part that makes me a little nervous going into it. I could be confident right now, but maybe in six months something changes and I'm not as confident going into the job market.

CCA's one-year program represents a strategic bet on adaptability over specialization. Elena Pacenti, the program's director, describes an intensive structure that "goes from a foundational semester with foundation of interaction design, form, communication, and research to the system part of it. So we do systems thinking, prototyping, also tangible computing." The program's Social Lab component is "two semester-long projects with community partners in partnership with stakeholders that are local or international from UNICEF down to the food bank in Oakland." It positions design as a tool for social impact rather than purely commercial purposes. This compressed timeline creates what Pacenti calls curricular agility: "We're lucky that we are very agile. We are a one-year program so we can implement changes pretty quickly without affecting years of classes and changes in the curriculum."

Josh Silverman, who chaired it for nearly five years, reports impressive historical outcomes: "I think historically for the first nine years of the program—this is cohort 10—I think we've had something like 85% job placement within six months of graduation."

Yet both educators acknowledge current market realities. Pacenti observes that "that fat and hungry market of UX designers is no longer there; it's on a diet," while maintaining optimism about design's future relevance: "I do not believe that designers will be less in demand. I think there will be a tremendous need for designers." Emma Haines's nervousness about rapid industry change reflects this broader tension—the gap between educational preparation and market evolution that defines professional training during transformative periods.

Bradford Prairie, who has taught in San Diego City College's Graphic Design program for nine years, embodies this experimental approach to AI in design education. "We get an easy out when it comes to AI tools," he explains, "because we're a program that's meant to train people for the field. And if the field is embracing these tools, we have an obligation to make students aware of them and give some training on how to use the tools."

Prairie's classroom experiments reveal both the promise and pitfalls of AI-assisted design. He describes a student struggling with a logo for a DJ app who turned to ChatGPT for inspiration: "It generates a lot of expected things like turntables, headphones, and waveforms... they're all too complicated. They all don't really look like logos. They look more like illustrations." But the process sparked some other ideas, so he told the student, "This is kind of interesting how the waveform is part of the turntable and… we can take this general idea and redraw it and make it simplified."

This tension between AI output and human refinement has become central to his teaching philosophy: "If there's one thing that AI can't replace, it's your sense of discernment for what is good and what is not good." The challenge, he acknowledges, lies in developing that discernment in students who may be tempted to rely too heavily on AI from the start.

## The Turning Point

These challenges are real, and they're reshaping the design profession in fundamental ways. Traditional apprenticeships are vanishing, entry-level opportunities are scarce, and new graduates face an increasingly competitive landscape. But within this disruption lies opportunity. The same forces that have eliminated routine design tasks have also elevated the importance of uniquely human skills—strategic thinking, cultural understanding, and creative problem-solving. The path forward requires both acknowledging what's been lost and embracing what's possible.

Despite her struggles to land a full-time job in design, Erika Kim remains optimistic because she's so enthused about her career choice and the opportunity ahead. Remarking on the parallels of today versus the beginning of the Covid-19 pandemic, she says "It's kind of interesting that I'm also on completely different grounds in terms of uncertainty. But you just have to get through it, you know. Why not?"

In the next part of this series, I'll focus on the opportunities ahead: how we as a design industry can do better and what we should be teaching our design students. In the final part, I'll touch on what recent grads can do to find a job in this current market.

> **Note:** The following article is reproduced verbatim from  
> Roger Wong, *Roger Wong* (2025):  
> [Beyond the Prompt: Finding the AI Design Tool That Actually Works for Designers](https://rogerwong.me/2025/04/beyond-the-prompt)  
> for internal educational use only (non-profit).

# Beyond the Prompt: Finding the AI Design Tool That Actually Works for Designers

There has been an explosion of AI-powered prompt-to-code tools within the last year. The space began with full-on integrated development environments (IDEs) like Cursor and Windsurf. These enabled developers to use leverage AI assistants right inside their coding apps. Then came a tools like v0, Lovable, and Replit, where users could prompt screens into existence at first, and before long, entire applications.

A couple weeks ago, I decided to test out as many of these tools as I could. My aim was to find the app that would combine AI assistance, design capabilities, and the ability to use an organization's coded design system.

While my previous essay was about the future of product design, this article will dive deep into a head-to-head between all eight apps that I tried. I recorded the screen as I did my testing, so I've put together a video as well, in case you didn't want to read this.

It is a long video, but there's a lot to go through. It's also my first video on YouTube, so this is an experiment.

## The Bottom Line: What the Testing Revealed

I won't bury the lede here. AI tools can be frustrating because they are probabilistic. One hour they can solve an issue quickly and efficiently, while the next they can spin on a problem and make you want to pull your hair out. Part of this is the LLM—and they all use some combo of the major LLMs. The other part is the tool itself for not handling what happens when their LLMs fail.

For example, this morning I re-evaluated Lovable and Bolt because they've released new features within the last week, and I thought it would only be fair to assess the latest version. But both performed worse than in my initial testing two weeks ago. In fact, I tried Bolt twice this morning with the same prompt because the first attempt netted a blank preview. Unfortunately, the second attempt also resulted in a blank screen and then I ran out of credits. 🤷‍♂️

For designers who want actual design tools to work on UI, Subframe is the clear winner. The other tools go directly from prompt to code, skipping giving designers any control via a visual editor. We're not developers, so manipulating the design in code is not for us. We need to be able to directly manipulate the components by clicking and modifying shapes on the canvas or changing values in an inspector.

For me, the runner-up is v0, if you want to use it only for prototyping and for getting ideas. It's quick—the UI is mostly unstyled, so it doesn't get in the way of communicating the UX.

## The Players: Code-Only vs. Design-Forward Tools

There are two main categories of contenders: code-only tools, and code plus design tools.

### Code-Only

- Bolt
- Lovable
- Polymet
- Replit
- v0

### Code + Design

- Onlook
- Subframe
- Tempo

## My Testing Approach: Same Prompt, Different Results

As mentioned at the top, I tested these tools between April 16–27, 2025. As with most SaaS products, I'm sure things change daily, so this report captures a moment in time.

For my evaluation, since all these tools allow for generating a design from a prompt, that's where I started. Here's my prompt:

> Create a complete shopping cart checkout experience for an online clothing retailer

I would expect the following pages to be generated:

- Shopping cart
- Checkout page (or pages) to capture payment and shipping information
- Confirmation

I scored each app based on the following rubric:

- **User experience (25)**
- **Visual design (15)**
- **Prototype (10)**
- **Ease of use (15)**
- **Control (15)**
- **Design system integration (10)**
- **Speed (10)**
- **Editor's discretion (±10)**

## The Scoreboard: How Each Tool Stacked Up

Final summary scores for AI design tools for designers. Evaluations conducted between 4/16–4/27/25.

Here are the summary scores for all eight tools. For the detailed breakdown of scores, view the scorecards here in this Google Sheet.

## The Blow-by-Blow: The Good, the Bad, and the Ugly

### Bolt

First up, Bolt. Classic prompt-to-code pattern here—text box, type your prompt, watch it work.

Bolt shows you the code generation in real-time, which is fascinating if you're a developer but mostly noise if you're not. The resulting design was decent but plain, with typical UX patterns. It missed delivering the confirmation page I would expect. And when I tried to re-evaluate it this morning with their new features? Complete failure—blank preview screens until I ran out of credits. No rhyme or reason. And there it is—a perfect example of the maddening inconsistency these tools deliver. Working beautifully in one session, completely broken in another. Same inputs, wildly different outputs.

**Score: 43**

### Lovable

Moving on to Lovable, which I captured this morning right after they launched their 2.0 version. The experience was a mixed bag. While it generated clean (if plain) UI with some nice touches like toast notifications and a sidebar shopping cart, it got stuck at a critical juncture—the actual checkout. I had to coax it along, asking specifically for the shopping cart that was missing from the initial generation.

The tool encountered an error but at least provided a handy "Try to fix" button. Unlike Bolt, Lovable tries to hide the code, focusing instead on the browser preview—which as a designer, I appreciate. When it finally worked, I got a very vanilla but clean checkout flow and even the confirmation page I was looking for. Not groundbreaking, but functional. The approach of hiding code complexity might appeal to designers who don't want to wade through development details.

**Score: 49**

### Polymet

Next up is Polymet. This one has a very interesting interface and I kind of like it. You have your chat on the left and a canvas on the right. But instead of just showing the screen it's working on, it's actually creating individual components that later get combined into pages. It's almost like building Figma components and then combining them at the end, except these are all coded components.

The design is pretty good—plain but very clean. I feel like it's got a little more character than some of the others. What's nice is you can go into focus mode and actually play with the prototype. I was able to navigate from the shopping cart through checkout (including Apple Pay) to confirmation. To export the code, you need to be on a paid plan, but the free trial gives you at least a taste of what it can do.

**Score: 58**

### Replit

Replit was a test of patience—no exaggeration, it was the slowest tool of the bunch at 20 minutes to generate anything substantial. Why so slow? It kept encountering errors and falling into those weird loops that LLMs often do when they get stuck. At one point, I had to explicitly ask it to "make it work" just to progress beyond showing product pages, which wasn't even what I'd asked for in the first place.

When it finally did generate a checkout experience, the design was nothing to write home about. Lines in the stepper weren't aligning properly, there were random broken elements, and ultimately—it just didn't work. I couldn't even complete the checkout flow, which was the whole point of the exercise. I stopped recording at that point because, frankly, I just didn't want to keep fighting with a tool that's both slow and ineffective.

**Score: 31**

### v0

Taking v0 for a spin next, which comes from Vercel. I think it was one of the earlier prompt-to-code generators I heard about—originally just for components, not full pages (though I could be wrong). The interface is similar to Bolt with a chat panel on the left and code on the right. As it works, it shows you the generated code in real-time, which I appreciate. It's pretty mature and works really well.

The result almost looks like a wireframe, but the visual design has a bit more personality than Bolt's version, even though it's using the unstyled shadcn components. It includes form validation (which I checked), and handles the payment flow smoothly before showing a decent confirmation page. Speed-wise, v0 is impressively quick compared to some others I tested—definitely a plus when you're iterating on designs and trying to quickly get ideas.

**Score: 61**

### Onlook

Onlook stands out as a self-contained desktop app rather than a web tool like the others. The experience starts the same way—prompt in, wait, then boom—but instead of showing you immediate results, it drops you into a canvas view with multiple windows displaying localhost:3000, which is your computer running a web server locally. The design it generated was fairly typical and straightforward, properly capturing the shopping cart, shipping, payment, and confirmation screens I would expect. You can zoom out to see a canvas-style overview and manipulate layers, with a styles tab that lets you inspect and edit elements.

The dealbreaker? Everything gets generated as a single page application, making it frustratingly difficult to locate and edit specific states like shipping or payment. I couldn't find these states visually or directly in the pages panel—they might've been buried somewhere in the layers, but I couldn't make heads or tails of it. When I tried using it again today to capture the styles functionality for the video, I hit the same wall that plagued several other tools I tested—blank previews and errors. Despite going back and forth with the AI, I couldn't get it running again.

**Score: 71**

### Subframe

My time with Subframe revealed a tool that takes a different approach to the same checkout prompt. Unlike most competitors, Subframe can't create an entire flow at once (though I hear they're working on multi-page capabilities). But honestly, I kind of like this limitation—it forces you as a designer to actually think through the process.

What sets Subframe apart is its MidJourney-like approach, offering four different design options that gradually come into focus. These aren't just static mockups but fully coded, interactive pages you can preview in miniature. After selecting a shopping cart design, I simply asked it to create the next page, and it intelligently moved to shipping/billing info.

The real magic is having actual design tools—layers panel, property inspector, direct manipulation—alongside the ability to see the working React code. For designers who want control beyond just accepting whatever the AI spits out, Subframe delivers the best combination of AI generation and familiar design tooling.

**Score: 79**

### Tempo

Lastly, Tempo. This one takes a different approach than most other tools. It starts by generating a PRD from your prompt, then creates a user flow diagram before coding the actual screens—mimicking the steps real product teams would take. Within minutes, it had generated all the different pages for my shopping cart checkout experience. That's impressive speed, but from a design standpoint, it's just fine. The visual design ends up being fairly plain, and the prototype had some UX issues—the payment card change was hard to notice, and the "Place order" action didn't properly lead to a confirmation screen even though it existed in the flow.

The biggest disappointment was with Tempo's supposed differentiator. Their DOM inspector theoretically allows you to manipulate components directly on canvas like you would in Figma—exactly what designers need. But I couldn't get it to work no matter how hard I tried. I even came back days later to try again with a different project and reached out to their support team, but after a brief exchange—crickets. Without this feature functioning, Tempo becomes just another prompt-to-code tool rather than something truly designed for visual designers who want to manipulate components directly. Not great.

**Score: 59**

## The Verdict: Control Beats Code Every Time

Subframe offers actual design tools—layers panel, property inspector, direct manipulation—along with AI chat.

I've spent the last couple weeks testing these prompt-to-code tools, and if there's one thing that's crystal clear, it's this: for designers who want actual design control rather than just code manipulation, Subframe is the standout winner.

I will caveat that I didn't do a deep dive into every single tool. I played with them at a cursory level, giving each a fair shot with the same prompt. What I found was a mix of promising starts and frustrating dead ends.

The reality of AI tools is their probabilistic nature. Sometimes they'll solve problems easily, and then at other times they'll spectacularly fail. I experienced this firsthand when retesting both Lovable and Bolt with their latest features—both performed worse than in my initial testing just two weeks ago. Blank screens. Error messages. No rhyme or reason.

For designers like me, the dealbreaker with most of these tools is being forced to manipulate designs through code rather than through familiar design interfaces. We need to be able to directly manipulate components by clicking and modifying shapes on the canvas or changing values in an inspector. That's where Subframe delivers while others fall short—if their audience includes designers, which might not be the case.

For us designers, I believe Subframe could be the answer. But I'm also looking forward to if Figma will have an answer. Will the company get in the AI > design > code game? Or will it be left behind?

The future belongs to applications that balance AI assistance with familiar design tooling—not just code generators with pretty previews.

> **Note:** The following article is reproduced verbatim from  
> Nielsen Norman Group Team, *Nielsen Norman Group* (2025):  
> [Redefine Your Design Skills to Prepare for AI](https://www.nngroup.com/articles/prepare-for-ai/)  
> for internal educational use only (non-profit).

# Redefine Your Design Skills to Prepare for AI

Generative AI is impacting job markets. According to researchers at Harvard Business School, the German Institute for Economic Research, and the U.K.'s Imperial College London Business School, demand for automation-prone jobs fell 21% eight months after the release of ChatGPT in late 2022. As a designer, how can you navigate these fast-paced disruptions and stay relevant?

## In This Article:

- Automation and Augmentation of Designers' Tasks
- Principle 1: Own Strategic Thinking While Outsourcing Tactical Tasks
- Principle 2: Balance Trust in AI with Scrutiny
- Principle 3: Design for Users and AI Agents
- Principle 4: Embrace Team Augmentation
- Principle 5: Address Unequal Effects to Prioritize Well-Being

## Automation and Augmentation of Designers' Tasks

Technology has been shifting how we complete tasks within most disciplines for decades. Take the role of an accountant: while the title remains unchanged, the day-to-day tasks of an accountant in 1954 (e.g., calculating sums by hand, on paper) are markedly different from those in 2024 (e.g., autocalculating sums in an advanced spreadsheet).

Design tasks depend on our role, goal, and expertise. They include facilitating workshops, ideating solutions, designing wireframes, implementing design systems, and reviewing requirement documents.

Erik Brynjolfsson identified two primary ways in which AI will affect human tasks: automation and augmentation.

A subset of our existing tasks will be automated, while a new set of tasks, previously outside of our capabilities, will become accessible.

### Task Automation

AI will allow us to perform certain tasks more efficiently.

Even today, tools like Miro or Mural can automatically cluster sticky notes, ChatGPT can draft an email. As AI models become more sophisticated, we can expect automation to increase in both scope and intensity.

Automation doesn't mean that tasks will be completely delegated to AI. Rather, it enables designers to work efficiently by combining AI capabilities with human oversight.

Automation isn't an overnight process. The pace varies across different tasks, with some changes happening slowly and others rapidly, but the direction toward increased automation remains consistent.

### Task Augmentation

AI will enable us to perform tasks that traditionally fell outside our professional roles, while also expanding what's possible within our domain. For designers, this means being able to interpret complex data patterns without a data analyst or code prototypes without a developer, and shifting from designing screens to defining the rules that generate them, as well as enhancing our existing capabilities in ways we're just beginning to explore.

### Increasing Competition in Design

Both these processes — automation and augmentation — are happening gradually and in parallel. We'll increase productivity — spending less time on some tasks or accomplishing more in the same amount of time for other tasks — while simultaneously expanding our capabilities into new areas.

As designers take on more tasks outside their expertise, the job market will only become more competitive. An individual contributor will be able to do more, and thus fewer contributors will be needed.

To stay competitive and relevant, designers must be prepared to shift how they think about their skillset.

## Principle 1: Own Strategic Thinking While Outsourcing Tactical Tasks

Not all tasks are created equal.

Most designers perform a range of tasks in their every day work — from tactical to strategic. For example, within a single project, a UX designer may work to define the product vision and create user flows (strategic-oriented tasks), while also organizing design assets and creating developer specs (tactical-oriented tasks).

UX designers balance tactical execution, like organizing assets and writing specifications, with strategic activities such as defining product vision and facilitating stakeholder alignment.

Similarly, a service designer may develop a roadmap and define high-level metrics (strategic-oriented tasks) in parallel to creating handoff documentation and making presentation decks (tactical-oriented tasks).

Service designers combine tactical work like handoff documentation and presentation decks with strategic activities like service metrics and roadmap development.

Choosing the right tasks to automate is critical to maintaining the integrity and value of UX work.

Tactical tasks, like organizing raw data or generating quick mockups, are good candidates for automation because they follow predictable patterns. Offloading these tasks to AI (with proper oversight) can save hours of effort, freeing up time and mental energy for designers to focus on high-impact activities.

In contrast, automating strategic tasks — like crafting a design vision — is risky. These activities require nuanced judgment, contextual awareness, crossdisciplinary collaboration, and an understanding of human behavior that AI, as it stands, cannot replicate.

Embracing automation for tactical efforts while intentionally cultivating strategic thinking ensures that designers maintain their thought-leader and decision-maker roles, while no longer acting as mere executors of predefined processes.

## Principle 2: Balance Trust in AI with Scrutiny

As AI becomes a trusted collaborator in our workflows, we must assess biases that it may introduce. AI systems are only as good as the data they're trained on and the algorithms behind them, which often reflect the limitations and biases existing in the world.

If we over-rely on AI without questioning its outputs, we risk perpetuating these biases. The solutions we design may be disconnected from the needs of our diverse user base. To mitigate this danger, designers and researchers should reflect on potential blind spots, scrutinize AI-generated insights, and validate them against real-world user research.

AI should enhance our ability to think critically, not replace it. By staying grounded in critical thinking, we ensure that AI augments our work without compromising our responsibility to create equitable, inclusive, and meaningful user experiences.

## Principle 3: Design for Users and AI Agents

AI agents (or assistants) will increasingly become active participants in the ecosystems we design. These agents will engage with systems on our behalf, make decisions, and execute tasks, fundamentally reshaping how we deliver our products and services. This shift requires designers to rethink not just user interactions but also the broader dynamics of our field.

To prepare for this future, we must embrace the complexities of designing for a dual audience: human users and their AI representatives. For human users, the design might prioritize clarity, accessibility, and ease of navigation. AI agents might access the system both programmatically and through the same interfaces as humans. However, for them, the design may need to prioritize different aspects of the experience — from structured data to predictable interaction patterns.

For example, imagine you're designing a healthcare appointment-scheduling system. An AI assistant scheduling an appointment on behalf of a user would need to query the system for available times, understand service details, and confirm the booking without manual intervention. By considering how these AI actors will evaluate, interact with, and optimize services on behalf of users, we ensure that our systems serve human needs while acknowledging the growing role of AI intermediaries.

## Principle 4: Embrace Team Augmentation

The augmentation and automation concepts apply to teams, not just tasks. AI-powered tools will extend teams' capabilities beyond traditional boundaries. Designers will do data analysis, developers will craft UIs, and analysts will write code.

Imagine a world where crossfunctional teammates could engage with your domain in ways that seem impossible today, while you simultaneously gain the ability to contribute meaningfully to theirs. This increased functional overlap could unlock numerous, unpredictable possibilities.

I anticipate two key implications of functional overlap: organizational boundaries will be redrawn and radical interdisciplinary collaboration will be enabled.

### Redrawing Organizational Boundaries

Team augmentation might lead to a restructuring of departmental limits within organizations. The nature of these changes will depend on both technological factors (like the evolution of AI tools) and cultural aspects (such as the organizational legitimacy of each team). Restructuring the organizational boundaries might initially seem like a zero-sum game, with departments competing for expanded territories. Be ready to navigate organizational changes and play the political game.

### Enabling Radical Interdisciplinary Collaboration

On a more optimistic note, picture the possibilities for unprecedented crossdisciplinary collaboration. AI tools, particularly those based on large language models (LLMs), could help us communicate complex ideas in terms that nonexperts can readily understand. A designer might use AI to perform initial data analysis and then consult with data scientists to validate the approach and results. Similarly, nondesigners might use AI-powered tools to create interface mockups and interactions while designers shift into expert curator roles, validating and refining the outcomes. In an industry where domain-specific jargon and tools have often hindered collaboration, AI could serve as a universal translator, breaking down these barriers.

AI enables two fundamental shifts in professional collaboration: the evolution of traditional team boundaries (left) and the emergence of fluid knowledge exchange across domains (right).

## Principle 5: Address Unequal Effects to Prioritize Well-Being

UX designers and researchers have long worked to understand how factors like class, race, gender, nationality, and culture intersect with the experiences they design. Our role is to ensure that the systems we design respect and enhance the well-being of everyone they touch — not of just the most visible or powerful groups or of the organizations we work for.

Significant technological shifts, such as AI, risk exacerbating existing inequalities, disproportionately impacting those with less power or representation. It becomes essential that UX professionals embed equity and inclusivity into design and research practices from the start. By engaging diverse perspectives, identifying potential harms early, and rigorously testing our assumptions, we can create AI-powered systems that empower rather than marginalize. As stewards of user-centered design, it is our responsibility to ensure that AI serves as a force of equity and good.

### References

Erik Brynjolfsson. 2022. The Turing Trap: The Promise & Peril of Human-Like Artificial Intelligence. Daedalus 151, 2 (2022), 272-287. https://doi.org/10.1162/daed_a_01915

Ozge Demirci, Jonas Hannane, and Xinrong Zhu. 2023. Who is AI replacing? The impact of ChatGPT on online freelancing platforms. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.4602944

## Related Courses

- **Designing AI Experiences**: Strategize, communicate, and design innovative AI products and features
- **Practical AI for UX Professionals**: Leverage artificial intelligence tools to enhance your UX work and save valuable time
- **Design Thinking Essentials**: Unearth user pain points to drive breakthrough design concepts

## Related Topics

- Artificial Intelligence
- Design Process
- UX Careers

> **Note:** The following article is reproduced verbatim from  
> Adobe Design Team, *Adobe Design* (2025):  
> [Designing for generative AI experiences](https://adobe.design/stories/leading-design/designing-for-generative-ai-experiences)  
> for internal educational use only (non-profit).

# Designing for generative AI experiences

## Shaping interfaces for artificial intelligence requires leaning into specific design skills

I'm now a senior experience designer on Adobe Design's Machine Intelligence New Technology (MINT) team, and alongside the quantum leap that generative AI has experienced there's been a paradigm shift in user expectations for the digital products that incorporate it. People want seamless experiences, personalized recommendations, and adaptive systems that cater to their unique needs and understanding.

Those new expectations are creating an expanding need, and a compelling opportunity, for UX designers to evolve the practice of designing for more static interfaces and begin crafting more natural, intuitive, personalized, human-centered experiences. The shift won't require developing new or different sets of skills, but it will require leaning into those that are most useful.

## The role of design in artificial intelligence

AI models are great at detection and pattern recognition (like spotting faces in a crowd). They're also great at classification (they can neatly organize a jumble of data), prediction (using historical data to forecast), and can make recommendations based on human interaction (learning from previous selections and preferences). They can also synthesize and generate text, images, video, and audio by weaving fragments of information into new creations.

But even with its impressive capabilities, AI walks a fine line between predictability and unpredictability. While it can unfailingly recognize patterns and classify data, there's a degree of nuance that it often misses (like human anatomy). Because models can only make sense of what they've seen before, their ability to grasp the idiosyncrasies of new data is limited. Finally, and most importantly, AI algorithms may not accurately capture or portray human emotion, cultural context, or the depth of personal experience.

When designing interfaces that incorporate AI, designers must never overlook the needs of the people behind each touch and interaction. The foundational focus of designing for them should be to create a reciprocal relationship between the technology and the people using it that evolves in tandem with each technological leap. When there's balance between users, design, and technology, it fosters a cyclical ecosystem where each enhances the other:

- **Users**, integral to the design decision-making process, provide insights and feedback that inform the development of AI systems.
- **Designers** leverage user feedback to enhance the value proposition of AI solutions, ensuring that they meet user needs.
- As **AI technology** evolves, designers adapt user experiences with enriched interaction and more personalized and contextually relevant solutions.

## Key responsibilities of AI designers

Designing experiences is as important as designing the underlying algorithm, infrastructures, and data of the models. Designers not only have the power to shape the future of AI, they also have the responsibility to ensure that the technology isn't just intelligent, but also approachable, useful, and aligns with human value. They can do that by facilitating control, personalizing digital experiences, and building understanding and trust.

### Facilitating agency and control

When designing for AI, designers must consider how to amplify human agency in the AI experience and make room for people's choices and decisions. Designing the experience for text to image in Adobe Firefly began simply: generate images by typing in a prompt. However, when we put ourselves in the shoes of users, we realized how intimidating an empty prompt field can be—crafting the right input may require the user to be equipped with professional knowledge from multiple creative fields and art movements.

To help people better understand and adopt the new task of prompt-writing, the prompt bar on the Firefly homepage uses a familiar search-like interface, where users can enter a prompt in much the same way that they would start a web search. But unlike search bars, a sample prompt describing the background image automatically fills the prompt bar to educate first-time visitors.

Additionally, to reduce the creative blocks that can arise during prompt writing, and to give people more agency over generated images, we created a panel with controls, presets, and parameters that people can use to refine prompts and generated results. Displaying style options, with corresponding categories and thumbnails, fosters a sense of ownership, builds a deeper connection between the user and AI, and encourages people of all skill levels to experiment.

### Personalizing and contextualizing experiences

In today's digital landscape, designers must understand that users want tailored and contextualized experiences (those that are maximally relevant to their individual needs). In UX design, contextual understanding recognizes a user's environment, needs, and situations and adapts experiences to fit them.

Contextual understanding is at the heart of delivering personalized experiences, even those deeply embedded in workflows. On the one hand, the contextual task bar in Adobe Photoshop reveals relevant information, actions, and features during a specific user journey. On the other, AI features like Generative Fill contextually understand image and input to update a section of an image in a consistent style (behind the scenes, AI predicts and fills in missing information based on contextual cues, minimizing the need for manual input).

Multi-modality (the ability to interpret multiple types of inputs like text, voice, or images) and dynamic interactions (a continuous and responsive interaction that adapts to user input in real-time) create rich, flexible, personalized, and contextual experiences. In Project Neo, users can easily create and modify shapes using multiple modes of interaction (3D shapes or multiple camera angles). It's a multifaceted approach that caters to diverse user preferences and skill levels by enabling people to engage with content in personalized and intuitive ways.

### Building understanding and trust

A large part of designing for AI experiences is building empathy between the user and the model to mitigate potential risk. A big step toward doing that is improving explainability/interpretability (providing clear rationale for a model's decisions and outcomes) so people can make sense of what's happening and why.

To improve the explainability of Firefly's Style reference feature, we introduced several design elements to inform people about how AI could enhance the experience. An onboarding tour provides a quick look at the feature and how data is stored. Later in the upload flow a pop-up modal communicates, "You should own this uploaded image" to help people understand the guidelines for uploading reference images.

Design can also make experiences more participatory by reducing jargon to explain the potential issues and abilities of AI. By designing clear feedback systems (and inserting them at the right moments in a user journey) and aligning with people's mental models of AI's abilities and limitations, users won't be easily caught off guard by incorrect or unexpected outputs. And, by predicting and learning from error conditions, designers can build trust and confidence in AI systems. In Firefly, we've made it easy for people to provide feedback and report issues. That explicit feedback allows designers to continually refine AI model quality to enhance the overall experience and it helps build trust by allowing users to make decisions about data provision.

## The soft skills UX designers need to design for AI

The practice of experience design is built on the understanding of our users. Empathy, a skill of UX designers, is undeniably valuable in designing for AI, but there are other qualities that make people particularly well-suited to this type of design work:

- **Thriving in the unknown.** AI is a rapidly evolving landscape, and technological research constantly pushes the threshold of innovation. In this environment, projects may not be predictable or clear. Designing for AI requires a level of comfort in navigating gray areas, exploring multiple paths, and iterating as you learn. That sort of openness to ambiguity, coupled with the ability to leverage user insights, can help designers craft experiences that feel intuitive even within the unknowns of AI.
- **Innate curiosity.** Staying curious about new technologies and learning about the intersection between technology and creativity is an integral part of my daily routine. I actively seek opportunities to experiment with AI-driven tools, attend workshops and conferences to stay updated, engage with communities of like-minded innovators to share ideas and perspectives, and, as much as possible, experiment with and experience new machine learning models.
- **Adaptability.** Five short years ago, image generation technology could only yield blurry, abstract results. Today there's much more control over generative outputs than ever. Designing for AI requires the ability to pivot quickly to integrate new data, models, controls, and user feedback. It also requires close collaborations with other stakeholders such as researchers, product managers, and engineers. Adaptability enables fluid responses to changes in AI capabilities and evolving user needs.
- **Innovating beyond current boundaries.** One of the most exciting aspects of designing for AI means working on cutting-edge and emerging technologies—often with tools and possibilities that haven't been fully explored. People interested in this field of design must feel comfortable thinking ahead to predict the future and look for ways to leverage technologies to create new experiences that may go beyond current user expectations.
- **A systems mindset.** AI designers cannot forget about the larger ecosystem they're designing for. Thinking holistically allows us to consider how different components interact with and affect each other, as well as how each design decision could affect the rest of the ecosystem. Systems thinking helps create coherent, reliable, and ethical AI experiences that feel seamless to users.

## Ways to expand an experience design practice

AI is rapidly maturing. Alongside that rapid growth there's a growing demand for skilled designers in a field that didn't exist three years ago. UX design skills can serve as a base but it's helpful for designers new to the field to understand fundamental concepts about artificial intelligence as well as a bit about what's under the hood. Fortunately, there are multiple avenues for learning, collaboration, and innovation as well as steps designers can take to begin making a career transition into this specialty:

- **Increase your data literacy.** Familiarize yourself with the fundamentals of data science and machine learning. Understand key concepts like data preprocessing, feature engineering, and model evaluation. This will enable you to have more informed discussions with data scientists and engineers, fostering effective collaboration.
- **Experiment with AI tools and platforms.** If you're extra adventurous and want to get hands-on experience with AI, explore online platforms that offer courses and certifications in AI and machine learning. There are also multiple platforms that provide resources and tutorials for designing and implementing AI models. By working on practical projects, you'll gain valuable insights and improve your technical skills.
- **Engage with AI-focused communities.** Connect (online and in-person) with designers, engineers, and industry experts to learn and discuss the latest developments and applications of artificial intelligence.
- **Collaborate.** Work closely with engineers, prototypers, user researchers, data scientists, and other stakeholders throughout the AI development process. Engage in cross-disciplinary discussions to bridge the gap between design and technical implementation.

As AI continues to be incorporated into digital products it will continue to create a compelling need for designers who understand the nuances of designing for those experiences. For experience designers, already accustomed to adapting design processes and approaches as advances in technology require, crafting natural, intuitive, personalized, human-centered experiences for AI will simply require leaning into and sharpening a set of skills they already have.

> **Note:** The following article is reproduced verbatim from  
> Adobe Design Team, *Adobe Design* (2025):  
> [From idea to interface: A designer's guide to AI-powered prototyping](https://adobe.design/stories/leading-design/from-idea-to-interface-a-designer-s-guide-to-ai-powered-prototyping)  
> for internal educational use only (non-profit).

# From idea to interface: A designer's guide to AI-powered prototyping

## Tips for using AI tools to explore design ideas in high fidelity

![A digital illustration of a light bulb with visible filament and screw base, rendered in a pointillism style using small dot of white, yellow, blue, green, and red, set against a dark purple background.](https://adobe.design/stories/leading-design/media_175f36ed1d4fc43c39a14a9b29e5613e335065246.jpg?width=750&format=jpg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

Illustration by Karan Singh

This line-drawing tendency is often visible between design and engineering. Software development tools and workflows, which can take a long time to master, have helped harden divisions between the work of designers and developers. But in the age of AI, those divisions are starting to blur and even shift.

While it's not uncommon for designers to feel intimidated by developer tools, as a design engineering manager on Adobe's Design Prototyping team, I've seen firsthand how AI-powered code generation tools are empowering designers, even those without programming experience. These tools don't just make prototyping possible; they give designers who don't code the ability to explore the key differentiator of their work—the user experience—in high fidelity.

As Adobe's designers have started creating rich, functional prototypes using these new AI workflows, I've thought a lot about why they matter and what they enable. I've come up with some practical tips for designers wanting to get the most out of them.

## The line between engineering and design

Software design and software engineering have long been separated by skill and medium. Designers are experts in user experience and fluent in design tools like Figma. They deliver their work as static designs or user flows to communicate design intent to stakeholders and engineers. Engineers understand code, the languages and systems used to bring design visions to life.

Going from design to code can be a telephone game of unspoken assumptions. It can also introduce friction due to the challenges of translating visual details into code. It's why Adobe has long emphasized prototyping as a key part of the design process. Rapid prototyping, quickly putting an idea to code, has many advantages over static frames:

- It connects designs to real data to provide a more realistic representation of how an application will feel
- It tightens the iterative loop, which makes it easier to identify issues earlier in the process
- It clarifies what's being built and provides a better surface for feedback and conversation

## AI prototyping for designers

Designers and the work of design are changing. The tech industry often refers to unicorns, people equally skilled in design and engineering. These "T-shaped" workers can cover a wide variety of skills but go deep in one domain. AI tools are allowing more designers to fit into this category. And, while it doesn't replace the need for engineers who have deep expertise and ownership, when designers can design with code, it gives them better control over the user experience.

With the pace of technological change and the increasing use of AI in software, designers can use APIs and AI models during the design process to better understand the full product experience. But static designs can fall short of inspiring confidence in the final product. App creation tools like Cursor, v0, Replit, Bolt, and Figma Make can bridge this gap. Designers can simply describe an application, and AI will generate the code for it: It's as simple as typing a prompt, like "a portfolio web layout" or "a circular slider component."

At Adobe, many designers are now leveraging these tools, which range from developer focused code editors to more design friendly Figma plugins and integrations. Emboldened by new superpowers, they can bring their designs to life by working with real data, exploring interactions more directly, playing with complex animations, and using AI models within the design. It gives designers independence and confidence and allows engineers to focus on more technical tasks.

## Practical tips for prototyping with AI

The way designers use AI to generate code differs significantly from how engineers use it: A designer's goal is to build quick, functional prototypes so they can rapidly explore ideas and user experiences. With that in mind, I've compiled a set of practical tips to help designers confidently get started with AI-powered prototyping.

### Plan before building

With AI-powered prototyping there's a tendency to dive in headfirst, but it's essential to think about what you want to make before you start making it. Many AI tools have a chat mode or planning phase; use them to refine your idea and clarify how your design should be implemented. For example, "Create a plan for building an app that turns horoscopes into images. Break it into concrete steps and components." Once it has a plan you like, you can even ask the model to "summarize this plan into a prompt to build the app."

### Consider the size and scope of your project

It's important to consider the complexity of the task you're asking the model to handle. For simpler projects, you can often get by with one-shot prompting, providing a single, detailed instruction like "make a video player component." But because these tools have limited context windows—the amount of information they can "remember" at once—as tasks grow in complexity, it's more effective to break them into smaller, focused steps. Instead of asking for an entire image editing canvas in a single go, try something like, "Create a panel that lists the properties of the active item, such as X, Y, and scale." You can also improve results and stay within context limits by asking an AI agent to split large files into smaller components. And, if you start working on a new feature, consider starting a fresh chat to reset the model's memory.

### Only build what you need

It can be easy with these new capabilities to be too ambitious, like attempting to prototype a full video editing application. But as the code gets larger, it gets harder for an AI agent to manage, and you'll more quickly run into problems. Always remember what you're trying to learn and limit your prompts to what you need. For example, in the context of a video editor, maybe you're just trying to test a timeline, and the rest of the app can be fake or placeholder.

### Be specific about what you're trying to create

The more information you can provide about what and how you want things to work, the better. Generative models don't know what you don't tell them. They will eagerly make assumptions, and it can be hard to make large changes once those assumptions have been made. I once watched a designer use AI to create a prototype of a canvas minimap (a smaller representation of a canvas that a user can navigate quickly). During the process, the AI agent made incorrect assumptions about the size and bounds of the canvas; providing that information up front would have helped it generate more accurate code.

### Provide the model with visuals

Many AI prototyping tools now support image input. You can include screenshots of designs, inspirational images, and links to design files in the chat interface, and the model will try to match those visuals as closely as possible. Some tools can even make things like functional sliders and image drop areas from a design document (this functionality is rapidly improving across the board).

### Don't be afraid to start over

Pay close attention to what the model is doing. Even if you don't follow every detail, you can pause and redirect it if it starts going off track, building unnecessary components. Also, let it know if examples already exist in the codebase and ask it not to duplicate work. And, if the agent gets stuck or falls into a loop, don't hesitate to start over (ideally, you shouldn't be using AI to prototype something so complex that you're afraid to throw it away). If you do hit a wall, ask the AI to summarize what you've already built so you can reboot the session while still carrying forward parts that are useful.

### Remember, perfect is the enemy of good

Prototyping is very different from designing. When you're prototyping, don't get hung up on details, don't over-optimize performance, and don't work on things you won't need until later. When reviewing an engineer's work, you'd of course want to flag performance issues and timing, but when prototyping with AI, those details shouldn't be your focus. If you get too in the weeds, you can waste time on things that aren't important. A good rule to follow: If you find yourself at a point where you want to optimize your prototype, it might be a good sign to start talking to an engineer.

### Accept imperfect code

If you're a designer just starting to prototype with AI tools, you might feel a wave of imposter syndrome. You might worry: "What if an engineer sees this and thinks I have no idea what I'm doing?" That fear is real, and it's one of the biggest momentum-killers in this space. But the truth is, you're not building production code. You're simply expressing an idea. Think of what you're building as a conversation starter, not a perfect solution.

### Lean into the defaults of the model

Every AI model has preferences that reflect their training data. For example, some like to use specific libraries (like React and Tailwind), while others use Shadcn/UI (pre-built React components that use Tailwind for styling). Unless you know what you're doing, it's better to work with those preferences rather than fight them. Getting a model to bend to your will can create more challenges and be difficult to maintain.

### Define your preferences

Many AI prototyping tools allow you to specify exactly how you want them to work and what rules they should follow (like using a certain component library or how to structure your code). These can help ensure that the output matches your vision, but they aren't perfect—these systems can still go off the rails, so don't steer too far from the defaults.

### Use the right model for your job

This is a fast-moving space. Although the latest models usually offer the best results, each one has strengths and weaknesses. Learn as much as you can about each of the different models you're considering, and if you're still not sure, find one that optimizes cost and quality for your specific needs. Then, pay close attention to the recommendations for using it, so you'll have the best experience.

### Have a plan for handling errors

You will run into issues. Simple things (like typos that cause compile errors), and more complicated ones (authentication problems), will come up. When they do, know how to access the error logs in the developer tools of the browser you're using. Once you find the errors, simply copy and paste them back into the chat and the AI agent will often resolve the problem for you.

### Don't forget to "save"

Because coding with AI is "destructive," and changes can replace or break what you previously had, it's easy to lose your work. Make sure to save your progress and familiarize yourself with some kind of version control, like Git, that allows you to store changes to your code at each step so you can bring it back to a working state if things go wrong.

As AI capabilities continue to evolve, and designers increasingly use it to collaborate more effectively across disciplines, it will enable faster, deeper exploration and more intentional design. Ultimately, that will lead to better products for users. While the tools may be changing—AI simply offers new ways to bring design ideas to life—the mission remains the same: Solve real problems with empathy and craft.

> **Note:** The following article is reproduced verbatim from  
> Adobe Design Team, *Adobe Design* (2025):  
> [The Post-test Debrief: A new superpower for growth designers](https://adobe.design/stories/leading-design/the-post-test-debrief-a-new-superpower-for-growth-designers)  
> for internal educational use only (non-profit).

# The Post-test Debrief: A new superpower for growth designers

## An innovative system for transforming A/B testing into customer-centered strategy

![An illustration of colorful data visualizations (bar graphs, pie charts, and line graphs) arranged around a central whiteboard on a blue background. Puzzle pieces in red, yellow, blue, and orange are interspersed, symbolizing interconnected data elements.](https://adobe.design/stories/leading-design/media_1ca53277e36a25a58539d719738913fda94a3c6f9.jpg?width=750&format=jpg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

Illustration by Iuliana Ladewig in Adobe Firefly

The Post-test Debrief, a collaborative process that transforms test data into actionable insights, can change that. It ensures that A/B tests don't just become performance numbers on a dashboard but instead help teams connect the key pieces of a broader story, have meaningful conversations about customer behavior, and ensure customer insights are meaningfully integrated into product decision-making. It's an approach that not only improves efficiency and strategic ideation but also accelerates brainstorming by surfacing key learnings that can shape future experimentation roadmaps.

I'm a staff growth designer on the Adobe Acrobat Growth team, and after five years collaborating across teams and functions, I've come to appreciate how much more powerful A/B testing can be when it's paired with thoughtful collaboration. The Post-test Debrief is one way to turn raw data into shared insight. It can help teams align faster, learn more deeply, and design experiments that aren't just data-informed but customer-focused.

## Why I created the Post-test Debrief

When I joined the Acrobat Growth team, I began conducting user testing to better understand customer pain points when they landed on Acrobat's pricing pages. While the data offered great insights, it didn't help triangulate what was happening and why it was happening.

I wanted to understand our customers' mindsets and whether they had the information they needed to make informed decisions about subscription plans, but it felt like we were testing in a vacuum—it was all raw data. We were generating insights, but because the wrap-up process was minimal, we weren't connecting them. Once results came in, a summary email would go out, and we'd move on to the next item on the roadmap. There was rarely a moment to pause and reflect.

When it came time to brainstorm and ideate, it felt like trying to find last year's holiday gift receipts: frustrating, scattered, and ultimately unhelpful. Ideas were pitched without context, and we were often chasing down old results just to understand whether something had already been tested. But with insights scattered across dashboards, Jira, emails, and Slack threads, trying to track down a past test result often required detective-level skills. ("Didn't we test something similar a few quarters ago? Where are the results of that test?" said every growth team ever.)

![A diagram showing a Microsoft Azure DevOps Kanban board at the center, surrounded by icons for PowerPoint, Excel, SharePoint, Outlook, and Slack. Arrows form a clockwise loop, indicating a connected workflow between the tools.](https://adobe.design/stories/leading-design/media_1afd32719a0b50084b8e4bfe67cbde585bd07b118.png?width=750&format=png&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

The process was time-consuming, frustrating, and inefficient. Imagine writing the same scene in a play over and over—the story would never move forward. More importantly, it was a missed opportunity to use data meaningfully. Without a system to connect past findings to new experiments, we wasted a lot of time. It left me feeling like these ideation sessions could be faster, sharper, and more strategic.

## The Post-test Debrief: The missing link between testing and strategy

Growth designers must fully understand the data and the ins and outs of tests so we can make informed design decisions. Too often, tests are labeled "successful" or "not," with little thought given to what they reveal about user behavior. Here's the truth: When a test doesn't "win," it doesn't mean it failed, it just means you learned something valuable about your customers.

After one A/B test, I gathered the principal manager, Ursula Fritsch, and the data analytics growth manger, Dalton Strayer, and asked if they'd be open to a new approach: a "Post-test Debrief." This self-invented term is a 30-minute calendar block after every A/B test to do a collective, detailed deep dive into the test and data so we can start knitting a narrative of what's happening with our customers.

That first Post-test Debrief, which came on the heels of a successful test, was like a highlight reel of valuable insights that could influence future decisions. Instead of burying insights it created a structured way to capture learnings, spark new ideas, and set the stage for faster, smarter brainstorming. It was the secret sauce that turned raw test results into actionable strategies. Even though our test was a big winner, we didn't just look back, we knit a story about what was happening with customers to ensure we're creating delightful experiences through testing.

Growth teams are often expected to produce year-long roadmaps, and the Post-test Debrief has kept us responsive to real-time insights and evolving customer behavior. Imagine you're planning next quarter's roadmap, would you rather:

1. Scramble through old docs and Slack messages to excavate what worked last time, remember what was tested, or to decide what you should test this time around?
1. Open an organized online whiteboard full of past test insights, behavioral patterns, and fresh hypotheses that you can easily jump into and digest because you already noted important information and emerging themes?

Here's what the second option changed for my team:

- From data points to customer stories. Instead of tracking clicks, we uncover why our customers behave in certain ways. That allows us to start knitting a story about what's working for our customers and what's not.
- From siloed insights to shared knowledge. Since everyone has access to test data, single-person knowledge banks cease to exist. The Post-test Debrief gives growth designers a platform to share user testing findings from a UX perspective and opens the door for growth designers to have collaborative, trust-building discussions with product managers and data analysts.
- From slow brainstorming to fast, focused ideation. The Post-test Debrief creates a continuous loop of insights, so teams never start from zero. When it's time to ideate, we open our board with our repository of debriefed tests and pull in all the respective hypotheses, both new ideas and those that build continued customer stories, to test in the upcoming quarter.

## How to implement a Post-test Debrief

Once test data has matured and stabilized, I add 30–40 minutes to the calendars of Acrobat's data analyst and product manager. Before we meet, I compile information and data from the Jira ticket and Adobe Analytics, important insights and interesting findings from user testing, as well as hypotheses, business goals, and any other meaningful deep dives.

### Who should take part?

If you're involved in A/B testing, this process is for you. That can mean growth designers, product/growth managers, data analysts and researchers, and almost anyone interested in the process.

Growth designers will get more and better insights into user behavior to design experiences that actually work. The process can help product and growth managers build data-backed strategies instead of taking shots in the dark, while enabling data analysts and researchers to bridge gaps between numbers and actionable insights.

### What's on the agenda?

A structured discussion framework ensures you're not just reviewing data but making it actionable. Meanwhile, a data analytics partner can walk the team through the new information while they take note of test findings and the sparks of new ideas. The goal should be a collaborative discussion, not just a data read out.

### What format to use?

Use an online whiteboard so it's easily accessible to the entire team, allowing anyone in the group to jump onto the board and jot down comments and important information. By capturing the conversation in a searchable, visual repository, the entire team has a living, breathing knowledge bank of past experiments. Focus content into pillars:

- What we know and learned: The facts, the surprises, and the hard data takeaways
- General observations: Broader trends and unexpected patterns
- Emerging themes and hypotheses: What the test suggests and what should be explored next

![A screenshot of an digital whiteboard titled "A/B Test Title—Testing new numbers on page." The slide is divided into sections: Hypothesis, Business Goal, and User Goal on the fart left; What we have learned and know, General observations, Emerging themes and hypotheses (each containing sticky notes with text); and comparison charts labeled Control and Challenger that display different metrics and data points with highlights in vibrant colors.](https://adobe.design/stories/leading-design/media_1b5f0f1a9e44e412c632a0f045345d3dbff2babaf.jpg?width=750&format=jpg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

### Ready to try it?

Start small. Pick your next A/B test and schedule a Post-test Debrief. Create a shared space to document key takeaways. Set a recurring cadence so it becomes part of your team's experimentation process. Encourage cross-functional participation for richer insights. Then watch as your experimentation practice becomes more efficient, insightful, and strategic than ever before.

## When A/B testing is human-centered it drives real results

At the end of the day, great design and product decisions aren't just about data, they're about understanding people, so we can design delightful products for them. The Post-test Debrief helps teams connect the dots between A/B tests and real user behavior, making every experiment count, and driving real results:

- Faster quarterly planning: No more wasting weeks trying to remember past tests. Teams can dive straight into ideation.
- Stronger hypothesis development: New tests, built on validated learnings, lead to smarter future experiments.
- Richer customer understanding: Tests aren't just about conversion rates; they also help map user behavior and motivations.
- Higher-quality tests: Instead of testing the same ideas over and over, each experiment moves the strategy forward.
- Proactive instead of reactive teams: Instead of scrambling to figure out "what to test next," teams have a clear roadmap of ideas grounded in real insights.

Let's stop running tests in isolation and start weaving data into actual stories about our customers.

> **Note:** The following article is reproduced verbatim from  
> Adobe Design Team, *Adobe Design* (2025):  
> [Building product alignment through design strategy](https://adobe.design/stories/leading-design/building-product-alignment-through-design-strategy)  
> for internal educational use only (non-profit).

# Building product alignment through design strategy

## How the design team behind Adobe GenStudio strengthens stakeholder confidence and conviction

![A digital illustration on a navy blue background with randomly placed stars. A Black woman wearing a light blue polo shirt, pink jeans, and red and white snearkers is holding a colorful, tangled string of shapes and lines that extend from both sides. On the left side, the strings are chaotic and intertwined with various shapes (circles, squares, ovals and rectangles). On the right side, the strings are organized into a complex circuit-like pattern with the original circles, squares, ovals, and rectangles accompanied by stylized charts and graphs.](https://adobe.design/stories/leading-design/media_12d4515cd9b1f516086edd49884f52ba42fb43b5d.jpg?width=750&format=jpg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

Illustration by Kenzo Hamazaki

It was a massive undertaking that required our design team to make a complex and layered product look and feel as simple and intuitive as possible. An effective design strategy was key.Equally important was building confidence and conviction among our stakeholders. If stakeholders aren't aligned and focused on the same vision and future, a design strategy will fail. Achieving alignment between teams requires understanding business and stakeholder goals, thinking about future uses for products and the technology behind them, and, finally, helping stakeholders fully understand a design strategy.

## Laying the groundwork for design strategy

Design strategy is many things to many people but for me, it's solving problems at the intersection of technology, business, and end users. If we aren't looking at that middle, no strategy will work. Design's strength is understanding and empathizing with the needs of users, but to drive strategy designers must also understand the goals and data driving business and the underlying technology of products.

### Understanding business and stakeholder goals

Design strategy must consider the entire product development process to define a clear North Star and scope a minimally viable experience. That means understanding use cases, users, workflows, journeys, and capabilities. It also means ensuring that what's been designed can be built, which means implementation checks, collaboration, and accountability with engineering partners. Finally, since nothing that's designed is ever really "done," getting to "great" requires buy-in and ongoing, great working relationships and collaboration with cross-functional partners.

For GenStudio, business and stakeholder goals included bringing together disparate product capabilities, tight timelines, generative AI capabilities, and a desire to co-create with customers (internal and external). To build confidence and conviction in our design concepts, we leveraged data from other products, interactions with customers, internal marketing team co-creation, and tangential product data (feature or workflow usage, like determining the most used parts of Adobe Workfront, that could help us define the experience) to build a shared vision across design, product, engineering, go-to-market, and executives.

After building that conviction and alignment we could spend our time driving experience alignment, quality, and strategy instead of debating our destination.

### Understanding how products are built and the technology they're built on

When design teams have clarity on how products are built, and the technology they're built on, it allows them to innovate in ways that are achievable and delightful rather than out of touch or impossible to achieve. Feasibility puts constraints on the design process based on what's "possible" and allows designers to push for new architecture or functionality to drive product innovation.

For GenStudio we had to understand how best to combine the deep capabilities of shared services and components into fantastic end-to-end workflows. To build a strategy for these disparate services and components we had to negotiate their evolution without breaking the underlying foundation for users and use cases. But because integrations and cross-team collaboration are where magic can happen, we focused on the commonalities rather than differences. While not an easy task because of the level of cross-cloud decision making it required, in the end, it enabled us to move faster, reduced duplication and wasted effort, and allowed all boats to rise (our partners' and ours) by evolving the services in partnership.

## Strategic foresight and its role in design strategy

It can be difficult for the human brain to deeply consider a future that's somewhere between rosy and doom and gloom. Strategic foresight developed as an academic practice to think about, define, and expand on possible futures to get closer to a preferable one. To do that, it looks across the entire landscape including the reality we're in, market forces, trends, financial dynamics, cultural signals, and what technologies are emerging.

For me, it's very connected to the things that define design strategy: It can provide a structure for helping teams imagine different future outcomes by helping them stay open to ideas and ways to achieve a desired outcome. One of the ways it can be brought into the process without derailing an organization's ways of working is to look at signals from inside the organization—customer sentiment, goals defined by leadership, what's being talked about, what teams are growing—while also scanning the external landscape for new ideas, innovative technologies, and ways of working.

With GenStudio we're designing for the future of how marketing and creative teams do business, so we're constantly defining design strategy by looking at signals to triangulate technology, business, and end users.

## Strategic facilitation: Helping teams understand and buy in to design strategy

By staying focused on the strategic objectives and outcomes of their work (where business, users, and technology intersect), design teams can create clarity for stakeholders so they can think past organizational boundaries and points of view. Driving alignment through strategic facilitation (a non-subjective and robust approach to decision making) requires using research to understand business and go-to-market goals, proof of concepts and design artifacts as validation, and sense-making to build conviction and confidence in an aligned design strategy.

### Research as proof of concept

If you look closely at product development, designers are often the people thinking deeply about the right thing for how humans need or want to work. Research is design's key tool for building conviction about people and experiences. When mapped to workflows, it looks at how people work, think, and behave (and, maybe most importantly, how they say they behave versus how they actually behave).

### Validate with the design artifacts

Designers have many ways to test strategy and to gauge sentiment internally and externally regardless of differing points of view. Visualization through design artifacts (prototypes, visual models, infographics, workflow diagrams) are facilitation tools, that help people make connection points. These are often not traditional "designs," they're simply a means to visualize and clarify things—to get to the right conversations so we can create the best user-driven experience. I talk often to my team about the artifacts that can drive alignment before getting to a "final design," that's often too late to have an impact on what's being built.

While working on GenStudio we had a conversation with stakeholders about the Adobe-wide strategy for the content supply chain (the lifecycle of digital content management, from creation and documentation to delivery and measurement, to ensure it meets the needs of all stakeholders). Everyone came to it with different mental models and different points of view that were often difficult to understand across silos. We ended up spending time whiteboarding and defining a light visual representation of the possible workflows people were describing. It wasn't a UI, it was a simple drawing with a clear workflow using arrows and text, but it was proof of an idea and concept that everyone understood and could say "yes" to. This created clarity on the destination for people across the organization.

### Sensemaking: An essential tool for building understanding

To drive alignment, design teams must help stakeholders make sense of design strategy. We do that by connecting dots to ensure the elements are understandable:

- By distilling content to a single page or, even better, a single visual. Very often people say the same things in different ways. Distilling content from meetings, research, and conversations into a digestible format (a single page or visual) makes ideas coherent and fosters understanding.
- By prioritizing based on customer needs, business goals, and available technology. When there's so much amazing technology available, that "palette to paint with" can cause teams to overlook the components of successful design strategy—where human needs, business goals, and technological capability intersect. When teams misprioritize they can end up with anti-value for customers, misidentify business goals, or misconnect to available technology.
- By being clear about thoughts, requests, and when to say no. There are always too many exciting things to be done so design teams must be able to focus on the few items that need design strategy, while working in parallel to execute against more straight-forward problems.
- By creating conviction and confidence in design strategy. Designers are magic makers who can turn a disparate set of requirements into a beautiful experience. Never rely on opinion-based design, with no expression of conviction or confidence in why a strategy needs to happen, or the opinion of the loudest person in the room will be the only one anyone hears. Create conviction through data, research, and a thorough understanding of the problem, then articulate it back visually to ensure everyone is aligned.
- By clearly defining the problem and the solution. Always make sure you're facilitating toward a strategy that's clearly defined, technology that's workable, business goals that are achievable, and a specific set of user needs.

## Tips for strategic facilitation

Design strategists see challenges and disconnects between what's being said and how that may translate to an experience, or how two stakeholders may be saying something similar but have very different assumptions or expectations. Strategic facilitation helps align teams by ensuring that every viewpoint is heard, understood, and respected. Some things to keep in mind when getting started:

- Stay neutral while driving the strategy forward. As strategic facilitators, it's easy to get caught up in opinions about what you think is "right." To drive strategy forward, focus first on the objective, the output, and the outcome, then advocate from your position (for design it's usually user needs) while trying to tame your biases. If you can't be neutral, at least be honest and transparent about where you're trying to go and why—without sugarcoating or trickery.
- Maintain a curious and equanimous mindset. Approach your partners with curiosity and openness. Instead of always trying to drive decisions, listen to, and try to understand, people's concerns, worries, and excitement.
- Be aware of challenges while looking for points of synergy rather than points of difference. When discussing design strategy, alignment never happens 100% of the time. To create a foundation based on agreement, try focusing on the 80% where you are aligned with stakeholders rather than the 20% where you're not.
- Share your knowledge. Be generous with information and make sure it's understandable and easy to consume. Alignment is easier when everyone has the same view of the problem.
- When you're wrong, be honest about it. We're all wrong some of the time but it can be difficult to own up to it. When teams can say, "Last time we made the wrong bet," that feedback can be integrated, and teams can move forward with increased understanding.

- Create conviction through data, research, and a thorough understanding of the problem, then articulate it back visually to ensure everyone is aligned.

Effective, thoughtful design strategies solve problems at the intersection of technology, business, and end users. Part of our job as designers is to help build stakeholder confidence and conviction in the strategies we're presenting... because when we don't, our strategies are destined to fail.

> **Note:** The following article is reproduced verbatim from  
> Adobe Design Team, *Adobe Design* (2025):  
> [The path to design principal: Mastering archetypes and defining success](https://adobe.design/stories/leading-design/the-path-to-design-principal-mastering-archetypes-and-defining-success)  
> for internal educational use only (non-profit).

# The path to design principal: Mastering archetypes and defining success

## The skills that can elevate your design career and your influence as an individual contributor

![A six by four grid divided by deep purple lines. Inside each square is an irregular hexagonal shape on a background of vibrant color (either blue, yellow, orange, or pink). The inside of each hexagon is randomly pattered with stripes, flowers, checkerboards, dots, oversized type, or wavy lines.](https://adobe.design/stories/leading-design/media_1960f957857dd842c5fe824ad2475909eb8aa95c2.jpeg?width=750&format=jpeg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

Illustration by Ellen Porteus

Drawing from personal experience, we've distilled the unique ways designers can approach leadership at the principal level and used them to create a set of archetypes that help illustrate the skills needed to succeed. Each principal designer embodies these archetypes and shapes them to their unique expertise and style of leadership.

By sharing these six archetypes we hoped to make it easier for Adobe Design's upcoming designers to make informed choices about successful leadership as design ICs, but we believe they can also help guide designers across the industry shape their career paths. Becoming a principal designer isn't about following a single career path, it's about mastering a mix of skills and strengths to shape the career you envision.

## Principal designer archetypes

We analyzed the skills we use daily, and looked at our industry counterparts, to create six principal designer archetypes. Each one represents a skill required to succeed at the principal designer level, but as individuals, every principal designer has a unique blend of strengths and backgrounds that form a unique leadership style.

Visionary: Being a visionary principal means predicting where industry trends are heading and coming up with innovative ideas and pioneering future trends. It's about envisioning the future and positioning yourself as a forward-thinker in your area of expertise.

Evangelist: This principal is expert at creating visibility for new ideas, products, or features both internally and externally. Promoting your work, so it has visibility beyond your immediate team, is key to establishing this type of leadership presence and building a wide internal audience is key to gaining traction. More than one of our principal designers has spent months at a time evangelizing new products within the company—both to gain internal support and to secure the resources for those products to be successful.

Researcher: A principal who embodies the researcher role forms hypotheses, takes part in research efforts, and grounds their ideas in the data they gather. This archetype focuses on ensuring that ideas are well-informed and backed by solid evidence. Sometimes this means finding ways to gather data on your own or seeking sources for the kind of data that will best support the story you need to tell.

Innovator: This entrepreneurial archetype presents new solutions or strategies for business ideas or products. This role involves identifying market gaps and developing innovative solutions to drive significant business results. More than one of Adobe Design's principal designers has strong skills in this area. It's most often seen when a principal's work is focused on evangelizing new products or innovative features both inside the company and in the design community (at conferences, and in articles and interviews).

Strategist: This principal type emphasizes strategic thinking and orchestrates plans to effectively and collaboratively launch new initiatives or products. The strategist shows up most often on 1.0 products to help guide and inform strategies for launching them and facilitating alignment around key user-focused experiences.

Subject matter expert: The subject matter expert focuses on becoming the authoritative voice in a specific area of knowledge. By honing expertise, they become the go-to person for answers and opinions, solidifying themselves as an invaluable resource within an organization. Many of our principal designers are subject-matter experts in a specific knowledge area (like video or emerging technologies) and are sought out by product teams and leadership to consult on or lead projects in their specialty.

## Archetype combinations: A different shape for every leader

The path to principal is demanding. It's also deeply rewarding and offers the opportunity to make a significant impact on a company and on the products it builds. Archetypes provide some clarity on the skillsets needed to move up the IC career path towards principal designer.

To show how differently these archetypes can be combined, each of Adobe's principal designers filled out a self-assessment, scoring themselves from 1 to 10. Charting the grades we give ourselves helps us visualize how each of us brings a distinct shape to the role—every cumulative score creates a slightly different shape.

![A hexagonal graph, with words at each vertice, on a pale yellow background. Clockwise from top left in black type: Thought leadership/Visionary, Researcher, Evangelist. Entrepreneur/Innovator, Strategist, SME (Subject Matter Expert). Charted inside the hexagonal graph, to the left of center, is an irregular hexagon in pink.](https://adobe.design/stories/leading-design/media_10e8e54cc11b2b7eff1f8407ca3f1423babc2110b.jpeg?width=750&format=jpeg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

![A hexagonal graph, with words at each vertice, on a pale yellow background. Clockwise from top left in black type: Thought leadership/Visionary, Researcher, Evangelist. Entrepreneur/Innovator, Strategist, SME (Subject Matter Expert). Charted inside the hexagonal graph, to the left of center, is an irregular hexagon in pink.](https://adobe.design/stories/leading-design/media_1c79b804fa1c5df9edf7b73e8bebf1e16c535bf25.jpeg?width=750&format=jpeg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

![A hexagonal graph, with words at each vertice, on a pale yellow background. Clockwise from top left in black type: Thought leadership/Visionary, Researcher, Evangelist. Entrepreneur/Innovator, Strategist, SME (Subject Matter Expert). Charted inside the hexagonal graph, to the right of center, is an irregular hexagon in pink.](https://adobe.design/stories/leading-design/media_15c3f6b7ef5d94e311261262057df2f3d2325e26f.jpeg?width=750&format=jpeg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

## Using the archetypes in your own career path

Every principal designer has their own unique combination of strengths, skills, and archetypes—and the same is true for the career path that led them to principal. Every designer has a different path, but there are some universal guidelines at the core of everyone's unique journey:

Find your specialization: Principal designers are known for their specific areas of expertise. To excel in this role, discover what truly ignites your passion within design and hone your skills accordingly. If you're not sure where to start, consider the archetypes and think about whether your strengths point you in a specific direction. Principal designers come in many forms, each with their own unique strengths and approaches. Explore the archetypes, but don't be afraid to create your own style by blending elements from different categories. Adobe Design principals have specializations (while one specializes in animation, another specializes in data visualization) that inform their work.

Build visibility: Climbing the ladder to principal often means extended exposure to the VP level or higher. Cultivate visibility within your product or team (and your design organization at large) so you can demonstrate your value and impact on the broadest scale.

Collaborate beyond borders: Successful principal designers work seamlessly with cross-functional partners, extending their influence beyond their immediate product team to shape the broader business and product landscape.

Measure your impact: To move up the IC career ladder you must be able to prove your impact on the business. This becomes more important the further up the career ladder you go. Remember: Your impact on the business is not the same as the amount of effort you put into doing your job or the number of tasks you take on.

## A self-assessment for your own archetype map

For designers considering this career path, the most important thing to develop is your specialization—some kind of work, or pattern of work, that you enjoy or are good at (such as specializing in a specific area like data visualization, or in designing for 1.0 products). You don't have to know everything about it, but the path to principal will be possible because you'll be known for that knowledge and your leadership potential in the subject area.

So much work as a principal is self-directed and a specialty provides the anchor for that self-direction. It's also what will create visibility for your work, enable you to transcend team boundaries to work with people outside of your immediate team, and provide the foundation for increasing your skills as you navigate this high-level IC career path.

Doing this archetype self-assessment can be a useful exercise and a guide to areas that you're interested in. And, because the role is so much more self-guided than other roles before it, it can help identify skills that might need to be strengthened as you move up this career ladder.Spend some time with this blank archetype map and rate how strongly you identify with each of the archetype areas on a scale of 1 to 10, with 10 being the strongest personal competency in an area and 1 the weakest. While you don't need to strive for tens on all axes, looking at those where you're weaker can provide some direction as to where a focus on improving your skills would create a more well-rounded approach.

![A hexagonal graph, with words at each vertice, on a pale yellow background. Clockwise from top left in black type: Thought leadership/Visionary, Researcher, Evangelist. Entrepreneur/Innovator, Strategist, SME (Subject Matter Expert).](https://adobe.design/stories/leading-design/media_1816cdd0d524f23e4f4f8e7a28667c35e2599155e.jpeg?width=750&format=jpeg&optimize=medium)
<sub>Source: *Adobe Design*, Adobe Design Team (2025).</sub>

If you're aspiring to reach the principal designer level in your work, your career journey will be unique to you—each person will discover their own strengths, face their own unique challenges, and craft the role that aligns with their passion and vision. As you navigate challenges and seize opportunities, remember that the principal designer role is not a destination, but a journey of growth, impact, and continuous learning. Embrace the adventure and let your passion for design drive you forward as you shape the future of your career.

## Sources

- [Design Patterns: Elements of Reusable Object-Oriented Software](https://en.wikipedia.org/wiki/Design_Patterns)
- [Python Design Patterns](https://python-patterns.guide/)
- [Refactoring Guru - Design Patterns](https://refactoring.guru/design-patterns)

## Collaboration Prompts for Engineers

### For Frontend Developers
"Implement the Observer pattern for real-time UI updates"

### For Backend Developers
"Design a Strategy pattern for different authentication methods"

### For DevOps Engineers
"Create a Factory pattern for different deployment strategies"

### For Data Scientists
"Build a Chain of Responsibility pattern for data validation pipelines"
