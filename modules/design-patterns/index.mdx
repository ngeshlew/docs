---
title: "Design Patterns"
slug: "modules-design-patterns"
updatedAt: "2025-08-18"
tags: [module, design-patterns, architecture, best-practices]
---

# Design Patterns

> Learn essential design patterns for building scalable, maintainable, and efficient AI-powered applications.

## What are Design Patterns?

Design patterns are proven solutions to common problems in software design. They provide reusable templates for solving recurring design challenges, making code more maintainable, scalable, and understandable. In AI applications, design patterns help structure complex interactions between models, data, and user interfaces.

## Why Design Patterns Matter

### Benefits

- **Reusability**: Proven solutions that can be applied across projects
- **Maintainability**: Well-structured code that's easier to understand and modify
- **Scalability**: Patterns that support growth and complexity
- **Consistency**: Standardized approaches across teams and projects
- **Best Practices**: Incorporates industry-proven methodologies

### Common Challenges in AI Applications

- Complex model interactions
- State management across sessions
- Error handling and fallbacks
- Performance optimization
- User experience consistency

## Creational Patterns

### Singleton Pattern

Ensure a class has only one instance and provide global access to it.

```python
class AIConfigManager:
    _instance = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self.config = \{\}
            self._initialized = True
    
    def set_config(self, key: str, value: any):
        self.config[key] = value
    
    def get_config(self, key: str):
        return self.config.get(key)

# Usage
config_manager = AIConfigManager()
config_manager.set_config('model_name', 'gpt-4')
```

### Factory Pattern

Create objects without specifying their exact class.

```python
from abc import ABC, abstractmethod
from typing import Dict, Any

class AIModel(ABC):
    @abstractmethod
    def generate(self, prompt: str) -> str:
        pass

class GPTModel(AIModel):
    def generate(self, prompt: str) -> str:
        return f"GPT response to: \{prompt\}"

class ClaudeModel(AIModel):
    def generate(self, prompt: str) -> str:
        return f"Claude response to: \{prompt\}"

class AIModelFactory:
    @staticmethod
    def create_model(model_type: str) -> AIModel:
        models = {
            'gpt': GPTModel,
            'claude': ClaudeModel
        }
        
        if model_type not in models:
            raise ValueError(f"Unknown model type: \{model_type\}")
        
        return models[model_type]()

# Usage
factory = AIModelFactory()
gpt_model = factory.create_model('gpt')
claude_model = factory.create_model('claude')
```

## Structural Patterns

### Adapter Pattern

Allow incompatible interfaces to work together.

```python
class LegacyAIService:
    def process_text(self, text: str) -> Dict[str, str]:
        return \{"result": f"Legacy processed: \{text\}"\}

class ModernAIInterface:
    def generate_response(self, input_text: str) -> str:
        return f"Modern response: \{input_text\}"

class AIAdapter:
    def __init__(self, legacy_service: LegacyAIService):
        self.legacy_service = legacy_service
    
    def generate_response(self, input_text: str) -> str:
        # Adapt the legacy interface to the modern one
        result = self.legacy_service.process_text(input_text)
        return result["result"]

# Usage
legacy_service = LegacyAIService()
adapter = AIAdapter(legacy_service)
response = adapter.generate_response("Hello, world!")
```

### Decorator Pattern

Add new functionality to objects without altering their structure.

```python
from abc import ABC, abstractmethod

class AIResponse(ABC):
    @abstractmethod
    def get_response(self) -> str:
        pass

class BasicAIResponse(AIResponse):
    def __init__(self, text: str):
        self.text = text
    
    def get_response(self) -> str:
        return self.text

class ResponseDecorator(AIResponse):
    def __init__(self, ai_response: AIResponse):
        self.ai_response = ai_response
    
    def get_response(self) -> str:
        return self.ai_response.get_response()

class LoggingDecorator(ResponseDecorator):
    def get_response(self) -> str:
        response = self.ai_response.get_response()
        print(f"Logging response: \{response\}")
        return response

class CachingDecorator(ResponseDecorator):
    def __init__(self, ai_response: AIResponse):
        super().__init__(ai_response)
        self.cache = \{\}
    
    def get_response(self) -> str:
        # Simple caching implementation
        response = self.ai_response.get_response()
        if response not in self.cache:
            self.cache[response] = response
        return self.cache[response]

# Usage
basic_response = BasicAIResponse("Hello, AI!")
logged_response = LoggingDecorator(basic_response)
cached_logged_response = CachingDecorator(logged_response)
result = cached_logged_response.get_response()
```

## Behavioral Patterns

### Observer Pattern

Define a one-to-many dependency between objects.

```python
from abc import ABC, abstractmethod
from typing import List

class Observer(ABC):
    @abstractmethod
    def update(self, message: str):
        pass

class AIService:
    def __init__(self):
        self.observers: List[Observer] = []
    
    def add_observer(self, observer: Observer):
        self.observers.append(observer)
    
    def remove_observer(self, observer: Observer):
        self.observers.remove(observer)
    
    def notify_observers(self, message: str):
        for observer in self.observers:
            observer.update(message)
    
    def process_request(self, request: str):
        # Process the request
        result = f"Processed: \{request\}"
        # Notify all observers
        self.notify_observers(result)

class LoggingObserver(Observer):
    def update(self, message: str):
        print(f"Logging: \{message\}")

class MetricsObserver(Observer):
    def update(self, message: str):
        print(f"Metrics: Processing request of length \{len(message)\}")

# Usage
ai_service = AIService()
logging_observer = LoggingObserver()
metrics_observer = MetricsObserver()

ai_service.add_observer(logging_observer)
ai_service.add_observer(metrics_observer)

ai_service.process_request("Hello, AI!")
```

### Strategy Pattern

Define a family of algorithms and make them interchangeable.

```python
from abc import ABC, abstractmethod

class PromptStrategy(ABC):
    @abstractmethod
    def generate_prompt(self, input_text: str) -> str:
        pass

class SimplePromptStrategy(PromptStrategy):
    def generate_prompt(self, input_text: str) -> str:
        return f"Answer this question: \{input_text\}"

class DetailedPromptStrategy(PromptStrategy):
    def generate_prompt(self, input_text: str) -> str:
        return f"""
        Please provide a detailed and comprehensive answer to the following question.
        Include examples, explanations, and relevant context.
        
        Question: \{input_text\}
        
        Please structure your response with:
        1. Direct answer
        2. Explanation
        3. Examples
        4. Related concepts
        """

class CreativePromptStrategy(PromptStrategy):
    def generate_prompt(self, input_text: str) -> str:
        return f"""
        Think creatively and provide an innovative perspective on:
        \{input_text\}
        
        Consider:
        - Alternative viewpoints
        - Creative solutions
        - Future implications
        - Unconventional approaches
        """

class AIContext:
    def __init__(self, strategy: PromptStrategy):
        self.strategy = strategy
    
    def set_strategy(self, strategy: PromptStrategy):
        self.strategy = strategy
    
    def process_input(self, input_text: str) -> str:
        prompt = self.strategy.generate_prompt(input_text)
        # Here you would send the prompt to an AI model
        return f"Generated prompt: \{prompt\}"

# Usage
simple_strategy = SimplePromptStrategy()
detailed_strategy = DetailedPromptStrategy()
creative_strategy = CreativePromptStrategy()

context = AIContext(simple_strategy)
print(context.process_input("What is AI?"))

context.set_strategy(detailed_strategy)
print(context.process_input("What is AI?"))

context.set_strategy(creative_strategy)
print(context.process_input("What is AI?"))
```

## AI-Specific Patterns

### Chain of Responsibility Pattern

Pass requests along a chain of handlers.

```python
from abc import ABC, abstractmethod
from typing import Optional

class Request:
    def __init__(self, text: str, request_type: str):
        self.text = text
        self.request_type = request_type
        self.response = None

class Handler(ABC):
    def __init__(self):
        self.next_handler: Optional[Handler] = None
    
    def set_next(self, handler: 'Handler') -> 'Handler':
        self.next_handler = handler
        return handler
    
    @abstractmethod
    def handle(self, request: Request) -> bool:
        pass

class InputValidationHandler(Handler):
    def handle(self, request: Request) -> bool:
        if not request.text or len(request.text.strip()) == 0:
            request.response = "Error: Empty input"
            return False
        
        if len(request.text) > 1000:
            request.response = "Error: Input too long"
            return False
        
        return self.next_handler.handle(request) if self.next_handler else True

class ContentFilterHandler(Handler):
    def handle(self, request: Request) -> bool:
        inappropriate_words = ['spam', 'inappropriate', 'blocked']
        
        for word in inappropriate_words:
            if word in request.text.lower():
                request.response = f"Error: Content contains inappropriate word: \{word\}"
                return False
        
        return self.next_handler.handle(request) if self.next_handler else True

class AIProcessingHandler(Handler):
    def handle(self, request: Request) -> bool:
        # Simulate AI processing
        request.response = f"AI processed: \{request.text\}"
        return True

# Usage
validation_handler = InputValidationHandler()
filter_handler = ContentFilterHandler()
ai_handler = AIProcessingHandler()

# Chain the handlers
validation_handler.set_next(filter_handler).set_next(ai_handler)

# Test requests
request1 = Request("Hello, AI!", "general")
validation_handler.handle(request1)
print(request1.response)

request2 = Request("", "general")
validation_handler.handle(request2)
print(request2.response)

request3 = Request("This is spam content", "general")
validation_handler.handle(request3)
print(request3.response)
```

### Template Method Pattern

Define the skeleton of an algorithm in a base class.

```python
from abc import ABC, abstractmethod

class AIPipeline(ABC):
    def process(self, input_data: str) -> str:
        """Template method defining the algorithm structure"""
        # Step 1: Preprocess
        processed_data = self.preprocess(input_data)
        
        # Step 2: Validate
        if not self.validate(processed_data):
            return "Validation failed"
        
        # Step 3: Generate response
        response = self.generate_response(processed_data)
        
        # Step 4: Post-process
        final_response = self.postprocess(response)
        
        return final_response
    
    @abstractmethod
    def preprocess(self, data: str) -> str:
        pass
    
    @abstractmethod
    def validate(self, data: str) -> bool:
        pass
    
    @abstractmethod
    def generate_response(self, data: str) -> str:
        pass
    
    @abstractmethod
    def postprocess(self, response: str) -> str:
        pass

class ChatbotPipeline(AIPipeline):
    def preprocess(self, data: str) -> str:
        return data.strip().lower()
    
    def validate(self, data: str) -> bool:
        return len(data) > 0 and len(data) <= 500
    
    def generate_response(self, data: str) -> str:
        return f"Chatbot response to: \{data\}"
    
    def postprocess(self, response: str) -> str:
        return response.capitalize()

class TranslationPipeline(AIPipeline):
    def preprocess(self, data: str) -> str:
        return data.strip()
    
    def validate(self, data: str) -> bool:
        return len(data) > 0
    
    def generate_response(self, data: str) -> str:
        return f"Translated: \{data\}"
    
    def postprocess(self, response: str) -> str:
        return response + " [Translation complete]"

# Usage
chatbot = ChatbotPipeline()
translation = TranslationPipeline()

print(chatbot.process("Hello, how are you?"))
print(translation.process("Bonjour, comment allez-vous?"))
```

## Best Practices

### 1. Choose the Right Pattern

- **Creational**: When object creation is complex or needs to be controlled
- **Structural**: When you need to compose objects or provide alternative interfaces
- **Behavioral**: When you need to manage algorithms, relationships, and responsibilities

### 2. Keep It Simple

- Don't over-engineer solutions
- Use patterns only when they add value
- Consider the complexity trade-offs

### 3. Document Your Patterns

- Explain why you chose a specific pattern
- Document the responsibilities of each component
- Provide usage examples

### 4. Test Your Patterns

- Unit test each pattern implementation
- Test pattern interactions
- Verify that patterns solve the intended problem

## Implementation Examples

### Configuration Management

```python
class ConfigManager:
    def __init__(self):
        self.config = \{\}
        self.observers = []
    
    def set_config(self, key: str, value: any):
        self.config[key] = value
        self.notify_observers(key, value)
    
    def get_config(self, key: str, default=None):
        return self.config.get(key, default)
    
    def add_observer(self, observer):
        self.observers.append(observer)
    
    def notify_observers(self, key: str, value: any):
        for observer in self.observers:
            observer.config_changed(key, value)

class LoggingObserver:
    def config_changed(self, key: str, value: any):
        print(f"Config changed: \{key\} = \{value\}")

# Usage
config = ConfigManager()
config.add_observer(LoggingObserver())
config.set_config('model_name', 'gpt-4')
```

### Error Handling Strategy

```python
class ErrorHandler:
    def __init__(self):
        self.strategies = {
            'retry': self.retry_strategy,
            'fallback': self.fallback_strategy,
            'ignore': self.ignore_strategy
        }
    
    def handle_error(self, error: Exception, strategy: str = 'retry'):
        if strategy in self.strategies:
            return self.strategies[strategy](error)
        else:
            raise error
    
    def retry_strategy(self, error: Exception):
        print(f"Retrying after error: \{error\}")
        # Implement retry logic
        return "Retry successful"
    
    def fallback_strategy(self, error: Exception):
        print(f"Using fallback for error: \{error\}")
        return "Fallback response"
    
    def ignore_strategy(self, error: Exception):
        print(f"Ignoring error: \{error\}")
        return None
```

> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [Figma Make Tutorial: Build Interactive Apps with AI](https://www.codecademy.com/article/figma-make-tutorial)  
> for internal educational use only (non-profit).

# Figma Make Tutorial: Build Interactive Apps with AI

Ever designed a great UI and thought, "I wish I could just make this work without jumping through extra tools or workflows"? What if you could bring your designs to life by describing what you want, no handoff, plugins, or context switching? That's precisely where Figma Make comes in!

Figma Make, launched at Config 2025, is Figma's AI-powered prompt-to-code environment that allows designers to build real, functional prototypes directly inside Figma. Unlike traditional files, it lets us use plain English to add logic, navigation, responsiveness, and interactivity to our designs without requiring manual coding.

Let's build a personalized finance app using Figma Make.

## Building a finance tracker app

In this project, we'll build a personalized finance tracker that helps users do exactly that, with a clean and interactive dashboard powered by Figma Make. This app gives users a snapshot of their financial health. It includes features like:

- A dashboard showing income, expenses, and account balances
- Monthly report generation, styled like a bank statement
- A transaction history with filters, tags, and categories

It's inspired by tools like Plaid, QuickBooks, and Notion-style finance templates, but it's streamlined for personal use.

Let's start by defining the visual structure, the wireframe. A strong wireframe sets the foundation for meaningful AI output.

Also, here's a video tutorial that walks through the same concepts step by step—with visuals, examples, and a real-time build. Use it alongside the text or on its own—whatever helps you learn best.

### Step 1: Create wireframes for your app

A wireframe is a low-fidelity blueprint of your app's layout. Create a wireframe for this app, focusing on the structure, which answers:

- "What goes where?"
- "How do screens connect?"
- "What components does the user need to interact with?"

Unlike typical prototypes, where wireframes are handed off to developers, in Figma Make, your wireframe becomes part of the functional build process. You can:

- Use it to plan the layout and logic clearly
- Feed it directly into the Figma Make file
- Guide your prompts more effectively, since the visual hierarchy already exists

Let's look at each screen in detail.

#### Screen 1: Dashboard overview

The dashboard is the main landing screen. It gives the user details of their current financial health, including their earnings and spending, and forming trends.

This screen will have:

- A top navigation bar with the app name and profile access
- Summary cards for total income and total expenses
- A toggleable line chart that switches views between income and expenses
- Quick action buttons like "Add Transaction", "View Report", "Export Data"

Here's a sample wireframe of the dashboard screen:

![Wireframe of a personal finance dashboard showing income, expenses, trends chart, and action buttons](https://static-assets.codecademy.com/figma-make/Wireframe-1-dashboard.png)
<sub>Source: *Codecademy*, Codecademy Team (2025).</sub>

> Note: The wireframes used here are just examples and you can use your layouts as the foundation for your prompts in Figma Make.

#### Screen 2: Monthly report page

The monthly report page will mimic a formatted statement view. Think of it like a printable snapshot of a user's income and expenses for any given month. It'll have the following components:

- A header with the report title and date filters
- A summary block showing totals and net savings
- A breakdown table by spending category
- A footer for optional disclaimers or notes

Here's a sample wireframe of this screen:

![Wireframe of a monthly financial summary with filters, income and expense overview, and a category breakdown table](https://static-assets.codecademy.com/figma-make/Wireframe-2-Monthly-financial-summary.png)
<sub>Source: *Codecademy*, Codecademy Team (2025).</sub>

#### Screen 3: Transactions details page

This is the most data-dense screen, designed to show the user a history of every transaction they've made. It'll have the following components:

- A search bar and filter options
- A list of transactions with details like date, amount, category, and description
- Pagination controls for large datasets
- Export functionality

Here's a sample wireframe of this screen:

![Wireframe of a transaction history page with search, filters, transaction list, and pagination](https://static-assets.codecademy.com/figma-make/Wireframe-3-Transaction-history.png)
<sub>Source: *Codecademy*, Codecademy Team (2025).</sub>

### Step 2: Set up your Figma Make file

Now that we have our wireframes, let's set up the Figma Make environment:

1. **Create a new Figma Make file**: Go to Figma and create a new file. You'll see a "Make" option in the file type selector.

2. **Import your wireframes**: You can either:
   - Create the wireframes directly in Figma Make
   - Import existing wireframes from other Figma files
   - Use the sample wireframes provided above as reference

3. **Set up the canvas**: Organize your screens in a logical flow, typically left to right or top to bottom.

### Step 3: Start with the dashboard screen

Let's begin building the dashboard. In Figma Make, you can use natural language to describe what you want:

**Prompt example**: "Create a dashboard with a header navigation bar, two summary cards for income and expenses, a line chart that can toggle between income and expense views, and three action buttons at the bottom."

The AI will generate the layout and basic styling. You can then refine it with more specific prompts:

**Refinement prompt**: "Make the income card green and the expense card red. Add icons to the action buttons and make them rounded with a subtle shadow."

### Step 4: Add interactivity and logic

This is where Figma Make really shines. You can add functionality without writing code:

**Navigation prompt**: "When the user clicks 'View Report', navigate to the monthly report screen."

**Data handling prompt**: "Create a data structure for transactions with fields for date, amount, category, and description. Display sample data in the dashboard."

**Chart functionality prompt**: "Make the line chart interactive. When users click the 'Income' or 'Expenses' toggle, update the chart data accordingly."

### Step 5: Build the monthly report screen

For the monthly report, you'll want to focus on data presentation and filtering:

**Layout prompt**: "Create a monthly report layout with a header showing the month and year, summary cards for total income, expenses, and net savings, and a detailed breakdown table."

**Filtering prompt**: "Add date picker controls that allow users to select different months. Update the report data when the date changes."

**Styling prompt**: "Style the report to look like a professional bank statement with clean typography and proper spacing."

### Step 6: Create the transaction history screen

The transaction history screen requires more complex data handling:

**Data structure prompt**: "Create a comprehensive transaction data structure with fields for ID, date, amount, category, description, and tags."

**Search functionality prompt**: "Add a search bar that filters transactions by description or category. Make it work in real-time as the user types."

**Pagination prompt**: "Implement pagination for the transaction list, showing 10 transactions per page with navigation controls."

### Step 7: Connect all screens together

Now it's time to make everything work together:

**Navigation system prompt**: "Create a navigation system that allows users to move between the dashboard, monthly report, and transaction history screens. Add a bottom navigation bar or breadcrumbs."

**Data consistency prompt**: "Ensure that data changes in one screen are reflected across all screens. For example, adding a transaction should update the dashboard totals and appear in the transaction history."

**State management prompt**: "Implement state management so that user preferences, filters, and current view are preserved when navigating between screens."

### Step 8: Add advanced features

Once the basic functionality is working, you can add more sophisticated features:

**Responsive design prompt**: "Make the app responsive so it works well on different screen sizes. Adjust layouts for mobile, tablet, and desktop views."

**Animations prompt**: "Add smooth transitions between screens and subtle animations for user interactions like button clicks and data updates."

**Export functionality prompt**: "Add the ability to export transaction data as CSV or PDF reports."

### Step 9: Test and refine

Testing is crucial for any app, even AI-generated ones:

**User testing prompt**: "Create a test scenario where a user adds a new transaction, views the monthly report, and exports their data. Ensure all flows work correctly."

**Error handling prompt**: "Add error handling for edge cases like empty data, network issues, or invalid user input."

**Performance optimization prompt**: "Optimize the app for performance, especially when handling large datasets or complex calculations."

## Best Practices for Figma Make

### 1. Start with Clear Requirements

Before diving into prompts, clearly define what you want to build:

- **User stories**: Write down what users should be able to do
- **Functional requirements**: List specific features and behaviors
- **Technical constraints**: Consider performance, compatibility, and scalability

### 2. Use Iterative Development

Build your app in small, manageable steps:

- **Start simple**: Begin with basic layouts and functionality
- **Add complexity gradually**: Build upon working features
- **Test frequently**: Verify each addition works before moving on

### 3. Write Effective Prompts

The quality of your prompts directly affects the output:

- **Be specific**: Include details about layout, styling, and behavior
- **Use clear language**: Avoid ambiguous terms and jargon
- **Provide context**: Reference existing elements and relationships
- **Iterate**: Refine prompts based on the results you get

### 4. Leverage Figma's Design System

Take advantage of Figma's built-in design capabilities:

- **Use components**: Create reusable UI elements
- **Apply styles**: Use consistent colors, typography, and spacing
- **Maintain consistency**: Ensure visual coherence across screens

### 5. Test Across Different Scenarios

Don't just test the happy path:

- **Edge cases**: Test with empty data, invalid inputs, and error conditions
- **User flows**: Walk through complete user journeys
- **Performance**: Test with realistic data volumes
- **Accessibility**: Ensure the app works for users with different needs

## Common Challenges and Solutions

### Challenge 1: Complex Data Relationships

**Problem**: Managing relationships between different data entities (transactions, categories, accounts).

**Solution**: Use clear data structures and implement proper state management. Break complex relationships into smaller, manageable pieces.

### Challenge 2: Performance with Large Datasets

**Problem**: The app becomes slow when handling many transactions.

**Solution**: Implement pagination, lazy loading, and efficient data filtering. Consider data virtualization for very large datasets.

### Challenge 3: Responsive Design

**Problem**: The app doesn't work well on different screen sizes.

**Solution**: Design with mobile-first approach and use flexible layouts. Test on various devices and screen sizes.

### Challenge 4: State Management

**Problem**: Data gets out of sync between different screens.

**Solution**: Implement a centralized state management system and ensure all components update consistently.

## Advanced Techniques

### 1. Custom Components

Create reusable components for common UI patterns:

**Prompt**: "Create a reusable transaction card component that displays date, amount, category, and description. Make it customizable for different transaction types."

### 2. Data Visualization

Add charts and graphs to make data more meaningful:

**Prompt**: "Create a pie chart showing spending by category and a line chart showing spending trends over time."

### 3. User Preferences

Allow users to customize their experience:

**Prompt**: "Add a settings screen where users can choose their preferred currency, date format, and default view."

### 4. Offline Functionality

Make the app work without an internet connection:

**Prompt**: "Implement local storage so users can add transactions offline and sync when they're back online."

## Conclusion

Figma Make represents a significant shift in how designers and developers can collaborate. By combining the visual design capabilities of Figma with AI-powered code generation, it opens up new possibilities for rapid prototyping and development.

The key to success with Figma Make is understanding that it's not just about generating code—it's about creating a seamless workflow between design and development. The better you can describe what you want, the better the results will be.

Remember that Figma Make is still evolving, and the quality of your prompts will improve with practice. Start with simple projects, experiment with different approaches, and gradually build up to more complex applications.

As you become more comfortable with Figma Make, you'll find that it can significantly speed up your development process while maintaining the quality and consistency of your designs. The ability to iterate quickly and see immediate results makes it an invaluable tool for modern app development.

Whether you're a designer looking to bring your ideas to life or a developer wanting to prototype quickly, Figma Make provides a powerful platform for building interactive applications with AI assistance.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Beyond The Hype: What AI Can Really Do For Product Design](https://www.smashingmagazine.com/2025/08/beyond-hype-what-ai-can-do-product-design/)  
> for internal educational use only (non-profit).

# Beyond The Hype: What AI Can Really Do For Product Design

These days, it's easy to find curated lists of AI tools for designers, galleries of generated illustrations, and countless prompt libraries. What's much harder to find is a clear view of how AI is actually integrated into the everyday workflow of a product designer — not for experimentation, but for real, meaningful outcomes.

I've gone through that journey myself: testing AI across every major stage of the design process, from ideation and prototyping to visual design and user research. Along the way, I've built a simple, repeatable workflow that significantly boosts my productivity.

In this article, I'll share what's already working and break down some of the most common objections I've encountered — many of which I've faced personally.

## Stage 1: Idea Generation Without The Clichés

Pushback: "Whenever I ask AI to suggest ideas, I just get a list of clichés. It can't produce the kind of creative thinking expected from a product designer."

That's a fair point. AI doesn't know the specifics of your product, the full context of your task, or many other critical nuances. The most obvious fix is to "feed it" all the documentation you have. But that's a common mistake as it often leads to even worse results: the context gets flooded with irrelevant information, and the AI's answers become vague and unfocused.

Current-gen models can technically process thousands of words, but the longer the input, the higher the risk of missing something important, especially content buried in the middle. This is known as the "lost in the middle" problem.

To get meaningful results, AI doesn't just need more information — it needs the right information, delivered in the right way. That's where the RAG approach comes in.

### How RAG Works

Think of RAG as a smart assistant working with your personal library of documents. You upload your files, and the assistant reads each one, creating a short summary — a set of bookmarks (semantic tags) that capture the key topics, terms, scenarios, and concepts. These summaries are stored in a kind of "card catalog," called a vector database.

When you ask a question, the assistant doesn't reread every document from cover to cover. Instead, it compares your query to the bookmarks, retrieves only the most relevant excerpts (chunks), and sends those to the language model to generate a final answer.

### How Is This Different from Just Dumping a Doc into the Chat?

Let's break it down:

**Typical chat interaction**

It's like asking your assistant to read a 100-page book from start to finish every time you have a question. Technically, all the information is "in front of them," but it's easy to miss something, especially if it's in the middle. This is exactly what the "lost in the middle" issue refers to.

**RAG approach**

You ask your smart assistant a question, and it retrieves only the relevant pages (chunks) from different documents. It's faster and more accurate, but it introduces a few new risks:

- **Ambiguous question**: You ask, "How can we make the project safer?" and the assistant brings you documents about cybersecurity, not finance.
- **Mixed chunks**: A single chunk might contain a mix of marketing, design, and engineering notes. That blurs the meaning so the assistant can't tell what the core topic is.
- **Semantic gap**: You ask, "How can we speed up the app?" but the document says, "Optimize API response time." For a human, that's obviously related. For a machine, not always.

![Diagram showing how RAG works: a user prompt triggers semantic search through a knowledge base. Relevant chunks are sent to a language model, which generates an answer based on retrieved content.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/beyond-hype-what-ai-can-do-product-design/1-rag-approach.png)
<sub>Instead of using the model's memory, it searches your documents and builds a response based on what it finds. (Large preview)</sub>

These aren't reasons to avoid RAG or AI altogether. Most of them can be avoided with better preparation of your knowledge base and more precise prompts. So, where do you start?

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Designing With AI, Not Around It: Practical Advanced Techniques For Product Design Use Cases](https://www.smashingmagazine.com/2025/08/designing-with-ai-practical-techniques-product-design/)  
> for internal educational use only (non-profit).

# Designing With AI, Not Around It: Practical Advanced Techniques For Product Design Use Cases

AI is almost everywhere — it writes text, makes music, generates code, draws pictures, runs research, chats with you — and apparently even understands people better than they understand themselves?!

It's a lot to take in. The pace is wild, and new tools pop up faster than anyone has time to try them. Amid the chaos, one thing is clear: this isn't hype, but it's structural change.

According to the Future of Jobs Report 2025 by the World Economic Forum, one of the fastest-growing, most in-demand skills for the next five years is the ability to work with AI and Big Data. That applies to almost every role — including product design.

![A figure showing skills on the rise in 2025-2030, which places AI and big data on the first place](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/designing-with-ai-practical-techniques-product-design/1-skills-on-the-rise-2025.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

What do companies want most from their teams? Right, efficiency. And AI can make people way more efficient. We'd easily spend 3x more time on tasks like replying to our managers without AI helping out. We're learning to work with it, but many of us are still figuring out how to meet the rising bar.

That's especially important for designers, whose work is all about empathy, creativity, critical thinking, and working across disciplines. It's a uniquely human mix. At least, that's what we tell ourselves.

Even as debates rage about AI's limitations, tools today (June 2025 — timestamp matters in this fast-moving space) already assist with research, ideation, and testing, sometimes better than expected.

Of course, not everyone agrees. AI hallucinates, loses context, and makes things up. So how can both views exist at the same time? Very simple. It's because both are true: AI is deeply flawed and surprisingly useful. The trick is knowing how to work with its strengths while managing its weaknesses. The real question isn't whether AI is good or bad — it's how we, as designers, stay sharp, stay valuable, and stay in the loop.

## Why Prompting Matters

Prompting matters more than most people realize because even small tweaks in how you ask can lead to radically different outputs. To see how this works in practice, let's look at a simple example.

Imagine you want to improve the onboarding experience in your product. On the left, you have the prompt you send to AI. On the right, the response you get back.

This side-by-side shows just how much even the smallest prompt details can change what AI gives you.

Talking to an AI model isn't that different from talking to a person. If you explain your thoughts clearly, you get a better understanding and communication overall.

> Advanced prompting is about moving beyond one-shot, throwaway prompts. It's an iterative, structured process of refining your inputs using different techniques so you can guide the AI toward more useful results. It focuses on being intentional with every word you put in, giving the AI not just the task but also the path to approach it step by step, so it can actually do the job.

![Advanced prompting vs basic promting](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/designing-with-ai-practical-techniques-product-design/2-advanced-prompting.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Where basic prompting throws your question at the model and hopes for a quick answer, advanced prompting helps you explore options, evaluate branches of reasoning, and converge on clear, actionable outputs.

But that doesn't mean simple prompts are useless. On the contrary, short, focused prompts work well when the task is narrow, factual, or time-sensitive. They're great for idea generation, quick clarifications, or anything where deep reasoning isn't required. Think of prompting as a scale, not a binary. The simpler the task, the faster a lightweight prompt can get the job done. The more complex the task, the more structure it needs.

In this article, we'll dive into how advanced prompting can empower different product & design use cases, speeding up your workflow and improving your results — whether you're researching, brainstorming, testing, or beyond. Let's dive in.

## Practical Cases

In the next section, we'll explore six practical prompting techniques that we've found most useful in real product design work. These aren't abstract theories — each one is grounded in hands-on experience, tested across research, ideation, and evaluation tasks. Think of them as modular tools: you can mix, match, and adapt them depending on your use case. For each, we'll explain the thinking behind it and walk through a sample prompt.

Important note: The prompts you'll see are not copy-paste recipes. Some are structured templates you can reuse with small tweaks; others are more specific, meant to spark your thinking. Use them as scaffolds, not scripts.

### 1. Task Decomposition By JTBD

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Design Patterns For AI Interfaces](https://www.smashingmagazine.com/2025/07/design-patterns-ai-interfaces/)  
> for internal educational use only (non-profit).

# Design Patterns For AI Interfaces

So you need to design a new AI feature for your product. How would you start? How do you design flows and interactions? And how do you ensure that that new feature doesn't get abandoned by users after a few runs?

In this article, I'd love to share a very simple but systematic approach to how I think about designing AI experiences. Hopefully, it will help you get a bit more clarity about how to get started.

This article is part of our ongoing series on UX. You can find more details on design patterns and UX strategy in Smart Interface Design Patterns 🍣 — with live UX training coming up soon. Jump to table of contents.

## The Receding Role of AI Chat

One of the key recent shifts is a slow move away from traditional "chat-alike" AI interfaces. As Luke Wroblewski wrote, when agents can use multiple tools, call other agents and run in the background, users orchestrate AI work more — there's a lot less chatting back and forth.

![AI Experience Paradigm by Luke Wroblewski](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/1-ai-experience-paradigm.jpg)
<sub>Messaging UI slowly starts feeling dated, and chat UI fades into background. By Luke Wroblewski. (Large preview)</sub>

In fact, chatbots are rarely a great experience paradigm — mostly because the burden of articulating intent efficiently lies on the user. But in practice, it's remarkably difficult to do well and very time-consuming.

Chat doesn't go away, of course, but it's being complemented with task-oriented UIs — temperature controls, knobs, sliders, buttons, semantic spreadsheets, infinite canvases — with AI providing predefined options, presets, and templates.

![AI Experience Paradigm by Luke Wroblewski](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/2-agentic-ai-design-patterns.jpg)
<sub>Agentic AI design patterns, with more task-oriented UIs, rather than chat. By Luke Wroblewski. (Large preview)</sub>

There, AI emphasizes the work, the plan, the tasks — the outcome, instead of the chat input. The results are experiences that truly amplify value for users by sprinkling a bit of AI in places where it delivers real value to real users.

To design better AI experiences, we need to study 5 key areas that we need to shape.

## Input UX: Expressing Intent

Conversational AI is a very slow way of helping users express and articulate their intent. Usability tests show that users often get lost in editing, reviewing, typing, and re-typing. It's painfully slow, often taking 30-60 seconds for input.

As it turns out, people have a hard time expressing their intent well. In fact, instead of writing prompts manually, it's a good idea to ask AI to write a prompt to feed itself.

![Illustration: How users can express their intent in AI interfaces.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/3-flora-ai.jpg)
<sub>Flora AI allows you to modify images and videos via nodes. (Large preview)</sub>

With Flora AI, users can still write prompts, but they visualize their intent with nodes by connecting various sources visually. Instead of elaborately explaining to AI how we need the pipeline to work, we attach nodes and commands on a canvas.

![Illustration of Output UX](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/4-illustration-output-ux.jpg)
<sub>With Krea.ai, users can move abstract shapes (on the left) to explain their goal to AI and study the outcome (on the right). (Large preview)</sub>

With input for AI, being precise is slow and challenging. Instead, we can abstract away the object we want to manipulate, and give AI precise input by moving that abstracted object on a canvas. That's what Krea.ai does.

In summary, we can minimize the burden of typing prompts manually — with AI-generated pre-prompts, prompt extensions, query builders, and also voice input.

## Output UX: Displaying Outcomes

AI output doesn't have to be merely plain text or a list of bullet points. It must be helpful to drive people to insights, faster. For example, we could visualize output by creating additional explanations based on the user's goal and motivations.

![Illustration of Output UX](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/5-illustration-output-ux.jpg)
<sub>Visualizing outcome through style lenses. By Amelia Wattenberger. (Large preview)</sub>

For example, Amelia Wattenberger visualized AI output for her text editor PenPal by adding style lenses to explore the content from. The output could be visualized in sentence lengths and scales Sad — Happy, Concrete — Abstract, and so on.

![Illustration of Output UX](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/design-patterns-ai-interfaces/6-aino-ai.jpg)
<sub>Aino.ai, an AI GIS Analyst for urban planning. (Large preview)</sub>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Using Manim For Making UI Animations](https://www.smashingmagazine.com/2025/04/using-manim-making-ui-animations/)  
> for internal educational use only (non-profit).

# Using Manim For Making UI Animations

Say you are learning to code for the first time, in Python, for example, which is a great starting point for getting into development. You are likely to come across some information like "a variable stores a value." That sounds straightforward, but if you are a beginner just starting, then it can also be a bit confusing. How does a variable store or hold something? What happens when we assign a new value to it?

To figure things out, you could read a bunch and watch tutorials, but sometimes, resources like these don't help the concept fully click. That's where animation helps. It has the power to take complex programming concepts and turn them into something visual, dynamic, and easy to grasp.

Let's break it down with an example: Say we have a box labeled X, first empty, then fill with a value 5, for this example, then update to 12, then 8, then 20, then 3.

Even if you are unfamiliar with Python, an animation like this makes the concept more obvious, helping you understand how variables work with visual cues. You can now visualize the variables as containers that hold and update values dynamically. It's way easier to see that than it is to just read about variables.

Well, Manim isn't just limited to programming; it works for math, physics, UI/UX, and more. In trigonometry, you can take something like a "Sine Wave" as an example, which is a smooth, continuous curve that moves up and down in a repeating pattern, and it is found everywhere from sound waves to electrical signals to the motion of a pendulum.

Sounds simple, right? Or maybe a bit confusing, especially if you're not a math person, but let me help with this:

Now, with this, you can see how the wave moves. Instead of just numbers and formulas, you're watching it happen. And that's pretty much the idea here! In this article, we'll explore Manim and how it makes concepts easier to understand through animation.

## Manim, Manim! What Is It?

By now, you may have a rough idea of what Manim can do, but let's break it down a little more. What exactly is Manim? Well, it's two things.

> First, Manim is an open-source Python library for creating high-quality mathematical animations.

If you've ever watched a 3Blue1Brown video, you've seen Manim in action because Grant Sanderson originally developed it for his YouTube channel.

> Second, Manim is a script-driven animation engine, meaning you write Python code to generate animations instead of dragging and dropping elements like in typical video editing software.

This gives you precise control over every detail, including text, color, shape, transformations, timing — you name it. Whether you're explaining math, physics, or programming concepts, Manim makes it fairly easy to create clear and dynamic visuals with just a few lines of code. Plus, it works seamlessly with LaTeX, so you can render mathematical equations beautifully without extra effort. That's why it's popular among educators, researchers, and content creators.

Of course, Manim isn't the only tool you can use. If it doesn't quite fit your needs or the programming language you are most comfortable with, here are some alternatives worth checking out:

- **Processing**: This is a Java-based coding framework, great for generative art and interactive visuals. If you enjoy experimenting with visual design through code, in Java, to be exact, then Processing gives you a solid foundation.
- **p5.js**: This is a JavaScript library, an alternative for web animations. If you're a front-end developer working with HTML and CSS, p5.js makes it easy for you to create graphics directly in the browser.
- **Desmos**: This focuses on math visualization. Desmos lets you create interactive graphs and scripted animations directly in the browser. You can use it through Desmos Graphs, Desmos Calculator, or the Desmos API.
- **Blender (with Python Scripting)**: This is mostly known for 3D animation, but with its Python API, you can script animations, including math and physics-based simulations.

## How To Get Started

There are multiple ways to install the library. You can set it up locally, use Conda or Docker, or run it inside Jupyter Notebooks. But if you don't want to deal with installations, Replit is a great alternative, as it's a real-time live editor that lets you start coding animations instantly.

### 1. Create An Account On Replit Using GitHub or Email.

Once you're in, your dashboard should look something like this:

![Replit dashboard](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-manim-making-ui-animations/1-create-account-replit.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Beyond Algorithms: Skills Of Designers That AI Can't Replicate](https://www.smashingmagazine.com/2023/04/skills-designers-ai-cant-replicate/)  
> for internal educational use only (non-profit).

# Beyond Algorithms: Skills Of Designers That AI Can't Replicate

At the start of the Coronavirus pandemic, I led the redesign of a tablet app used by sales representatives of the world's largest food & beverage company. Never having been a sales representative, nor having ever played one on TV, I was curious about their typical workday. Adapting the first rule of design — Know Thy User — our lockdown approach was to conduct video interviews. As soon as company restrictions allowed, I met two sales representatives at a local Walmart.

Masked and socially distant, I walked a mile in their shoes through the dairy, pet food, and freezer aisles. This single visit uncovered many insights that had not come up in the video interviews and online walkthroughs. I shared this with the team, spread across the world, and everyone could empathize with the sales representatives: juggling multiple devices and printouts, struggling to make technology work in extreme conditions like a low-lit walk-in freezer, and trying to work without hindering harried shoppers. The sales reps would repeat these tasks between twenty and thirty times a day, five days a week, which sounds about as fun as it is.

Our team used these insights to experiment with different concepts, refine them based on feedback from sales representatives, and launch a redesigned app that received glowing feedback from the representatives and praise from the company stakeholders.

Curiosity, empathy, and collaboration were some of the designer-like or designerly behaviors we used to transform the sales representatives' experience. These behaviors are a few of the behaviors and skills that designers use throughout the design process. Design researcher and educator Nigel Cross first used the word designerly to refer to underlying patterns of how designers think and act.

Designers spend years learning technical design skills, and as they use those hard skills to do their jobs, their designs are impactful when they actively use these non-technical designerly skills and behaviors. Designerly skills and behaviors make us creative and innovative and distinguish us from machines and technology like Artificial Intelligence (AI).

Yes, the same AI that you can't avoid reading or hearing about on social media or in the news. Stories and posts about people being equally worried about layoffs and AI taking over their jobs, and some even suggesting that AI is why those jobs won't come back. Creators and people traditionally considered creative, like artists, writers, and designers, seem especially concerned about AI making them redundant. Guesstimates of when AI will perform tasks better than humans just add to the frenzy.

![Timeline of AI development in text, code, images, and video categories](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/skills-designers-ai-cant-replicate/1-timeline-ai-development.jpg)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The assumption that AI will replace people is based on the premise that both have the same qualities, abilities, and skills. But that's just not true. Artificial intelligence is simply technology that is taught to mimic human intelligence to perform tasks. It is trained on large amounts of data — by some estimates, the equivalent of a quarter of the Library of Congress, the world's largest library.

AI is better than humans in certain tasks that involve processing and analyzing large amounts of data quickly, accurately, rationally, and consistently. Artificial Intelligence may create, but it can't be creative. It cannot match humans in areas that rely on skills and behaviors that are distinctly human, like intuition, emotional intelligence, cultural context, and changing situations.

Humans are conscious beings with a subconscious mind that can influence decisions and change those decisions based on experience, context, environment, wisdom, and understanding. This takes us years, decades, and even a lifetime to learn and apply, and it cannot be programmed in machines, no matter how sentient they may appear to be. Not for the foreseeable future.

Being designerly takes thinking, feeling, and acting like a designer. I've been thinking about and observing what it means to be designerly, and by using six such skills and behaviors, I will discuss how humans have an advantage over AI. I used the head, heart, and hands approach for transformative sustainability learning (Orr, Sipos, et al.) to organize these designerly skills related to thinking (head), feeling (heart), and doing (hands), and offer ways to practice them.

> Using our head, heart, and hands together to make a transformative difference is what distinguishes us from AI and makes us human, creative, and innovative.

![A picture of a wooden Lego man with designerly skills written next to him grouped and organized by the head, heart, and hands](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/skills-designers-ai-cant-replicate/3-designerly-skills.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

## Head

The skills, behaviors, and habits to help you think like a designer and create a designerly mindset include curiosity and observation.

### Cultivate Curiosity

Curiosity is the desire to know. It is a pleasure to ask, explore, experiment, discover, learn, and understand. We see this relentless curiosity in small children, who explore everything novel around them. As they grow up, that curiosity starts getting stifled in many, partly because they are taught to look for an answer instead of exploring questions.

This curiosity stifler of focusing on the answer is what AI is programmed to do. AI is also limited by its knowledge and understanding of the world, unable to explore beyond those boundaries. Also, without physical senses, AI cannot experience the world and be curious about things we see, hear, touch, taste, and smell around us.

This gives us a leg up on AI if we can overcome other curiosity-stiflers like self-consciousness, the shame of not knowing, and the fear of ridicule.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [A Week In The Life Of An AI-Augmented Designer](https://www.smashingmagazine.com/2025/08/week-in-life-ai-augmented-designer/)  
> for internal educational use only (non-profit).

# A Week In The Life Of An AI-Augmented Designer

Artificial Intelligence isn't new, but in November 2022, something changed. The launch of ChatGPT brought AI out of the background and into everyday life. Suddenly, interacting with a machine didn't feel technical — it felt conversational.

Just this March, ChatGPT overtook Instagram and TikTok as the most downloaded app in the world. That level of adoption shows that millions of everyday users, not just developers or early adopters, are comfortable using AI in casual, conversational ways. People are using AI not just to get answers, but to think, create, plan, and even to help with mental health and loneliness.

In the past two and a half years, people have moved through the Kübler-Ross Change Curve — only instead of grief, it's AI-induced uncertainty. UX designers, like Kate (who you'll meet shortly), have experienced something like this:

- **Denial**: "AI can't design like a human; it won't affect my workflow."
- **Anger**: "AI will ruin creativity. It's a threat to our craft."
- **Bargaining**: "Okay, maybe just for the boring tasks."
- **Depression**: "I can't keep up. What's the future of my skills?"
- **Acceptance**: "Alright, AI can free me up for more strategic, human work."

As designers move into experimentation, they're not asking, Can I use AI? but How might I use it well?.

> Using AI isn't about chasing the latest shiny object but about learning how to stay human in a world of machines, and use AI not as a shortcut, but as a creative collaborator.

It isn't about finding, bookmarking, downloading, or hoarding prompts, but experimenting and writing your own prompts.

To bring this to life, we'll follow Kate, a mid-level designer at a FinTech company, navigating her first AI-augmented design sprint. You'll see her ups and downs as she experiments with AI, tries to balance human-centered skills with AI tools, when she relies on intuition over automation, and how she reflects critically on the role of AI at each stage of the sprint.

The next two planned articles in this series will explore how to design prompts (Part 2) and guide you through building your own AI assistant (aka CustomGPT; Part 3). Along the way, we'll spotlight the designerly skills AI can't replicate like curiosity, empathy, critical thinking, and experimentation that will set you apart in a world where automation is easy, but people and human-centered design matter even more.

Note: This article was written by a human (with feelings, snacks, and deadlines). The prompts are real, the AI replies are straight from the source, and no language models were overworked — just politely bossed around. All em dashes are the handiwork of MS Word's autocorrect — not AI. Kate is fictional, but her week is stitched together from real tools, real prompts, real design activities, and real challenges designers everywhere are navigating right now. She will primarily be using ChatGPT, reflecting the popularity of this jack-of-all-trades AI as the place many start their AI journeys before branching out. If you stick around to the end, you'll find other AI tools that may be better suited for different design sprint activities. Due to the pace of AI advances, your outputs may vary (YOMV), possibly by the time you finish reading this sentence.

Cautionary Note: AI is helpful, but not always private or secure. Never share sensitive, confidential, or personal information with AI tools — even the helpful-sounding ones. When in doubt, treat it like a coworker who remembers everything and may not be particularly good at keeping secrets.

## Prologue: Meet Kate (As She Preps For The Upcoming Week)

Kate stared at the digital mountain of feedback on her screen: transcripts, app reviews, survey snippets, all waiting to be synthesized. Deadlines loomed. Her calendar was a nightmare. Meanwhile, LinkedIn was ablaze with AI hot takes and success stories. Everyone seemed to have found their "AI groove" — except her. She wasn't anti-AI. She just hadn't figured out how it actually fit into her work. She had tried some of the prompts she saw online, played with some AI plugins and extensions, but it felt like an add-on, not a core part of her design workflow.

Her team was focusing on improving financial confidence for Gen Z users of their FinTech app, and Kate planned to use one of her favorite frameworks: the Design Sprint, a five-day, high-focus process that condenses months of product thinking into a single week. Each day tackles a distinct phase: Understand, Sketch, Decide, Prototype, and Test. All designed to move fast, make ideas tangible, and learn from real users before making big bets.

![Stages of a 5-Day Design Sprint](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/week-in-life-ai-augmented-designer/1-stages-design-sprint.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

This time, she planned to experiment with a very lightweight version of the design sprint, almost "solo-ish" since her PM and engineer were available for check-ins and decisions, but not present every day. That gave her both space and a constraint, and made it the perfect opportunity to explore how AI could augment each phase of the sprint.

She decided to lean on her designerly behavior of experimentation and learning and integrate AI intentionally into her sprint prep, using it as both a creative partner and a thinking aid. Not with a rigid plan, but with a working hypothesis that AI would at the very least speed her up, if nothing else.

She wouldn't just be designing and testing a prototype, but prototyping and testing what it means to design with AI, while still staying in the driver's seat.

Follow Kate along her journey through her first AI-powered design sprint: from curiosity to friction and from skepticism to insight.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> ["Powered By AI" Is Not a Value Proposition](https://www.nngroup.com/articles/powered-by-ai-is-not-a-value-proposition/)  
> for internal educational use only (non-profit).

# "Powered By AI" Is Not a Value Proposition

## In This Article:

- A Clear Value Proposition Helps Both Users and Product Teams
- Good Value Propositions Are About the What, Not the How
- Why AI Is Not a Value Proposition on Its Own
- Treating AI as a Value Proposition Leads Teams to Lose Focus

## A Clear Value Proposition Helps Both Users and Product Teams

To help make decisions throughout the product lifecycle, designers create principles at the beginning of a project. The value proposition is the foremost of those principles — a promise to the user of the value they can expect to receive from using the product.

A clearly defined value proposition ensures that the product delivers the value that the team wants it to provide. Once the team knows the problem it's solving for the user, it can concentrate on supporting the user journey associated to that user goal. Aligning a team around one value proposition gives it the focus to prioritize features that are necessary to deliver that value and to say "no" to features that might be nice-to-have but aren't part of that user journey. As a result, the team can deliver a complete solution to the problem more quickly.

This is why the most successful products start by doing just one thing and doing it well. At launch, Dropbox only stored files in the cloud. Instagram let users upload photos and apply a few filters. Uber connected riders to luxury cars in San Francisco.

Products like these became successful because their laser focus on a single capability allowed them to fully satisfy their chosen market segment, before expanding their scope to additional value propositions. Not only did this focused design make it easier to build valuable products, but it also made their adoption more likely. No matter how valuable these tools could be in theory, it would only matter if users tried them — which meant that potential users first had to recognize that the product could be useful. Doing one clear thing well made the job of marketing these products much simpler.

## Good Value Propositions Are About the What, Not the How

A product design that flows from a solid value proposition creates confidence at every touchpoint. Potential users can immediately grasp the product's value and how it might fit into their workflow.

Strong value propositions don't leave this outcome up to chance but emphasize it from the very beginning of the design process. They include both a goal that is obviously valuable for the user and the experience that will let the user achieve it. In contrast, relative descriptors such as "easier" or "more powerful" are often a sign that the value proposition is not defined, because they leave it to the user to figure out how those things are going to make their lives better.

**Well-defined value propositions**
- Accelerate your research to make decisions more quickly
- Save time when you skip waiting in line
- Never miss a bill payment again with automatic payments

**Poorly defined value propositions**
- Redesigned UI that's easier to use
- Ten exciting new features
- More powerful than ever before

We should think about how to deliver product value only after firmly establishing what that value is going to be. This is because the experience we want to create determines which technologies and form factors are most suitable.

Beginning with the how — the technologies or form factors — before determining the what leads to weak value propositions. Without a clear user goal to guide design choices, the decision of which features to include will be made based on incentives such as what is easiest to build or most exciting. And when the features this technology enables are presented to the user, they will not add up to a product whose value is easy to understand.

![Tagline saying "Find the best prices on fashion instantly."](https://media.nngroup.com/media/editor/2025/06/05/ai-prices.jpeg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Why AI Is Not a Value Proposition on Its Own

Many product teams today are under pressure from stakeholders and investors to integrate generative AI into their products. Unfortunately, reorienting from a user-centered mode of design to one that focuses on technology is causing these teams to lose track of their value proposition and to lose the ability to tell an effective story. When the most prominent part of the pitch is that the product has AI in it, users struggle to understand how that helps them.

![Wondering Tagline: A new way to understand your customers, powered by AI.](https://media.nngroup.com/media/editor/2025/06/05/ai-wonder.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Unlike value propositions focused on product functionality, "powered by AI" could mean just about anything. This framing makes users wholly responsible for identifying opportunities to integrate the tool into their workflows. Shifting this burden from the designers of the tool onto the users means that fewer users will be likely to give it a chance, use it successfully, or return to it.

On top of that, unlike with most technologies, the mere presence of the team "AI" in product descriptions can trigger fear and reduce the likelihood of conversion, as shown by Mesur Cicek, Dogan Gursoy, and Lu Lu. By now the drawbacks of LLMs are well-known to both users and designers, and successful AI-powered products need to be able to address users' specific anxieties about AI in their domain.

For example, Grammarly must wrestle with common perceptions of AI writing: it won't sound like me, it will come across as stuffy and corporate, it will be obvious that I used AI and people will judge me. Grammarly's landing page (above) is able to address that anxiety — it's your writing and reputation, the AI is just a partner that helps you find the words.

![The landing page for Grammarly. Responsible AI that ensures your writing and reputation shine Work with an AI writing partner that helps you find the words you need⁠—⁠to write that tricky email, to get your point across, to keep your work moving.](https://media.nngroup.com/media/editor/2025/06/05/ai-grammarly.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Grammarly can alleviate users' concerns about AI with just a few words because the product's AI features are tailored to one specific task. The broader the product's AI feature set, the more anxieties its value proposition will have to soothe for it to be successful.

## Treating AI as a Value Proposition Leads Teams to Lose Focus

AI is an "everything" technology. Foundation models are trained on the contents of the entire Internet and promise to do anything that anyone on the Internet can: write an email or a poem, update a resume, act as a therapist, give investment advice, or write code.

As a result, there is a lot of overlap between what AI can do and any given problem space. This is where the presence or absence of a clear value proposition shapes two different courses of action for product teams.

Product teams with a clear value proposition as their guiding principle can anticipate the capabilities that users will need to reach their goal. Some of those capabilities may require AI, and others may not, but they are all structured around one continuous user journey. Features extraneous to that journey can be safely left out, redirecting development efforts towards making the experience work holistically. The resulting product will be easy to describe with one clear and simple message, because it focuses on doing one thing well.

But when teams who treat "it has AI in it" as its own value proposition try to optimize that value, the only guiding principle they will derive is to make the experience as AI as possible. In practice, this often means adding a broad-scope chatbot to the product. This interface format may present the user with the greatest range of AI capabilities, but it also has the highest barrier to adoption. Other common AI paradigms such as summarizers can be inappropriate for delivering a good experience, but are implemented as low-hanging fruit.

The dynamics of B2B software design present another scenario. While users often struggle to identify uses for AI in their workflows, business leaders are increasingly willing to pay for AI features merely for the bragging rights of providing their people with cutting-edge tools. Product teams may be tempted to meet this demand, but allowing it to dilute their value propositions is unlikely to result in long-term profits. A team that starts with user-centered design can choose the type of AI integration that provides the greatest value to the user. And a product with clear value proposition beyond "powered by AI" is going to not just land business contracts but also keep them after the excitement has faded.

![A screenshot of Meta AI summarizing comments on a post and saying that commenters are calling it clickbait and inaccurate.](https://media.nngroup.com/media/editor/2025/06/05/meta-ai.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [The Future-Proof Designer](https://www.nngroup.com/articles/future-proof-designer/)  
> for internal educational use only (non-profit).

# The Future-Proof Designer

## In This Article:

- How AI Is Reshaping Product Development
- The Expert Panel of Product and Design Strategists
- How Designers Can Stay Indispensable in the Age of AI
- Conclusion

## How AI Is Reshaping Product Development

Integrating new technologies into design workflows is nothing new — from the dot-com boom to responsive design and blockchain, designers have always adapted. AI is simply the latest shift, but its speed and scale can feel overwhelming. AI is reshaping product development by automating tactical design tasks, accelerating feature production, and surfacing patterns in data at unprecedented speeds. While these advancements offer efficiency gains, they also introduce new risks:

- Design may become marginalized as UI execution is automated.
- With the ability to build new functionality more quickly, teams may flood products with low-value features.
- The large number of data patterns identified with the help of AI might obscure the insights that matter most.

Still, seasoned experts advise against panic. The core principles of UX and product design remain unchanged, and AI amplifies their importance in many ways. To stay indispensable, designers must evolve: adapt to new workflows, deepen their judgment, and double down on the uniquely human skills that AI can't replace.

## The Expert Panel of Product and Design Strategists

Many designers wonder how they can future-proof their careers in the age of AI. (We use "designers" to refer broadly to both design and user-experience research professionals.)

To explore this question, we spoke with seven leading experts in product and design; these experts bring over 150 years of combined experience across product management, UX design, user research, behavioral psychology, growth strategy, and digital innovation. Meet the panel:

- **Anuj Adhiya**: Author of Growth Hacking for Dummies; fractional growth exec, advisor for founders, and coach for heads of growth.
- **Nir Eyal**: Author of Hooked: How to Build Habit-Forming Products, exploring the intersection of behavioral psychology, design, and business
- **Ramli John**: Founder at Delight Path and author of Product-Led Onboarding
- **Laura Klein**: Principal experience specialist at Nielsen Norman Group and author of Build Better Products; expert advisor in product strategy and design
- **Melissa Perri**: Author of Escaping the Build Trap, CEO and founder of Product Institute, and strategic advisor to leaders at Fortune 500 companies and scale-up companies. Melissa spent many years as a UX designer and product manager. She previously taught Product Management at Harvard Business School.
- **Josh Seiden**: Coauthor of Lean UX, the author of Outcomes Over Output, and the co-founder of Sense & Respond Learning.
- **Teresa Torres**: Product-discovery coach, author of Continuous Discovery Habits, and thought leader in user research and iterative development.

## How Designers Can Stay Indispensable in the Age of AI

Our expert panel recommends several tactics for designers to stay indispensable as AI reshapes product development. The key is to become more strategic and make the most of what AI offers by leveraging AI-driven insights while applying human judgment and critical thinking to make informed product decisions. Our panel of experts shared their top 4 pieces of advice, summarized below.

### 1. Embrace the Strategic Scope of Design

There is a growing misconception that AI tools can take over design, engineering, and strategy. However, designers offer more than interaction and visual-design skills. They offer judgment, built on expertise that AI cannot replicate.

Our panelists return to a consistent message: across every tech hype cycle, from responsive design to AI, the value of design hasn't changed. Good design goes deeper than visuals; it requires critical thinking, empathy, and a deep understanding of user needs. It involves:

- **Systems thinking**: Understanding how different parts of a product – features, multiple user flows, backend systems, and business processes – work together to create a cohesive experience
- **Use-case evaluation**: Anticipating all scenarios, including less-than-ideal ones, in which someone might use the product
- **Service design**: Designing, aligning, and optimizing how business operations support complete user journeys efficiently and sustainably

If your role as a designer is limited to polishing UIs or producing high-fidelity prototypes, you've likely been boxed into a narrow definition of design that is not UX in the first place. To future-proof yourself in the age of AI tools, Josh Seiden urges designers to avoid getting stuck in this box and seek to break out of it actively. He said, "Make yourself valuable by not limiting yourself with a rigid definition of your role."

![Headshot of Josh Seiden. Text reads: "Josh Seiden Coauthor of Lean UX, the author of Outcomes Over Output, and the co-founder of Sense & Respond Learning."](https://media.nngroup.com/media/editor/2025/06/03/joshseiden.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Remember that design activities exist on a spectrum. Some activities are tactical (e.g., creating mockups, writing specifications), while others are strategic (e.g., defining the product vision, creating user flows, and aligning stakeholders). Strategic design activities are often intangible and, thus, easy to overlook and deprioritize. If designers can shift focus to strategic activities, automation will not replace them.

### 2. Strengthen Storytelling Skills

Many stakeholders still equate the role of design with outputs like mockups, prototypes, and polished visuals, instead of strategy. Multiple experts agree that designers can shift this perception through compelling storytelling, which AI can't replicate. While AI can generate ideas and data, most of AI's outputs are adequate at best, and some are poor. AI can't explain tradeoffs, justify decisions, or connect solutions to business outcomes. That's where designers add lasting value.

Melissa Perri puts it simply when she says, "Be able to tell the story of why your design matters to your customers and the business." In AI-rich work environments, storytelling helps cut through the noise. It aligns stakeholders, highlights impact, and positions designers as strategic leaders. As Nir Eyal said, "Suddenly, you're not just designing—you're leading."

![Headshot of Melissa Perri. Text reads: "Melissa Perri Author of Escaping the Build Trap, CEO and founder of Product Institute, and strategic advisor to leaders at Fortune 500 companies and scale-up companies. Melissa spent many years as a UX designer and product manager. She previously taught Product Management at Harvard Business School."](https://media.nngroup.com/media/editor/2025/06/03/melissaperri.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Effective storytelling also depends on emotional intelligence and audience awareness, which AI tools lack. As Laura Klein advises, "Treat your coworkers the way you treat customers, and understand what drives them." Nir Eyal emphasizes that great design is less about perfect pixels and more about psychology: understanding cognitive biases, attention, and motivation, both for users and your stakeholders.

Melissa Perri highlights that storytelling starts with framing. For example, instead of saying This flow has poor usability, reframe it in business terms that resonate with your stakeholders: We're seeing high churn here. But when other customer segments adopt the product, churn drops significantly. I can reduce churn by rethinking this design.

Speaking the language of your audience helps, too. For example, if business analytics speaks to your product manager more than design terminology, overlay key metrics directly on wireframes. This approach shows how layout, content, and research connect to measurable outcomes in a format that aligns with your product manager's thinking.

Ultimately, as Teresa Torres reminds us, effective communication means staying anchored in shared goals:

> "Remember, we all share the same goal — serving the customer in a way that also serves the business."

![Headshot of Teresa Torres. Text reads: "Teresa Torres Product-discovery coach, author of Continuous Discovery Habits, and thought leader in user research and iterative development."](https://media.nngroup.com/media/editor/2025/06/03/teresatorres.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [AI Design Tools Are Marginally Better: Status Update](https://www.nngroup.com/articles/ai-design-tools-update-2/)  
> for internal educational use only (non-profit).

# AI Design Tools Are Marginally Better: Status Update

This is a follow-up to the 2024 article AI UX-Design Tools Are Not Ready for Primetime: Status Update.

In April 2024, AI-powered design tools were not useful to designers. As of May 2025, their usefulness has improved, but we're still nowhere near the AI-powered design tools we've been promised, nor are design professionals yet in danger of being replaced by AI. (This is true even if Figma's new features announced at Config this week turn out to be as good as their demos — which is never guaranteed with AI tools.)

## In This Article:

- Narrow-Scope Features Are the Most Useful
- Wireframe and Prototype Generation Still Need Work
- How We Evaluated These Tools
- Looking Forward

## Narrow-Scope Features Are the Most Useful

The greatest improvements in design-specific AI tools is within narrow-scoped genAI. Unlike broad AIs (like ChatGPT), which accept a wide range of inputs and produce an equally wide array of outputs, narrow-scope AIs specialize in one or few specific tasks.

Narrow AI, in general, tends to be more easily adopted and appreciated by users. It's more likely to meet a specific user need and be understood by those using it. This holds true for AI tools for designers.

Based on interactions with practicing designers, we've found that the most helpful tools (actually adopted by designers for everyday work) are decidedly narrow — that is, they are usually focused on completing one specific task. Unlike broader products that generate entire designs or prototypes, these narrow tools take advantage of what current genAI is good at — automating repetitive tasks with targeted suggestions based on strong pattern recognition.

In our evaluation, the narrow-scope features in three tools stood out: Figma, Khroma Color, and Midjourney.

### Tool: Figma

Since our original article, Figma has released several narrow-scope AI tools that are meaningfully helpful for designers.

#### Rename Layers

This Figma AI tool completely eliminates the tedious task of renaming layers, saving designers time and effort. According to Figma, this tool uses a layer's contents, location, and relationship to other selected layers to recognize patterns, rename layers, and increase the organization of a design.

As many designers know, naming layers is tedious, does not serve an immediate purpose (you're not creating something; you're just adding metadata), and even the most detail-oriented designers forget to do it. Even the simple layer naming provided by the Rename layers tool makes it easier to quickly search for layers within your Figma.

#### Rewrite This

Designers are not necessarily writers, but they often need to write copy for their design. Figma's Rewrite this feature leverages genAI's talent for text generation. By giving the AI a short prompt, designers can adjust copy or have it generated for them entirely from scratch, thus freeing time to focus on their primary task.

Figma also has similar features that shorten and translate text, creating many opportunities to augment designers' skills and speed up content production.

#### Find More Like

The final Figma AI feature we're highlighting removes the stress of digging through files across multiple projects and teams to find the right asset. By using Find more like, designers can find their missing or related assets almost instantly. Keywords, descriptive text, a layer selection, or an image can be used as prompts for this functionality. Once you find the design you are looking for, you can open the source file or insert it into your current file. When working for a large company with teams split across features, products, and even time zones, it can be hard to know where to look when you're trying to find an idea or a model for a design. And even if you are part of a small team or a freelancer, this tool is useful for helping you quickly locate a similar design.

### Tool: Khroma Color

Khroma Color uses AI to assess patterns in designers' color choices and assemble custom color palettes. Like many of Figma's AI tools, Khroma Color employs genAI's pattern recognition and automation to reduce the time spent finding colors that are both aesthetically pleasing and brand-compliant.

### Tool: Midjourney

Midjourney and other diffusion AI models specialize in generating images; these broader tools offer many of the same benefits as design-specialized AI tools.

![Midjourney's UI with a generated picture of a puppy up close.](https://media.nngroup.com/media/editor/2025/05/07/midjourney-puppy.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Like Figma's Rewrite this, Midjourney provides an excellent resource for creating placeholder content (in this case, images). Once again, this feature allows designers to free up time for the task at hand. It can be a quick workaround for prototype usability testing, when you don't want to spend time or don't have the resources to get professional photos (stock or not).

Some teams (especially small companies and startups) use AI-generated images for final designs as well. While we do not recommend you do so, since user trust can plummet if they notice AI use, it does work in a pinch when resources are scarce.

## Wireframe and Prototype Generation Still Need Work

More complicated genAIs like wireframe and prototype design tools, which require a broader skillset and understanding of real-world context, still do not meet expectations. Unlike a human designer who can understand and adjust to a variety of contexts and needs, most genAI tools lack the sophistication to balance all the requirements of a design.

At present, wireframe and prototype tools work best for ideation, possibly as starting points for newer designers or freelancers. While they might provide some good ideas, eliminate some additional overhead, and fill in knowledge gaps, they cannot replace the level of detail brought by an experienced human designer.

### Design Systems Are Needed

Currently, no genAI tool effectively supports design systems; this limitation lowers their utility within design teams. Most designers aren't building things from scratch. A prototype composed of random design elements is not helpful. AI needs to be able to pull from established design systems and create a cohesive look across designs.

Figma and other design tools are working towards AI features that create designs integrated with users' design systems. After Figma's ConFig 2025 conference, where no major design-system-related updates were announced, it is unknown when these features will be released.

### Prompt-Length Limitations

Another barrier to design-specific genAI success is the strict limit on the number of words in a prompt. With these constraints, it's impossible for the AI to be aware of all the context that goes into a UX design. Currently, only a human designer can balance the design, business, and user needs that go into a great visual design.

500-character prompts are not enough. AI needs to be able to process complex context information (including business goals, user needs, information about the existing product, etc.) like a human designer. Simply increasing token limits won't solve the problem, though — it's hard for designers to create prompts that give sufficient context. To be really useful, these tools need to be able to learn their users' context over time (like ChatGPT's Memory feature).

We also acknowledge this isn't a simple fix for most teams. A longer context window might not be available for these tools' AI models. Even if the models could support more context, the additional resources needed to run these long prompts might make it infeasible for many. But while this problem has no easy or quick fix, it needs to be solved for these AI tools to become truly helpful.

## Next Steps

- **Study**: Learn more about design patterns and their applications
- **Practice**: Implement patterns in your own projects
- **Refactor**: Apply patterns to existing code to improve structure
- **Share**: Document and share pattern implementations with your team

## Sources

- [Design Patterns: Elements of Reusable Object-Oriented Software](https://en.wikipedia.org/wiki/Design_Patterns)
- [Python Design Patterns](https://python-patterns.guide/)
- [Refactoring Guru - Design Patterns](https://refactoring.guru/design-patterns)

## Collaboration Prompts for Engineers

### For Frontend Developers
"Implement the Observer pattern for real-time UI updates"

### For Backend Developers
"Design a Strategy pattern for different authentication methods"

### For DevOps Engineers
"Create a Factory pattern for different deployment strategies"

### For Data Scientists
"Build a Chain of Responsibility pattern for data validation pipelines"
