---
title: "Tree of Thoughts"
slug: "modules-prompting-techniques-tree-of-thoughts"
updatedAt: "2025-08-18"
tags: "prompting-technique,tree-of-thoughts,reasoning"
---

# Tree of Thoughts

> Learn how to explore multiple reasoning paths simultaneously to find the best solution.

## What is Tree of Thoughts?

Tree of Thoughts (ToT) is an advanced prompting technique that explores multiple reasoning paths simultaneously, similar to how humans consider different approaches to solve complex problems. It generates a tree of possible solutions and evaluates each path to find the optimal answer.

## Key Concepts

- **Multiple Paths**: Explore different reasoning approaches
- **Evaluation**: Assess the quality of each path
- **Backtracking**: Return to previous nodes if needed
- **Optimal Solution**: Find the best path through the tree

## Basic Implementation

```python
class TreeOfThoughts:
    def __init__(self, max_depth=3, branching_factor=3):
        self.max_depth = max_depth
        self.branching_factor = branching_factor
    
    def generate_thoughts(self, prompt, current_thoughts):
        # Generate multiple possible next thoughts
        thoughts = []
        for _ in range(self.branching_factor):
            thought = self.generate_next_thought(prompt, current_thoughts)
            thoughts.append(thought)
        return thoughts
    
    def evaluate_thoughts(self, thoughts):
        # Evaluate the quality of each thought
        scores = []
        for thought in thoughts:
            score = self.evaluate_thought(thought)
            scores.append(score)
        return scores
```

## Example Application

**Problem**: "Plan a weekend trip to Paris"

**Tree Structure**:

```
Root: Plan weekend trip
├── Transportation: Flight
│   ├── Budget airline
│   ├── Regular airline
│   └── Private jet
├── Transportation: Train
│   ├── High-speed rail
│   └── Regular train
└── Transportation: Car
    ├── Rental car
    └── Own car
```

## Best Practices

1. **Define Evaluation Criteria**: Establish clear metrics for evaluating paths
2. **Limit Tree Size**: Control depth and branching to manage complexity
3. **Prune Poor Paths**: Eliminate low-quality branches early
4. **Iterative Refinement**: Continuously improve the search process

# **Tree of Thoughts (ToT)**

For complex tasks that require exploration or strategic lookahead, traditional or simple prompting techniques fall short. [Yao et el. (2023)](https://arxiv.org/abs/2305.10601) and [Long (2023)](https://arxiv.org/abs/2305.08291) recently proposed Tree of Thoughts (ToT), a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem solving with language models.

ToT maintains a tree of thoughts, where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem. This approach enables an LM to self-evaluate the progress through intermediate thoughts made towards solving a problem through a deliberate reasoning process. The LM's ability to generate and evaluate thoughts is then combined with search algorithms (e.g., breadth-first search and depth-first search) to enable systematic exploration of thoughts with lookahead and backtracking.

The ToT framework is illustrated below:

![TOT Web](/images/TOT.webp)

Image Source: [Yao et el. (2023)](https://arxiv.org/abs/2305.10601)

When using ToT, different tasks requires defining the number of candidates and the number of thoughts/steps. For instance, as demonstrated in the paper, Game of 24 is used as a mathematical reasoning task which requires decomposing the thoughts into 3 steps, each involving an intermediate equation. At each step, the best b=5 candidates are kept.

To perform BFS in ToT for the Game of 24 task, the LM is prompted to evaluate each thought candidate as "sure/maybe/impossible" with regard to reaching 24. As stated by the authors, "the aim is to promote correct partial solutions that can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on "too big/small" commonsense, and keep the rest "maybe"". Values are sampled 3 times for each thought. The process is illustrated below:

![TOT2 Web](/images/TOT2.webp)

Image Source: [Yao et el. (2023)](https://arxiv.org/abs/2305.10601)

From the results reported in the figure below, ToT substantially outperforms the other prompting methods:

![TOT3 Web](/images/TOT3.webp)

Image Source: [Yao et el. (2023)](https://arxiv.org/abs/2305.10601)

Code available [here](https://github.com/princeton-nlp/tree-of-thought-llm) and [here](https://github.com/jieyilong/tree-of-thought-puzzle-solver)

At a high level, the main ideas of [Yao et el. (2023)](https://arxiv.org/abs/2305.10601) and [Long (2023)](https://arxiv.org/abs/2305.08291) are similar. Both enhance LLM's capability for complex problem solving through tree search via a multi-round conversation. One of the main difference is that [Yao et el. (2023)](https://arxiv.org/abs/2305.10601) leverages DFS/BFS/beam search, while the tree search strategy (i.e. when to backtrack and backtracking by how many levels, etc.) proposed in [Long (2023)](https://arxiv.org/abs/2305.08291) is driven by a "ToT Controller" trained through reinforcement learning. DFS/BFS/Beam search are generic solution search strategies with no adaptation to specific problems. In comparison, a ToT Controller trained through RL might be able learn from new data set or through self-play (AlphaGo vs brute force search), and hence the RL-based ToT system can continue to evolve and learn new knowledge even with a fixed LLM.

[Hulbert (2023)](https://github.com/dave1010/tree-of-thought-prompting) has proposed Tree-of-Thought Prompting, which applies the main concept from ToT frameworks as a simple prompting technique, getting the LLM to evaluate intermediate thoughts in a single prompt. A sample ToT prompt is:

```
Imagine three different experts are answering this question.All experts will write down 1 step of their thinking,then share it with the group.Then all experts will go on to the next step, etc.If any expert realises they're wrong at any point then they leave.The question is...
```

[Sun (2023)](https://github.com/holarissun/PanelGPT) benchmarked the Tree-of-Thought Prompting with large-scale experiments, and introduce PanelGPT --- an idea of prompting with Panel discussions among LLMs.

## Related Techniques

- [Chain-of-Thought Prompting](./chain-of-thought)
- [Self-Consistency](./self-consistency)
- [ReAct Framework](./react)