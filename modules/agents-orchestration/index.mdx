---
title: "Agents & Orchestration"
slug: "modules-agents-orchestration"
updatedAt: "2025-08-16"
tags: [module, agents, orchestration, crewai]
---

# Agents & Orchestration

> Start reading here to understand how AI agents work together to solve complex problems.

## What are AI Agents?

AI agents are autonomous units that can perform specific tasks, make decisions, and collaborate with other agents to achieve complex objectives. Think of them as specialized team members, each with their own expertise and responsibilities.

## Agent Design Patterns

<Tabs>
  <Tab title="Good Design" icon="users">
    ### ✅ Good Agent Design
    
    **Clear Role**: Each agent has a specific function and expertise
    - **Defined Goals**: Individual objectives guide decision-making
    - **Appropriate Tools**: Access to relevant capabilities and APIs
    - **Collaboration**: Can work with other agents when needed
    
    **Example Implementation:**
    ```python
    # Well-designed research agent
    researcher = Agent(
        role="Research Analyst",
        goal="Find the most relevant and accurate information",
        backstory="Expert at analyzing market trends and data patterns",
        tools=[web_search_tool, database_tool],
        verbose=True,
        allow_delegation=True  # Can collaborate with other agents
    )
    
    # Well-designed writer agent
    writer = Agent(
        role="Content Writer", 
        goal="Create compelling and accurate content",
        backstory="Skilled at transforming complex information into clear narratives",
        tools=[document_tool, grammar_checker],
        verbose=True
    )
    
    # Clear collaboration setup
    crew = Crew(
        agents=[researcher, writer],
        tasks=[research_task, writing_task],
        process=Process.sequential  # Clear workflow
    )
    ```
  </Tab>
  
  <Tab title="Poor Design" icon="user-x">
    ### ❌ Poor Agent Design
    
    **Vague Responsibilities**: Unclear what the agent should do
    - **Conflicting Goals**: Multiple agents competing for the same objective
    - **Tool Mismatch**: Agents given tools they can't effectively use
    - **Isolation**: Agents that can't communicate or collaborate
    
    **Example Implementation:**
    ```python
    # Poorly designed agent with vague role
    agent1 = Agent(
        role="Helper",  # Too vague
        goal="Help with stuff",  # Unclear objective
        backstory="I help people",  # Generic backstory
        tools=[web_search_tool, database_tool, writing_tool],  # Too many tools
        verbose=True
    )
    
    # Conflicting agent with same goals
    agent2 = Agent(
        role="Assistant",
        goal="Help with stuff",  # Same goal as agent1
        backstory="I also help people",
        tools=[web_search_tool],  # Tool mismatch
        verbose=True
    )
    
    # No collaboration setup
    crew = Crew(
        agents=[agent1, agent2],
        tasks=[task1, task2],
        # No process defined - agents work in isolation
    )
    ```
  </Tab>
</Tabs>

## Agent Components

Every AI agent consists of several key components that define its behavior and capabilities:

### Core Attributes

| Attribute | Type | Description | Example |
|-----------|------|-------------|---------|
| **Role** | `str` | Defines the agent's function and expertise | "Research Analyst" |
| **Goal** | `str` | Individual objective that guides decisions | "Find the most relevant information" |
| **Backstory** | `str` | Context and personality for interactions | "Expert at analyzing market trends" |
| **LLM** | `Union[str, LLM]` | Language model that powers the agent | "gpt-4" |
| **Tools** | `List[BaseTool]` | Capabilities available to the agent | Web search, database access |

### Advanced Configuration

| Attribute | Type | Description | Default |
|-----------|------|-------------|---------|
| **Max Iterations** | `int` | Maximum steps before providing best answer | 20 |
| **Max RPM** | `Optional[int]` | Rate limit for API calls | None |
| **Max Execution Time** | `Optional[int]` | Time limit in seconds | None |
| **Allow Delegation** | `bool` | Can delegate tasks to other agents | False |
| **Verbose** | `bool` | Enable detailed execution logs | False |

## Agent Types and Specializations

### Research Agents
Specialized in gathering and analyzing information:

```python
researcher = Agent(
    role="Research Analyst",
    goal="Find the most relevant and accurate information",
    backstory="Expert at analyzing market trends and data patterns",
    tools=[web_search_tool, database_tool],
    verbose=True
)
```

### Writing Agents
Focused on content creation and communication:

```python
writer = Agent(
    role="Content Writer",
    goal="Create compelling and accurate content",
    backstory="Skilled at transforming complex information into clear narratives",
    tools=[document_tool, grammar_checker],
    verbose=True
)
```

### Analysis Agents
Specialized in data analysis and insights:

```python
analyst = Agent(
    role="Data Analyst",
    goal="Extract meaningful insights from data",
    backstory="Expert at statistical analysis and pattern recognition",
    tools=[data_analysis_tool, visualization_tool],
    verbose=True
)
```

## Orchestration Patterns

### Sequential Processing
Agents work in a specific order, passing results to the next agent:

```python
from crewai import Crew, Process

crew = Crew(
    agents=[researcher, writer, analyst],
    tasks=[research_task, writing_task, analysis_task],
    process=Process.sequential
)
```

### Hierarchical Processing
Agents are organized in a hierarchy with managers and workers:

```python
manager = Agent(
    role="Project Manager",
    goal="Coordinate and oversee the project",
    backstory="Experienced at managing complex projects",
    tools=[delegation_tool, monitoring_tool],
    allow_delegation=True
)

crew = Crew(
    agents=[manager, researcher, writer, analyst],
    tasks=[management_task, research_task, writing_task, analysis_task],
    process=Process.hierarchical
)
```

### Collaborative Processing
Agents work together simultaneously, sharing information:

```python
crew = Crew(
    agents=[researcher, writer, analyst],
    tasks=[research_task, writing_task, analysis_task],
    process=Process.collaborative
)
```

## Advanced Workflow Patterns

Building on the [MCP Agent framework](https://github.com/lastmile-ai/mcp-agent), here are advanced workflow patterns that enable sophisticated agent orchestration:

### Workflow: Routing

Routing workflows direct tasks to specialized agents based on content classification or task type.

<Card title="Routing Implementation">
  <h4>Key Components:</h4>
  <ul>
    <li><strong>Classifier Agent:</strong> Determines task type and routes to appropriate specialist</li>
    <li><strong>Specialist Agents:</strong> Handle specific task categories</li>
    <li><strong>Router:</strong> Manages task distribution and result aggregation</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  ```python
  from mcp_agent import Agent, Orchestrator
  
  # Classifier agent determines task type
  classifier = Agent(
      name="Task Classifier",
      instruction="Classify incoming tasks into categories: research, writing, analysis, or support",
      functions=[classify_task],
      server_names=["filesystem"]
  )
  
  # Specialist agents for each category
  researcher = Agent(
      name="Research Specialist", 
      instruction="Conduct thorough research on given topics",
      functions=[web_search, database_query],
      server_names=["fetch", "filesystem"]
  )
  
  writer = Agent(
      name="Content Writer",
      instruction="Create compelling content based on research",
      functions=[write_content, edit_text],
      server_names=["filesystem"]
  )
  
  # Router orchestrates the workflow
  router = Orchestrator(
      llm_factory=AnthropicAugmentedLLM,
      available_agents=[classifier, researcher, writer],
      planner=planner_llm
  )
  ```
  
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Customer Service:</strong> Route different query types to specialized agents</li>
    <li><strong>Content Processing:</strong> Direct content to appropriate analysis or creation agents</li>
    <li><strong>Model Selection:</strong> Route simple queries to faster models, complex ones to more capable models</li>
  </ul>
</Card>

### Workflow: Parallelization

Parallelization workflows run multiple agents simultaneously for speed or diverse perspectives.

<Card title="Parallelization Implementation">
  <h4>Two Key Variations:</h4>
  
  <h5>Sectioning:</h5>
  <p>Breaking a task into independent subtasks run in parallel</p>
  
  <h5>Voting:</h5>
  <p>Running the same task multiple times to get diverse outputs</p>
  
  <h4>Implementation Example:</h4>
  ```python
  # Sectioning: Parallel subtask processing
  content_analyzer = Agent(
      name="Content Analyzer",
      instruction="Analyze content for sentiment, key themes, and action items",
      functions=[analyze_sentiment, extract_themes, identify_actions],
      server_names=["filesystem"]
  )
  
  safety_checker = Agent(
      name="Safety Checker", 
      instruction="Check content for inappropriate or harmful material",
      functions=[safety_scan, content_filter],
      server_names=["filesystem"]
  )
  
  # Both agents process the same content in parallel
  parallel_orchestrator = Orchestrator(
      llm_factory=AnthropicAugmentedLLM,
      available_agents=[content_analyzer, safety_checker],
      planner=planner_llm
  )
  
  # Voting: Multiple perspectives on same task
  code_reviewer_1 = Agent(
      name="Security Reviewer",
      instruction="Review code for security vulnerabilities",
      functions=[security_scan, vulnerability_check],
      server_names=["filesystem"]
  )
  
  code_reviewer_2 = Agent(
      name="Performance Reviewer", 
      instruction="Review code for performance issues",
      functions=[performance_analysis, optimization_check],
      server_names=["filesystem"]
  )
  
  code_reviewer_3 = Agent(
      name="Best Practices Reviewer",
      instruction="Review code for best practices and maintainability", 
      functions=[best_practices_check, maintainability_analysis],
      server_names=["filesystem"]
  )
  ```
  
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Content Moderation:</strong> Multiple agents check different aspects simultaneously</li>
    <li><strong>Code Review:</strong> Different reviewers focus on security, performance, and best practices</li>
    <li><strong>Quality Assurance:</strong> Parallel validation of different quality dimensions</li>
  </ul>
</Card>

### Workflow: Orchestrator-Workers

In orchestrator-workers workflows, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.

<Card title="Orchestrator-Workers Implementation">
  <h4>Key Characteristics:</h4>
  <ul>
    <li><strong>Dynamic Task Breakdown:</strong> Subtasks aren't pre-defined but determined by the orchestrator</li>
    <li><strong>Flexible Delegation:</strong> Workers can be assigned different tasks based on current needs</li>
    <li><strong>Result Synthesis:</strong> Orchestrator combines worker outputs into final result</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  ```python
  # Central orchestrator agent
  orchestrator = Agent(
      name="Project Orchestrator",
      instruction="Break down complex projects into manageable tasks and coordinate workers",
      functions=[decompose_task, assign_workers, synthesize_results],
      server_names=["filesystem"]
  )
  
  # Worker agents with specialized skills
  finder_agent = Agent(
      name="Information Finder",
      instruction="Find relevant information from various sources",
      functions=[web_search, database_query, file_search],
      server_names=["fetch", "filesystem"]
  )
  
  writer_agent = Agent(
      name="Content Writer", 
      instruction="Create high-quality written content",
      functions=[write_content, edit_text, format_document],
      server_names=["filesystem"]
  )
  
  proofreader = Agent(
      name="Proofreader",
      instruction="Review and improve content quality",
      functions=[grammar_check, style_improve, fact_verify],
      server_names=["filesystem"]
  )
  
  fact_checker = Agent(
      name="Fact Checker",
      instruction="Verify factual accuracy of content",
      functions=[fact_verification, source_validation],
      server_names=["fetch", "filesystem"]
  )
  
  style_enforcer = Agent(
      name="Style Enforcer",
      instruction="Ensure content follows specified style guidelines",
      functions=[style_check, format_standardize],
      server_names=["filesystem"]
  )
  
  # Orchestrator manages all workers
  orchestrator = Orchestrator(
      llm_factory=AnthropicAugmentedLLM,
      available_agents=[finder_agent, writer_agent, proofreader, fact_checker, style_enforcer],
      planner=planner_llm
  )
  ```
  
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Complex Coding Tasks:</strong> Dynamic file changes based on requirements</li>
    <li><strong>Research Projects:</strong> Multi-source information gathering and synthesis</li>
    <li><strong>Content Creation:</strong> Multi-stage content development with specialized workers</li>
  </ul>
</Card>

### Workflow: Evaluator-Optimizer

In evaluator-optimizer workflows, one LLM call generates a response while another provides evaluation and feedback in a loop.

<Card title="Evaluator-Optimizer Implementation">
  <h4>Key Components:</h4>
  <ul>
    <li><strong>Generator Agent:</strong> Creates initial responses or content</li>
    <li><strong>Evaluator Agent:</strong> Assesses quality and provides feedback</li>
    <li><strong>Optimization Loop:</strong> Iterative improvement based on feedback</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  ```python
  # Generator agent creates content
  generator = Agent(
      name="Content Generator",
      instruction="Create high-quality content based on requirements",
      functions=[generate_content, create_draft],
      server_names=["filesystem"]
  )
  
  # Evaluator agent provides feedback
  evaluator = Agent(
      name="Quality Evaluator",
      instruction="Evaluate content quality and provide specific improvement feedback",
      functions=[evaluate_quality, provide_feedback, suggest_improvements],
      server_names=["filesystem"]
  )
  
  # Optimizer agent implements feedback
  optimizer = Agent(
      name="Content Optimizer",
      instruction="Improve content based on evaluator feedback",
      functions=[implement_feedback, refine_content, polish_text],
      server_names=["filesystem"]
  )
  
  # Orchestrator manages the evaluation loop
  evaluator_orchestrator = Orchestrator(
      llm_factory=AnthropicAugmentedLLM,
      available_agents=[generator, evaluator, optimizer],
      planner=planner_llm
  )
  ```
  
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Content Refinement:</strong> Iterative improvement of written content</li>
    <li><strong>Code Optimization:</strong> Continuous improvement of code quality</li>
    <li><strong>Translation Quality:</strong> Multi-round translation refinement</li>
  </ul>
</Card>

### Workflow: Signaling and Human Input

Advanced workflows can pause and resume tasks, allowing for human input and external signals.

<Card title="Human Input Integration">
  <h4>Key Features:</h4>
  <ul>
    <li><strong>Pause/Resume:</strong> Workflows can be paused for human review</li>
    <li><strong>Human Input Callbacks:</strong> Agents can request user input mid-workflow</li>
    <li><strong>Approval Gates:</strong> Critical decisions require human approval</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  ```python
  from mcp_agent.human_input.handler import console_input_callback
  
  # Agent with human input capability
  customer_service_agent = Agent(
      name="Customer Service Agent",
      instruction="Handle customer inquiries with human oversight for complex cases",
      functions=[escalate_to_human, resolve_simple_issues, transfer_to_specialist],
      server_names=["fetch", "filesystem"],
      human_input_callback=console_input_callback  # Enable human input
  )
  
  # Workflow with approval gates
  approval_workflow = Orchestrator(
      llm_factory=AnthropicAugmentedLLM,
      available_agents=[customer_service_agent],
      planner=planner_llm
  )
  ```
  
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Customer Service:</strong> Escalation to human agents for complex cases</li>
    <li><strong>Content Approval:</strong> Human review before publishing</li>
    <li><strong>Critical Decisions:</strong> Human oversight for important business decisions</li>
  </ul>
</Card>

---

## MCP Server Management

The [MCP Agent framework](https://github.com/lastmile-ai/mcp-agent) provides sophisticated tools for managing Model Context Protocol (MCP) servers, enabling seamless integration of external capabilities into your agent workflows.

### MCP Server Configuration

<Card title="Server Setup">
  <h4>Configuration File:</h4>
  <p>Create an <code>mcp_agent.config.yaml</code> to define server configuration:</p>
  
  ```yaml
  mcp:
    servers:
      fetch:
        command: "uvx"
        args: ["mcp-server-fetch"]
        description: "Fetch content at URLs from the world wide web"
      
      filesystem:
        command: "uvx"
        args: ["mcp-server-filesystem"]
        description: "Access local filesystem for reading and writing files"
      
      database:
        command: "uvx"
        args: ["mcp-server-sqlite"]
        description: "Query and manage SQLite databases"
  ```
  
  <h4>Secrets Management:</h4>
  <p>Define secrets via either a gitignored <code>mcp_agent.secrets.yaml</code> or a local <code>.env</code> file. In production, prefer <code>MCP_APP_SETTINGS_PRELOAD</code> to avoid writing plaintext secrets to disk.</p>
</Card>

### Server Connection Patterns

<Card title="Connection Management">
  <h4>1. Context Manager Pattern (Recommended):</h4>
  <p>Use <code>gen_client</code> for automatic lifecycle management:</p>
  
  ```python
  from mcp_agent.mcp.gen_client import gen_client
  
  async with gen_client("fetch") as fetch_client:
      # Fetch server is initialized and ready to use
      result = await fetch_client.list_tools()
  
  # Fetch server is automatically disconnected/shutdown
  ```
  
  <h4>2. Persistent Connections:</h4>
  <p>For workflows requiring persistent server connections:</p>
  
  ```python
  from mcp_agent.mcp.gen_client import connect, disconnect
  
  fetch_client = None
  try:
      fetch_client = connect("fetch")
      result = await fetch_client.list_tools()
  finally:
      disconnect("fetch")
  ```
  
  <h4>3. Advanced Connection Management:</h4>
  <p>Use <code>MCPConnectionManager</code> for fine-grained control:</p>
  
  ```python
  from mcp_agent.context import get_current_context
  from mcp_agent.mcp.mcp_connection_manager import MCPConnectionManager
  
  context = get_current_context()
  connection_manager = MCPConnectionManager(context.server_registry)
  
  async with connection_manager:
      fetch_client = await connection_manager.get_server("fetch")
      result = fetch_client.list_tool()
      fetch_client2 = await connection_manager.get_server("fetch")  # Reuses same connection
  ```
</Card>

### MCP Server Aggregation

<Card title="Server Aggregation">
  <h4>MCPAggregator:</h4>
  <p>Acts as a "server-of-servers" providing a single MCP server interface for multiple servers:</p>
  
  ```python
  from mcp_agent.mcp.mcp_aggregator import MCPAggregator
  
  aggregator = await MCPAggregator.create(server_names=["fetch", "filesystem"])
  
  async with aggregator:
      # Combined list of tools from all servers
      tools = await aggregator.list_tools()
      
      # Namespaced tool calls
      fetch_result = await aggregator.call_tool(
          name="fetch-fetch", 
          arguments={"url": "https://example.com"}
      )
      
      # Non-namespaced tool calls (first server wins)
      read_file_result = await aggregator.call_tool(
          name="read_file", 
          arguments={"path": "data.txt"}
      )
  ```
  
  <h4>Benefits:</h4>
  <ul>
    <li><strong>Unified Interface:</strong> Single point of access to multiple servers</li>
    <li><strong>Tool Namespacing:</strong> Avoid conflicts between servers with similar tools</li>
    <li><strong>Simplified Management:</strong> Handle multiple server connections through one interface</li>
  </ul>
</Card>

## MCP Agent Framework Benefits

The [MCP Agent framework](https://github.com/lastmile-ai/mcp-agent) provides several core benefits that make it an excellent choice for building sophisticated AI agent systems:

<Card title="Core Benefits">
  <h4>🤝 Interoperability</h4>
  <p>Ensures that any tool exposed by any number of MCP servers can seamlessly plug into your agents. This creates a modular ecosystem where you can mix and match capabilities.</p>
  
  <h4>⛓️ Composability & Customizability</h4>
  <p>Implements well-defined workflows in a composable way that enables compound workflows and allows full customization across model provider, logging, orchestrator, etc.</p>
  
  <h4>💻 Programmatic Control Flow</h4>
  <p>Keeps things simple as developers just write code instead of thinking in graphs, nodes and edges. For branching logic, you write <code>if</code> statements. For cycles, use <code>while</code> loops.</p>
  
  <h4>🖐️ Human Input & Signals</h4>
  <p>Supports pausing workflows for external signals, such as human input, which are exposed as tool calls an Agent can make.</p>
</Card>

### Deployment Options

<Card title="Flexible Deployment">
  <h4>1. MCP-Agent Server</h4>
  <p>Expose mcp-agent applications as MCP servers themselves, allowing MCP clients to interface with sophisticated AI workflows using the standard tools API of MCP servers.</p>
  
  <h4>2. MCP Client or Host</h4>
  <p>Embed mcp-agent in an MCP client directly to manage the orchestration across multiple MCP servers.</p>
  
  <h4>3. Standalone</h4>
  <p>Use mcp-agent applications in a standalone fashion (i.e. they aren't part of an MCP client). The examples are all standalone applications.</p>
</Card>

## Advanced Agent Architectures

*Content from [Prompt Engineering Guide](https://www.promptingguide.ai/agents)*

### Agent Components Deep Dive

#### 1. Memory Systems

Agents need memory to maintain context across interactions:

**Types of Memory:**
- **Short-term Memory**: Current conversation context
- **Long-term Memory**: Persistent knowledge and preferences
- **Episodic Memory**: Past experiences and outcomes
- **Semantic Memory**: Factual knowledge and concepts

**Memory Implementation:**
```python
class AgentMemory:
    def __init__(self):
        self.short_term = []
        self.long_term = {}
        self.episodic = []
        self.semantic = {}
    
    def add_to_short_term(self, message):
        self.short_term.append(message)
        if len(self.short_term) > 10:
            self.short_term.pop(0)
    
    def store_in_long_term(self, key, value):
        self.long_term[key] = value
    
    def retrieve_relevant(self, query):
        # Implement retrieval logic
        pass
```

#### 2. Tool Integration

Tools extend agent capabilities beyond language generation:

**Tool Categories:**
- **Information Retrieval**: Search engines, databases
- **Computation**: Calculators, data analysis tools
- **Communication**: Email, messaging APIs
- **File Operations**: Read/write files, process documents
- **External APIs**: Weather, news, financial data

**Tool Implementation:**
```python
class Tool:
    def __init__(self, name, description, function):
        self.name = name
        self.description = description
        self.function = function
    
    def execute(self, *args, **kwargs):
        return self.function(*args, **kwargs)

# Example tools
web_search = Tool(
    name="web_search",
    description="Search the web for current information",
    function=search_web
)

calculator = Tool(
    name="calculator",
    description="Perform mathematical calculations",
    function=calculate
)
```

#### 3. Planning and Reasoning

Agents need planning capabilities to break down complex tasks:

**Planning Approaches:**
- **Hierarchical Planning**: Break tasks into subtasks
- **Reactive Planning**: Respond to immediate needs
- **Predictive Planning**: Anticipate future requirements
- **Adaptive Planning**: Modify plans based on feedback

**Planning Implementation:**
```python
class Planner:
    def __init__(self, agent):
        self.agent = agent
        self.current_plan = []
        self.goals = []
    
    def create_plan(self, goal):
        # Break down goal into actionable steps
        steps = self.decompose_goal(goal)
        self.current_plan = steps
        return steps
    
    def execute_step(self, step):
        # Execute a single step in the plan
        result = self.agent.execute_tool(step.tool, step.parameters)
        return result
    
    def adapt_plan(self, feedback):
        # Modify plan based on feedback
        self.current_plan = self.replan(self.current_plan, feedback)
```

### Multi-Agent Systems

#### 1. Agent Communication

Agents need to communicate effectively with each other:

**Communication Patterns:**
- **Direct Communication**: Agents talk directly to each other
- **Broadcast Communication**: One agent sends to all others
- **Mediated Communication**: Central coordinator manages communication
- **Structured Communication**: Predefined message formats

**Message Format:**
```python
class AgentMessage:
    def __init__(self, sender, receiver, content, message_type):
        self.sender = sender
        self.receiver = receiver
        self.content = content
        self.message_type = message_type
        self.timestamp = time.time()
        self.id = generate_id()

# Message types
class MessageType:
    REQUEST = "request"
    RESPONSE = "response"
    NOTIFICATION = "notification"
    ERROR = "error"
```

#### 2. Coordination Mechanisms

**Centralized Coordination:**
- Single coordinator manages all agents
- Clear hierarchy and control flow
- Easier to debug and monitor

**Decentralized Coordination:**
- Agents coordinate directly with each other
- More flexible and scalable
- Requires robust communication protocols

**Market-Based Coordination:**
- Agents bid on tasks
- Price-based resource allocation
- Self-organizing system

#### 3. Conflict Resolution

**Common Conflicts:**
- **Resource Conflicts**: Multiple agents need the same resource
- **Goal Conflicts**: Agents have competing objectives
- **Timing Conflicts**: Agents need to coordinate timing

**Resolution Strategies:**
- **Negotiation**: Agents discuss and compromise
- **Voting**: Majority decides on conflicts
- **Priority-Based**: Higher priority agents get preference
- **Time-Sharing**: Resources shared over time

### Agent Learning and Adaptation

#### 1. Reinforcement Learning

Agents can learn from their experiences:

```python
class LearningAgent:
    def __init__(self):
        self.policy = {}
        self.value_function = {}
        self.experience_buffer = []
    
    def update_policy(self, state, action, reward, next_state):
        # Update policy based on experience
        self.experience_buffer.append((state, action, reward, next_state))
        self.learn_from_experience()
    
    def choose_action(self, state):
        # Choose action based on learned policy
        return self.policy.get(state, random_action())
```

#### 2. Transfer Learning

Agents can transfer knowledge between tasks:

**Knowledge Transfer Methods:**
- **Feature Transfer**: Share learned features
- **Policy Transfer**: Adapt policies to new tasks
- **Model Transfer**: Use pre-trained models
- **Experience Transfer**: Share relevant experiences

#### 3. Meta-Learning

Agents can learn how to learn:

```python
class MetaLearner:
    def __init__(self):
        self.meta_policy = {}
        self.task_embeddings = {}
    
    def adapt_to_new_task(self, task_description):
        # Quickly adapt to new tasks
        task_embedding = self.encode_task(task_description)
        adapted_policy = self.meta_policy.adapt(task_embedding)
        return adapted_policy
```

### Agent Safety and Ethics

#### 1. Safety Mechanisms

**Alignment:**
- Ensure agents pursue intended goals
- Prevent goal misalignment
- Monitor for unintended behaviors

**Constrained Optimization:**
- Set safety constraints
- Implement guardrails
- Monitor constraint violations

**Robustness:**
- Handle adversarial inputs
- Maintain performance under stress
- Recover from failures

#### 2. Ethical Considerations

**Fairness:**
- Avoid bias in decision-making
- Ensure equal treatment
- Monitor for discrimination

**Transparency:**
- Explain agent decisions
- Provide reasoning behind actions
- Enable human oversight

**Accountability:**
- Track agent actions
- Maintain audit trails
- Enable responsibility assignment

### Advanced Orchestration Patterns

#### 1. Swarm Intelligence

Multiple simple agents working together:

```python
class SwarmAgent:
    def __init__(self, position, velocity):
        self.position = position
        self.velocity = velocity
        self.best_position = position
        self.best_fitness = float('inf')
    
    def update(self, global_best, neighbors):
        # Update based on personal best, global best, and neighbors
        self.velocity = self.calculate_velocity(global_best, neighbors)
        self.position += self.velocity
        self.update_best()
```

#### 2. Emergent Behavior

Complex behaviors arising from simple rules:

**Emergence Examples:**
- **Flocking**: Birds flying in formation
- **Traffic Flow**: Cars organizing into lanes
- **Market Dynamics**: Prices emerging from individual decisions

#### 3. Self-Organizing Systems

Agents organize themselves without central control:

**Self-Organization Principles:**
- **Local Interactions**: Agents interact with neighbors
- **Feedback Loops**: Actions affect future behavior
- **Stigmergy**: Communication through environment
- **Emergence**: Complex patterns from simple rules

### Agent Evaluation and Testing

#### 1. Performance Metrics

**Task Completion:**
- Success rate
- Completion time
- Quality of results

**Efficiency:**
- Resource usage
- Computational cost
- Communication overhead

**Robustness:**
- Error handling
- Recovery time
- Performance under stress

#### 2. Testing Strategies

**Unit Testing:**
- Test individual agent components
- Verify tool functionality
- Validate decision logic

**Integration Testing:**
- Test agent interactions
- Verify communication protocols
- Validate coordination mechanisms

**System Testing:**
- Test complete multi-agent systems
- Verify end-to-end functionality
- Validate performance under load

#### 3. Continuous Monitoring

**Real-time Monitoring:**
- Track agent performance
- Monitor system health
- Detect anomalies

**Logging and Analytics:**
- Record agent actions
- Analyze performance patterns
- Identify improvement opportunities

> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [Building LangChain Agents for LLM Applications in Python](https://www.codecademy.com/article/building-langchain-agents-for-llm-applications-in-python)  
> for internal educational use only (non-profit).

## Building LangChain Agents for LLM Applications in Python

Large language models (LLMs) have transformed the way we work. LLMs are trained on large datasets, and they have access to an amount of information beyond the capabilities of human beings. LLMs also have reasoning, question-answering, and conversational capabilities that enable them to write code, analyze data, make reports, and perform various other tasks. However, they lack access to real-time information, struggle with complex mathematical computations, and their knowledge is constrained by the cutoff date of their training data. To overcome these limitations, we can use LangChain agents.

With langchain agents, we can enable LLMs to fetch up-to-date information, perform precise mathematical calculations, and interact with external environments dynamically. In this article, we'll discuss what LangChain agents are and their components. We will also build langchain agents that perform tasks by fetching real-time data and using specific tools for mathematical operations.

### What are langchain agents?

Langchain agents are intelligent AI applications that enable LLM applications to interact with external tools, APIs, and inputs dynamically. Unlike basic LLM applications that generate responses based on static training data, agents can reason, plan, and execute tasks using different tools. A langchain agent uses decision-making logic and reasoning steps to determine the requirements to generate the desired output and the actions needed.

#### Why do we need langchain agents?

Large language models lack access to real-time information, struggle with complex mathematical computations, and their knowledge is constrained by the cutoff date of their training data. To understand this, let's create an LLM application using langchain and ask it the question, "Which government department is Elon Musk heading currently?". To run the application, you must have a gemini API key.

```python
from langchain_google_genai import ChatGoogleGenerativeAI
import os
os.environ['GOOGLE_API_KEY'] = "your_API_key"
llm = ChatGoogleGenerativeAI(model="gemini-pro")
prompt="Which government department is Elon Musk heading currently?"
print("The prompt is:",prompt)
llm_output=llm.invoke(prompt)
print("The output for the prompt is:")
print(llm_output.content)
```

Output:

```
The prompt is: Which government department is Elon Musk heading currently?
The output for the prompt is:
Elon Musk is not currently heading any government department.
```

When writing this article in 2025, Elon Musk heads the "Department of Government Efficiency (DOGE)" in the United States government. However, the output says otherwise. The LLM application doesn't have this updated information because the cutoff date for the training data for the latest gemini-pro model is March 29, 2023. The model doesn't know the events after this date, which affects the model output. How do we overcome this limitation?

The answer is langchain agents. We can use a langchain agent that produces the output using a search engine with the LLM model. In such a case, the agent can always fetch the results from the search engine if it doesn't have any information about a topic. Similarly, we need langchain agents to use databases, perform mathematical calculations, and execute any other task that cannot be done using just the large language model.

### How does a langchain agent work?

Instead of generating the output using the training data in the LLM application, a langchain agent dynamically chooses the tools, databases, APIs, etc., to use based on the input and current context. For this, it uses the following steps:

1. **Receive and preprocess input**: The langchain agent takes a question or a command from the user and creates a prompt using prompt templates if necessary.
2. **Generate requirements**: After preprocessing the input, the agent analyzes the requirements based on the current context to generate the output.
3. **Decide on an action**: After analyzing the requirements, the agent decides on the tool and action to get the output.
4. **Execute the action and generate output**: Next, the langchain agent executes the decided action to get the output.
5. **Analyze the output**: After executing the action, the agent checks if it has obtained the desired final output. If yes, it returns the output. Otherwise, it analyzes the information collected from the previous steps and decides on the following action. For this, the agent returns to step 2 and iterates until it reaches the desired output.
6. **Return a final response**: After obtaining the desired result, the agent formats and returns the final output.

To execute the above steps, a langchain agent uses multiple components. Let's look at the different components a langchain agent can use to generate outputs.

### Different components of a langchain agent

A langchain agent processes the input, analyzes the context, gathers information, and executes actions based on reasoning and requirements to generate an output. To complete these steps, the agent needs different components.

- **Large language model**: Each langchain agent uses an LLM model to generate responses to the inputs and decide on the next step. The LLM model helps the agent to understand the inputs, reason about the problems, and generate meaningful outputs.
- **Prompt template**: The prompt template in a langchain agent contains a set of instructions that helps the LLM in reasoning and problem-solving. It enables the agent to understand the user input and the expected output.
- **Tools**: Tools are external resources that a langchain agents use to perform a specific task. A tool is designed to be called by an agent, its input is designed to be generated by an LLM, and its output is intended to be passed back to the LLM. We can use a database, an API, a search engine, a web browser, or even a custom Python function as a tool. You can get a list of all the available tools at langchain toolkit.
- **Memory**: LangChain agents can have the memory to store information. The memory helps the langchain agent store previous interactions and use the information for decision-making, making them more intelligent over time.
- **Environment**: The environment is the contextual space, including the LLM, input prompts, intermediate outputs, tools, and memory using which the langchain agent works.
- **Agent executor**: The agent executor runs the langchain agent. It manages the interaction between the agent and the tools or resources that the langchain agent uses. The agent executor also manages the execution of tasks based on the agent's decisions and ensures that all the actions are executed based on the agent's reasoning.

Using these components, we can create langchain agents that extend an LLM's capabilities. Let's now explore how to build a langchain agent in Python.

### How to build a langchain agent in Python

Let's build a langchain agent that uses a search engine to get information from the web if it doesn't have specific information. For this task, we will use the DuckDuckGo search engine, which comes as a tool in Langchain. First, install the duckduckgo-search module by executing the following command in the command-line terminal.

```bash
pip3 install duckduckgo-search
```

Now, we will first create an LLM object and a search tool for the agent. To create an LLM object, you can use the OpenAI models or Gemini AI. We will be using the gemini-pro for the demonstration.

```python
from langchain_google_genai import ChatGoogleGenerativeAI
import os
os.environ['GOOGLE_API_KEY'] = "your_API_key"
llm = ChatGoogleGenerativeAI(model="gemini-pro")
```

After creating the LLM object, we will create the search tool using the DuckDuckGoSearchResults() function defined in the langchain_community.tools module. If the langchain_community module is not already installed on your system, you can install it with the command: `pip install langchain-community`.

```python
from langchain_community.tools import DuckDuckGoSearchResults
ddg_search = DuckDuckGoSearchResults()
```

We now have an LLM object that can answer questions using the information available in the training data. We also have a search engine tool that the langchain agent can use if it needs any new information. Hence, let's now create a langchain agent using these components.

To create an agent, we will use the initialize_agents() function defined in the langchain.agents module. The initialize_agents() function takes the LLM object as input to its llm parameter and a list of tools as input to the tools parameter. We also need to define the agent type to initialize the langchain agent. We will use an agent with the ZERO_SHOT_REACT_DESCRIPTION type that does a reasoning step before acting. You can create an agent for other agent types too.

```python
from langchain.agents import AgentType, initialize_agent
agent = initialize_agent(
    tools=[ddg_search],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION
)
```

Our langchain agent with a built-in search engine tool is ready now. Let's now ask the langchain agent "Which government department is Elon Musk heading currently?" using the invoke() method.

```python
prompt="Which government department is Elon Musk heading currently?"
print("The prompt is:",prompt)
# Get output
agent_output= agent.invoke(prompt)
print("The output for the prompt is:")
print(agent_output.get('output'))
```

Output:

```
The prompt is: Which government department is Elon Musk heading currently?
The output for the prompt is:
Elon Musk is heading the Department of Government Efficiency (DOGE) in the Trump administration.
```

In this output, you can observe that the agent correctly answers our question about Elon Musk heading the Department of Government Efficiency. Thus, we have successfully built a langchain agent that uses a search engine tool to answer the input prompts.

Now, let's see the steps the langchain agent uses to generate this output. To do this, we will set the verbose parameter to True in the initialize_agent() method. This makes the langchain agent print all the steps it uses while working on an input prompt, as shown in this example:

```python
verbose_agent = initialize_agent(
    tools=[ddg_search],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

prompt="Which government department is Elon Musk heading currently?"
print("The prompt is:",prompt)
# Get output
agent_output= verbose_agent.invoke(prompt)
print("The output for the prompt is:")
print(agent_output.get('output'))
```

Output:

```
The prompt is: Which government department is Elon Musk heading currently?

> Entering new AgentExecutor chain...
Action: duckduckgo_results_json
Action Input: What government department is Elon Musk heading?
Observation: snippet: People protest against President Trump and Elon Musk's Department of Government Efficiency (DOGE) near the U.S. Capitol in Washington, D.C., on Feb. 5, 2025., title: Who is part of Elon Musk's DOGE, and what are they doing?, link: https://www.npr.org/2025/02/07/nx-s1-5288988/doge-elon-musk-staff-trump, snippet: With Elon Musk's new department comes a number of questions about his position, the program itself and whether Musk is allowed to work in federal government at all. Recently, Musk was made a ..., title: What's Elon Musk's position in government? The DOGE leader's role, link: https://www.usatoday.com/story/news/2025/02/07/elon-musk-position-us-government/78328968007/, snippet: President-elect Donald Trump announced Tuesday that Elon Musk and Vivek Ramaswamy will lead a new "Department of Government Efficiency" in his second administration. "Together, these two ..., title: Elon Musk and Vivek Ramaswamy will lead new 'Department of Government ..., link: https://www.cnn.com/2024/11/12/politics/elon-musk-vivek-ramaswamy-department-of-government-efficiency-trump/index.html, snippet: President Trump said the entity would focus on cutting government waste and slashing federal regulations, and he put tech billionaire and adviser Elon Musk in charge. Politics DOGE is making major ..., title: What is the Department of Government Efficiency, or DOGE? : NPR, link: https://www.npr.org/2025/02/04/nx-s1-5286314/department-of-government-efficiency-doge-explainer-elon-musk
Final Answer: Elon Musk is heading the Department of Government Efficiency (DOGE) in the Trump administration.

> Finished chain.
The output for the prompt is:
Elon Musk is heading the Department of Government Efficiency (DOGE) in the Trump administration.
```

In this output, you can observe that the langchain agent executes the duckduckgo_results_json action with the input What government department is Elon Musk heading?. You can see that the input to the duckduckgo_results_json action differs from the input prompt. This is because the agent first analyzes the input prompt and finds that it cannot retrieve the output from the llm. Hence, it searches for the question What government department is Elon Musk heading? in the DuckDuckGo search engine. The duckduckgo_results_json action returns text snippets from multiple new links. The agent analyzes these snippets and finds out the answer to the input prompt.

### Creating a langchain agent with multiple tools in Python

To make the langchain agent more capable, we can add multiple tools to it. For this, we need to pass all the tools to the tools parameter in the initialize_agent() method. To see how it works, let's create a tool for mathematical calculations. For this, we will use LLMMathChain, which is defined in the langchain.chains module. LLMMathChain is a specialized chain designed to handle mathematical operations within the context of a large language model.

To define the math tool using LLMMathChain, we use the from_function() method defined in the langchain.agents.Tools module. The from_function() method creates a tool that executes a specific function when the langchain agent calls it. We use the following steps to create the math tool.

- First, we will create a LLMMathChain object using the from_llm() method. The from_llm() method takes an LLM object as input and returns an LLMMathChain object. Let's name the chain math_chain.
- Next, we will use the from_function() method to create the math tool. It takes the name for the tool, the function to call when the tool is executed, and a description of the tool as inputs to the name, func, and description parameters respectively. We will pass the name Calculator, the function math_chain.run, and the description Use this tool for math questions and nothing else. Only input math expressions. as the input.

```python
from langchain.chains import LLMMathChain
from langchain.agents import Tools
math_chain = LLMMathChain.from_llm(llm=llm)
math_tool = Tool.from_function(name="Calculator",
                                func=math_chain.run,
                                description="Use this tool for mathematical operations and nothing else. Only input math expressions.")
```

After executing the from_function() method, we will get a Calculator tool that the langchain agent can use for mathematical operations. Now, let's create a langchain agent using the search engine and the calculator tool. This time, we will ask the question, "Which government department is Elon Musk heading currently? How much cost does he aim to save for the USA government as an absolute number and as a percentage of the total GDP of the USA?" so that the agent uses the Calculator tool.

```python
agent_with_two_tools = initialize_agent(
    tools=[ddg_search,math_tool],
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

prompt="""Which government department is Elon Musk heading currently? How much cost does he aim to save for the USA government as an absolute number and as a percentage of the total GDP of the USA?"""
print("The prompt is:",prompt)
agent_output= agent_with_two_tools.invoke(prompt)
print("The output for the prompt is:")
print(agent_output.get('output'))
```

Output:

```
The prompt is: Which government department is Elon Musk heading currently? How much cost does he aim to save for the USA government as an absolute number and as a percentage of the total GDP of the USA?

> Entering new AgentExecutor chain...
Action: duckduckgo_results_json
Action Input: How much cost does Elon Musk aim to save for the USA government as absolute number and as a percentage of total GDP of USA.
Observation: snippet: President Trump, Elon Musk and the Department of Government Efficiency have touted billions in terminated contracts deemed wasteful, but there's little transparency about how savings are tracked., title: Musk claims DOGE saves taxpayers billions, but data is unclear - NPR, link: https://www.npr.org/2025/02/11/nx-s1-5290288/doge-savings-billions-contracts-musk-trump, snippet: Mr. Trump and Musk have also taken aim at the U.S. Agency for International Development, or USAID, upending an organization that provides humanitarian aid to more than 100 countries., title: What is DOGE? Here's what to know about Elon Musk's latest cost-cutting ..., link: https://www.cbsnews.com/news/what-is-doge-elon-musk-findings-trump/
Action: duckduckgo_results_json
Action Input: Which government department is Elon Musk heading currently?
Observation: snippet: Elon Musk has finally received his official White House email address. This means that general citizens will now be able to reach out to the Tesla CEO and newly appointed head of the Department of Government Efficiency through this email address., title: Elon Musk White House email: Elon Musk gets the coveted White House ..., link: https://economictimes.indiatimes.com/news/international/us/elon-musk-gets-the-coveted-white-house-email-address-heres-how-people-can-reach-out-to-him/articleshow/117429421.cms, snippet: With Elon Musk's new department comes a number of questions about his position, the program itself and whether Musk is allowed to work in federal government at all. Recently, Musk was made a ..., title: What's Elon Musk's position in government? The DOGE leader's role, link: https://www.usatoday.com/story/news/2025/02/07/elon-musk-position-us-government/78328968007/
Final Answer: Elon Musk is heading the Department of Government Efficiency (DOGE) and aims to save $3 billion for the USA government, which is 0.001% of the total GDP of the USA.

> Finished chain.
The output for the prompt is:
Elon Musk is heading the Department of Government Efficiency (DOGE) and aims to save $3 billion for the USA government, which is 0.001% of the total GDP of the USA.
```

In this output, you can observe that the langchain agent uses the DuckDuckGo search tool two times to get the output. Here, the agent got the percentage value from the news snippets and didn't use the Calculator tool. Let's now ask the agent a question that will force it to do a mathematical calculation.

```python
prompt="""Which government department is Elon Musk heading currently? Add 11117 to how much cost he aims to save for the USA government and give the number."""
print("The prompt is:",prompt)
agent_output= agent_with_two_tools.invoke(prompt)
print("The output for the prompt is:")
print(agent_output.get('output'))
```

Output:

```
The prompt is: Which government department is Elon Musk heading currently? Add 11117 to how much cost he aims to save for the USA government and give the number.

> Entering new AgentExecutor chain...
Assistant
Action: duckduckgo_results_json
Action Input: Which government department is Elon Musk heading currently?
Observation: snippet: Elon Musk has finally received his official White House email address. This means that general citizens will now be able to reach out to the Tesla CEO and newly appointed head of the Department of Government Efficiency through this email address., title: Elon Musk White House email: Elon Musk gets the coveted White House ..., link: https://economictimes.indiatimes.com/news/international/us/elon-musk-gets-the-coveted-white-house-email-address-heres-how-people-can-reach-out-to-him/articleshow/117429421.cms, snippet: With Elon Musk's new department comes a number of questions about his position, the program itself and whether Musk is allowed to work in federal government at all. Recently, Musk was made a ..., title: What's Elon Musk's position in government? The DOGE leader's role, link: https://www.usatoday.com/story/news/2025/02/07/elon-musk-position-us-government/78328968007/, snippet: In an unorthodox move, Elon Musk, now head of the newly established Department of Government Efficiency (DOGE), stood alongside President Donald J. Trump to address the public regarding his role ..., title: Elon Musk's New Role: Head of Department of Government Efficiency ..., link: https://dallasweekly.com/2025/02/elon-musk-doge-reform-government/, snippet: President-elect Donald Trump tapped Elon Musk and Vivek Ramaswamy to lead the Department of Government Efficiency, or DOGE, in November. Brandon Bell via Getty Images So, why all the changes?, title: DOGE Is Official, but It's a Very Different Department - Business Insider, link: https://www.businessinsider.com/doge-different-musk-official-white-house-trump-2025-1?op=1
Thought: I know Elon Musk is the head of the Department of Government Efficiency.
Action: Calculator
Action Input: 11117 + 11325000000
Observation: Answer: 11325011117
The total cost is 11325011117.
Final Answer: 11325011117

> Finished chain.
The output for the prompt is:
11325011117
```

In this example, we asked the langchain agent to add 11117 to the expected savings. Due to this, the agent uses the Calculator tool to generate the final output. Hence, langchain agents use their tools only when needed. At every stage, they analyze the context, available information, and the goal to decide on the following action. Once the agent achieves the goal, it returns the output.

### Chains vs agents: what should you use?

In langchain, chains are used to perform a task with predefined steps. They lack decision-making capabilities and aren't designed to adapt to changing circumstances. Hence, you can use chains if you want to perform a task with predefined steps. For example, you can use chains to extract data from a JSON file or perform a series of calculations on a dataset.

On the flip side, langchain agents are capable of reasoning and can perform an action based on the context and the goal. They are more autonomous and can identify what to do next. This makes agents an ideal choice for complex, non-linear workflows where we need flexibility. For example, you can use langchain agents to create LLM applications that answer open-ended questions with context-aware answers or manage a workflow that requires real-time decision-making.

### Conclusion

With external tools and access to real-time information, langchain agents are one of the best ways to use large language models. They help us automate complex tasks by dynamically making decisions based on context and available information. We just need to give the input prompt to the agent, and it will decide on what tools to use and what actions to take to produce the output. Whether you want to fetch and analyze stock prices, solve complex mathematical equations, or automate tasks in applications like Slack, Jira, GitHub, or Microsoft 365, langchain agents can perform all these tasks by combining the reasoning capabilities of large-language models and task-specific tools.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [Insights on AI Usage for an Agentic AI Future](https://www.uxmatters.com/mt/archives/2025/08/insights-on-ai-usage-for-an-agentic-ai-future.php)  
> for internal educational use only (non-profit).

# Insights on AI Usage for an Agentic AI Future

## Conscious Experience Design

### Designing for the evolving human+machine relationship

> We're now on the brink of an even bigger shift: agentic AI, in which AI's can act independently on a user's behalf to complete complex, multistep tasks.

Artificial intelligence (AI) is changing rapidly and steadily reshaping consumers' views and behaviors. For over two decades, AI has shaped digital experiences, from invisible graphic user interfaces (GUIs) and voice assistants to generative chatbots. But we're now on the brink of an even bigger shift: agentic AI, in which AI's can act independently on a user's behalf to complete complex, multistep tasks.

To prepare for the coming broad adoption of agentic AI, we look to consumers. In our latest research at Punchcut, we surveyed over 600 consumers to understand how today's attitudes and behaviors regarding generative AI (GenAI) can inform the creation of tomorrow's agentic AI experiences. We looked at proto-behaviors to understand technology usage that is not yet widespread, such as agentic AI. What we found is a story of rapid adoption, trust, and opportunity—one to which product teams and UX designers should pay close attention.

## From Curiosity to Capability

> The rapid uptake of GenAI is setting the stage for agentic AI to emerge much faster than previous technology revolutions….

GenAI adoption has gone mainstream, paving the way for agentic AI. According to our recent survey, 64% of respondents use at least one GenAI app and, among younger generations—especially Gen Z—that number jumps above 80%. The rapid uptake of GenAI is setting the stage for agentic AI to emerge much faster than previous technology revolutions such as the Web or smartphones.

But adoption isn't one-size-fits-all users. Our research identified three main user segments, each of which has distinct needs and behaviors. The data showed that the majority—51% of GenAI users—are power users. Multipurpose GenAI experiences remain the most popular. We identified the following three key user segments:

1. **AI dabblers** are early explorers who use AI only a few times a month, often for narrow or experimental purposes.
2. **Casual users** engage with AI weekly or daily, but their interactions remain focused on a limited set of use cases.
3. **Power users**, in contrast, tap into AI multiple times a day across a wide range of applications, pushing the boundaries of what the technology can do for them.

For product teams and UX designers, this segmentation underscores the need to create experiences that scale with the user's comfort level with AI. Each AI offering should provide an easy on-ramp for newcomers while continuing to challenge and engage seasoned power users.

## Designing for Generational Differences

> The way people use AI varies dramatically across age groups, and these generational differences should guide how we approach the design of AI experiences.

Generational differences are driving what role AI plays in consumers' lives. Across our research, we've found that the way people use AI varies dramatically across age groups, and these generational differences should guide how we approach the design of AI experiences.

These usage patterns indicate that there is an opportunity to create an agent platform that can adapt to serve all users better. It highlights the importance of designing for flexible role-shifting with agent experiences that can adapt their tone, capabilities, and personality based on the user's context and intent. A one-size-fits-all persona would quickly feel out of step in a world where the user might ask the same AI to write a business proposal one moment and offer relationship advice the next.

### Boomers

> Boomers … rely on AI apps for research, writing, search, and content creation.

Boomers use AIs as consultants, engaging in frequent short sessions for business coaching, content creation, and companionship.

Boomers use AI just one to three times a week, for five to fifteen minutes at a time, to write and create content. They look to AI as a business coach, helping them create promotional content for their businesses. Boomers who participated in our survey shared that they rely on AI apps for research, writing, search, and content creation. Several respondents described creating AI friends, with custom appearances, for social interactions.

*"I use it sometimes for writing, but mostly for creativity and brainstorming—getting ideas for things I'm working on."*—Boomer participant quotation

### Gen X

Gen Xers are frequently power users who are deeply integrating AI as assistants into their daily life and work tasks.

Gen X is increasingly adopting AI as a versatile virtual assistant, streamlining a wide range of daily personal and professional tasks and effectively leveraging AI to enhance their productivity, as well as to offload routine redundancies. They are using AI multiple times a day for practical tasks such as controlling their smart-home environment and devices, including lighting, climate, and cameras. They're the generation most likely to be power users.

*"I really enjoy using AI, and it's something I use on a daily basis as a friend, as a coworker, as someone to assist me in my work and personal daily tasks."*—Gen X participant quotation

### Millennials

> Some respondents are interacting with AI to help them refine their professional communication style, using the AI's outputs to achieve a new tone in written and verbal communications.

Millennials tend to use AIs as mavens, who are capable of providing expert sources of knowledge, personal recommenders, and collaborative planners.

Millennials are using AI for planning, typically spending 30 to 60 minutes per session. Key use cases for our respondents were meal planning and nutrition, with 56% of Millennials citing such AI use cases. Of those using AI to aid in their job searches, 62% of respondents were Millennials. Some respondents are interacting with AI to help them refine their professional communication style, using the AI's outputs to achieve a new tone in written and verbal communications.

*"I've asked AI to play the role of knowledge keeper. I'll share memories, stories, and personal preferences, so they're able to gain an idea of my likes and dislikes, and provide personal recommendations."*—Millennial participant quotation

### Gen Z

Gen Z experiments with using AI as a mentor, advisor, or even a friend, often blending professional and personal roles in a single interaction.

Gen Z is navigating independence, from starting careers to building routines, and AI fills the gap by handling flexible roles to satisfy these diverse needs. Whether it's as a therapist, executive assistant, or friend, we saw Gen Z experimenting with AI to fulfill a variety of roles for advice inputs. Research remains a primary use case, with users explicitly stating that they ask questions, seeking quick information, often as a replacement for a traditional search engine.

*"I asked them to be a life coach, providing me with guidance on major decisions in my career, relationships, and personal life."*—Gen Z participant quotation

## Trust Is the Gateway to the Adoption of Agentic AI

> We must design experiences that are transparent about what's happening with user data, consistent in their behavior, and capable of expressing empathy in a genuine way.

Across all segments, people are eager to use AI more, but trust—particularly around data privacy—remains a significant barrier to their usage of AI. A much lower percentage of users responded that they perceive data that they've shared with AI tools as private. Interestingly, users reporting both the lowest and highest AI usage, AI dabblers and power users, reported the highest rates of seeing AI as private, suggesting that privacy might be a factor keeping casual users from becoming power users.

Even among the most frequent users, most say that interacting with AI doesn't feel natural. The data showed that just over 60% of power users and over 80% of AI dabblers report that AI interactions don't feel natural. While the specific factors that are impacting this feeling of inauthenticity might include a lack of personalization or the use of a direct tone, more research will be necessary to assess the actual cause.

These are critical insights for UX designers, suggesting that trust will be as important as utility in an agentic future. Building trust requires more than privacy statements that are buried under a Settings menu. We must design experiences that are transparent about what's happening with user data, consistent in their behavior, and capable of expressing empathy in a genuine way. In other words, the experiences that succeed will be those that make users feel not only understood but also respected.

## Personalization Without the Settings Menu

> Personalization should emerge naturally—through conversational prompts, ambient cues, and adaptive onboarding that quickly learns and reflects a user's preferences.

While personalization has the potential to create richer AI interactions, most users have not yet taken the steps necessary to adjust their AI's tone, name, or personality. Power users are significantly more likely to customize AIs across various dimensions, indicating their desire for highly personalized AI experiences. Among the various dimensions of personalization, tone of voice, name, and personality are the most personalized elements of GenAI experiences.

When we asked consumers about their preference for an AI's tone of voice when interacting with GenAI platforms, across all user segments, most cited that they preferred a direct tone as their communication style. As users become more deeply engaged with AI, the preference for a direct tone decreases slightly, while the use of a polite tone sees a modest increase. The use of a personal tone remains relatively consistent across all age groups, but is still not the choice that the majority of consumers make. Today's personalization often requires navigating through static menus and toggles, making it more of a chore than a delight.

In the future of agentic AI, personalization should emerge naturally—through conversational prompts, ambient cues, and adaptive onboarding that quickly learns and reflects a user's preferences. The most effective personalization will feel less like configuration and more like collaboration, evolving with each user interaction.

## Designing for the Agentic Leap

> To build widespread adoption of agentic AI, future experiences must appeal to a wide range of consumers, not just power users.

Agentic AI shifts the design challenge from creating tools to cultivating relationships. That shift demands a different mindset. AI experiences must be approachable enough for AI dabblers to explore without friction, yet deep enough to reward ongoing user engagement. They must be capable of adapting in tone, style, and emotional resonance to make interactions feel more natural and personal.

We must build for scalability from the very beginning—enabling AIs to grow in complexity as the user's trust and ambitions grow. Takeaways about the impacts of Agentic AI design include the following:

- **Design for AI dabblers and beyond**: To build widespread adoption of agentic AI, future experiences must appeal to a wide range of consumers, not just power users. Create low-friction, exploratory user interfaces that welcome people with limited experience and teach them how best to derive value from agentic AI.
- **Work overtime to earn users' trust—from utility to emotions**: Even GenAI power users said that using AI does not yet feel natural to them. Agentic AI experiences require trust, and an AI must earn trust over time through genuine connection and reliable relationship building. Don't just optimize for utility. Explore AI that reflects and adapts to complex tones with a broad range of emotional expression, from casual to caring to commanding.

## Next Steps

Now that you understand agent architectures and orchestration, explore:

- [Memory & State](../memory-state/index.md) - How agents maintain context and state
- [RAG Systems](../rag/index.md) - How agents access and use knowledge
- [Prompting Basics](../prompting-structured-outputs/index.md) - How to design effective agent prompts
- [Evaluation & Observability](../../production-operations/evaluation-observability/index.md) - How to monitor and evaluate agent performance

## Sources

<Card title="Reference Materials">
  <ul>
    <li><strong>MCP Agent Framework:</strong> <a href="https://github.com/lastmile-ai/mcp-agent">https://github.com/lastmile-ai/mcp-agent</a></li>
    <li><strong>Prompt Engineering Guide - Agents:</strong> <a href="https://www.promptingguide.ai/agents">https://www.promptingguide.ai/agents</a></li>
    <li><strong>CrewAI Documentation:</strong> <a href="https://docs.crewai.com/">https://docs.crewai.com/</a></li>
    <li><strong>Anthropic Agent Building Guide:</strong> <a href="https://www.anthropic.com/engineering/building-effective-agents">https://www.anthropic.com/engineering/building-effective-agents</a></li>
    <li><strong>Model Context Protocol:</strong> <a href="https://modelcontextprotocol.io/">https://modelcontextprotocol.io/</a></li>
  </ul>
</Card>

