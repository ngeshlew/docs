---
title: "Multimodality in AI"
description: "Explore how AI systems process and understand multiple types of data simultaneously, from text and images to audio and video, enabling richer user experiences"
slug: "modules-multimodality"
updatedAt: "2025-08-19"
tags: [module, multimodality, multimodal, ai, text, image, audio, video, claude, anthropic]
---

# Multimodality in AI

<Callout type="info">
  **Learning Objective**: Understand how AI systems process multiple data types simultaneously and how to design interfaces that leverage multimodal capabilities for richer user experiences, with special focus on Claude's vision and multimodal capabilities.
</Callout>

## Overview

Multimodality in AI refers to systems that can understand, process, and generate multiple types of data simultaneously - including text, images, audio, and video. This capability enables more natural and intuitive interactions that mirror how humans perceive and communicate with the world.

<CardGroup cols={2}>
  <Card title="Natural Interaction" icon="users">
    Multimodal AI enables interactions that feel more natural and human-like, combining different senses and communication modes.
  </Card>
  <Card title="Richer Context" icon="layers">
    Multiple data types provide richer context and understanding, leading to more accurate and relevant responses.
  </Card>
</CardGroup>

## Claude's Multimodal Capabilities

<Callout type="info">
  **Claude's Vision**: Claude 3 models feature sophisticated vision capabilities that enable the model to understand and analyze images, charts, diagrams, and visual content alongside text.
</Callout>

### Claude's Vision Features

<Card title="Claude's Visual Understanding">
  <p>Claude can process and understand visual content in various formats:</p>
  
  <h4>Supported Image Types:</h4>
  <ul>
    <li><strong>Photographs:</strong> Real-world images, objects, scenes</li>
    <li><strong>Charts and Graphs:</strong> Data visualizations, analytics</li>
    <li><strong>Diagrams:</strong> Technical drawings, flowcharts, schematics</li>
    <li><strong>Documents:</strong> Screenshots, forms, handwritten text</li>
    <li><strong>Code:</strong> Screenshots of code, terminal output</li>
    <li><strong>UI/UX Elements:</strong> Interface mockups, wireframes</li>
  </ul>
  
  <h4>Visual Analysis Capabilities:</h4>
  <ul>
    <li><strong>Object Recognition:</strong> Identify objects, people, scenes</li>
    <li><strong>Text Extraction:</strong> Read text from images (OCR)</li>
    <li><strong>Data Interpretation:</strong> Analyze charts and graphs</li>
    <li><strong>Code Understanding:</strong> Read and analyze code screenshots</li>
    <li><strong>Document Processing:</strong> Extract information from forms and documents</li>
    <li><strong>Visual Reasoning:</strong> Understand spatial relationships and context</li>
  </ul>
</Card>

### Claude's Multimodal Processing

<Card title="Text + Vision Integration">
  <h4>Combined Processing:</h4>
  <ul>
    <li><strong>Contextual Understanding:</strong> Claude can reference images while discussing text</li>
    <li><strong>Cross-Modal Reasoning:</strong> Connect visual elements with textual concepts</li>
    <li><strong>Enhanced Responses:</strong> Provide more accurate and detailed answers</li>
    <li><strong>Visual Explanations:</strong> Explain visual content in natural language</li>
  </ul>
  
  <h4>Example Use Cases:</h4>
  <ul>
    <li><strong>Document Analysis:</strong> Extract and summarize information from complex documents</li>
    <li><strong>Data Visualization:</strong> Analyze charts and provide insights</li>
    <li><strong>Code Review:</strong> Review code screenshots and suggest improvements</li>
    <li><strong>Design Feedback:</strong> Analyze UI mockups and provide design suggestions</li>
    <li><strong>Educational Content:</strong> Explain diagrams and visual concepts</li>
  </ul>
</Card>

### Claude Vision Implementation

<Card title="Using Claude's Vision">
  <h4>Basic Implementation:</h4>
  <pre><code>import anthropic
import base64

client = anthropic.Anthropic(api_key="your-api-key")

def analyze_image_with_claude(image_path, question):
    """
    Analyze an image using Claude's vision capabilities
    """
    
    # Read and encode image
    with open(image_path, "rb") as image_file:
        image_data = base64.b64encode(image_file.read()).decode('utf-8')
    
    # Create message with image
    response = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=1000,
        messages=[
            \{\{
                "role": "user",
                "content": [
                    \{\{
                        "type": "image",
                        "source": \{\{
                            "type": "base64",
                            "media_type": "image/jpeg",
                            "data": image_data
                        \}\}
                    \}\},
                    \{\{
                        "type": "text",
                        "text": question
                    \}\}
                ]
            \}\}
        ]
    )
    
    return response.content[0].text

# Example usage
result = analyze_image_with_claude(
    "chart.png", 
    "What insights can you draw from this data visualization?"
)
print(result)
</code></pre>
</Card>

## Why It's Important for Designers to Know

### 1. **Enhanced User Experience**

<Card title="Multimodal UX Benefits">
  <ul>
    <li><strong>Natural Communication:</strong> Users can interact using multiple modalities simultaneously</li>
    <li><strong>Reduced Cognitive Load:</strong> Information can be presented in the most appropriate format</li>
    <li><strong>Accessibility:</strong> Multiple input/output options accommodate different user needs</li>
    <li><strong>Engagement:</strong> Rich, dynamic interactions increase user engagement</li>
    <li><strong>Efficiency:</strong> Faster and more accurate information exchange</li>
  </ul>
</Card>

### 2. **Design Opportunities**

<Callout type="warning">
  **Design Challenge**: Multimodal interfaces require careful consideration of how different modalities work together and when to use each one.
</Callout>

<Table>
  <TableHead>
    <TableRow>
      <TableHeader>Modality</TableHeader>
      <TableHeader>Strengths</TableHeader>
      <TableHeader>Design Considerations</TableHeader>
      <TableHeader>Best Use Cases</TableHeader>
    </TableRow>
  </TableHead>
  <TableBody>
    <TableRow>
      <TableCell><strong>Text</strong></TableCell>
      <TableCell>Precise, searchable, accessible</TableCell>
      <TableCell>Readability, character limits, language support</TableCell>
      <TableCell>Detailed information, instructions, search</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Images</strong></TableCell>
      <TableCell>Visual, immediate, emotional impact</TableCell>
      <TableCell>Resolution, loading times, alt text</TableCell>
      <TableCell>Visual content, demonstrations, branding</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Audio</strong></TableCell>
      <TableCell>Hands-free, natural, emotional</TableCell>
      <TableCell>Clarity, background noise, accessibility</TableCell>
      <TableCell>Voice commands, music, notifications</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Video</strong></TableCell>
      <TableCell>Dynamic, engaging, comprehensive</TableCell>
      <TableCell>Bandwidth, autoplay, mobile optimization</TableCell>
      <TableCell>Tutorials, entertainment, live content</TableCell>
    </TableRow>
  </TableBody>
</Table>

### 3. **Technical Understanding**

<Card title="Technical Implications">
  <ul>
    <li><strong>Model Complexity:</strong> Multimodal models are more complex and resource-intensive</li>
    <li><strong>Data Processing:</strong> Different modalities require different preprocessing</li>
    <li><strong>Integration Challenges:</strong> Coordinating multiple input/output streams</li>
    <li><strong>Performance Trade-offs:</strong> Balancing quality vs. speed vs. cost</li>
    <li><strong>Error Handling:</strong> Managing failures across different modalities</li>
  </ul>
</Card>

## Core Multimodal Concepts

### 1. **Modality Types and Characteristics**

<Card title="Understanding Modalities">
  <h4>Primary Modalities:</h4>
  <ul>
    <li><strong>Text:</strong> Written language, symbols, structured data</li>
    <li><strong>Visual:</strong> Images, videos, graphics, charts</li>
    <li><strong>Audio:</strong> Speech, music, sounds, voice</li>
    <li><strong>Temporal:</strong> Time-series data, sequences, events</li>
  </ul>
  
  <h4>Modality Characteristics:</h4>
  <ul>
    <li><strong>Dimensionality:</strong> Text is 1D, images are 2D, video is 3D (2D + time)</li>
    <li><strong>Information Density:</strong> Visual content often contains more information per unit</li>
    <li><strong>Processing Speed:</strong> Text is fastest to process, video is slowest</li>
    <li><strong>Storage Requirements:</strong> Visual and audio content require more storage</li>
  </ul>
</Card>

### 2. **Multimodal Fusion**

<Card title="Combining Modalities">
  <h4>Fusion Strategies:</h4>
  <ul>
    <li><strong>Early Fusion:</strong> Combine modalities at the input level</li>
    <li><strong>Late Fusion:</strong> Process each modality separately, then combine results</li>
    <li><strong>Intermediate Fusion:</strong> Combine at intermediate processing stages</li>
    <li><strong>Cross-Modal Attention:</strong> Use attention mechanisms to focus on relevant modalities</li>
  </ul>
  
  <h4>Claude's Approach:</h4>
  <ul>
    <li><strong>Unified Processing:</strong> Claude processes text and images together</li>
    <li><strong>Contextual Integration:</strong> Visual context enhances textual understanding</li>
    <li><strong>Cross-Reference Capability:</strong> Can reference visual elements in text responses</li>
    <li><strong>Seamless Interaction:</strong> Natural conversation about visual content</li>
  </ul>
</Card>

### 3. **Multimodal Understanding**

<Card title="Understanding Across Modalities">
  <h4>Key Capabilities:</h4>
  <ul>
    <li><strong>Cross-Modal Translation:</strong> Convert between modalities (text to image, etc.)</li>
    <li><strong>Modality Alignment:</strong> Understand relationships between different modalities</li>
    <li><strong>Context Preservation:</strong> Maintain context across modality switches</li>
    <li><strong>Semantic Understanding:</strong> Extract meaning regardless of modality</li>
  </ul>
  
  <h4>Claude's Strengths:</h4>
  <ul>
    <li><strong>Visual Comprehension:</strong> Deep understanding of image content</li>
    <li><strong>Text-Vision Alignment:</strong> Seamless integration of text and visual information</li>
    <li><strong>Contextual Reasoning:</strong> Can reason about visual content in context</li>
    <li><strong>Detailed Analysis:</strong> Provide comprehensive analysis of visual elements</li>
  </ul>
</Card>

## Claude-Specific Implementation

### 1. **Basic Vision Integration**

<Card title="Simple Image Analysis">
  <h4>Python Implementation:</h4>
  <pre><code>import anthropic
import base64
from PIL import Image
import io

def analyze_image_claude(image_path, prompt):
    """
    Analyze an image using Claude's vision capabilities
    """
    
    # Load and prepare image
    with Image.open(image_path) as img:
        # Convert to RGB if necessary
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # Save to bytes
        img_byte_arr = io.BytesIO()
        img.save(img_byte_arr, format='JPEG')
        img_byte_arr = img_byte_arr.getvalue()
        
        # Encode to base64
        image_data = base64.b64encode(img_byte_arr).decode('utf-8')
    
    # Create Claude client
    client = anthropic.Anthropic(api_key="your-api-key")
    
    # Send request
    response = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=2000,
        messages=[
            \{\{
                "role": "user",
                "content": [
                    \{\{
                        "type": "image",
                        "source": \{\{
                            "type": "base64",
                            "media_type": "image/jpeg",
                            "data": image_data
                        \}\}
                    \}\},
                    \{\{
                        "type": "text",
                        "text": prompt
                    \}\}
                ]
            \}\}
        ]
    )
    
    return response.content[0].text

# Example usage
result = analyze_image_claude(
    "dashboard.png",
    "Analyze this dashboard and provide insights about the data shown."
)
print(result)
</code></pre>
</Card>

### 2. **Advanced Multimodal Processing**

<Card title="Complex Multimodal Analysis">
  <h4>Multiple Images and Text:</h4>
  <pre><code>def multimodal_analysis_claude(images, text_prompt):
    """
    Analyze multiple images with text using Claude
    """
    
    client = anthropic.Anthropic(api_key="your-api-key")
    
    # Prepare content array
    content = []
    
    # Add images
    for image_path in images:
        with open(image_path, "rb") as img_file:
            image_data = base64.b64encode(img_file.read()).decode('utf-8')
        
        content.append(\{\{
            "type": "image",
            "source": \{\{
                "type": "base64",
                "media_type": "image/jpeg",
                "data": image_data
            \}\}
        \}\})
    
    # Add text prompt
    content.append(\{\{
        "type": "text",
        "text": text_prompt
    \}\})
    
    # Send request
    response = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=3000,
        messages=[
            \{\{
                "role": "user",
                "content": content
            \}\}
        ]
    )
    
    return response.content[0].text

# Example usage
images = ["chart1.png", "chart2.png", "dashboard.png"]
prompt = """
Compare these three visualizations and provide insights about:
1. Data trends across the charts
2. Key differences and similarities
3. Recommendations based on the data
"""

result = multimodal_analysis_claude(images, prompt)
print(result)
</code></pre>
</Card>

### 3. **JavaScript Implementation**

<Card title="JavaScript Vision Integration">
  <h4>Browser-based Implementation:</h4>
  <pre><code>import Anthropic from '@anthropic-ai/sdk';

async function analyzeImageWithClaude(imageFile, prompt) {
    const client = new Anthropic({
        apiKey: 'your-api-key',
    });
    
    // Convert image to base64
    const base64Image = await fileToBase64(imageFile);
    
    // Create content array
    const content = [
        {
            type: 'image',
            source: {
                type: 'base64',
                media_type: imageFile.type,
                data: base64Image
            }
        },
        {
            type: 'text',
            text: prompt
        }
    ];
    
    // Send request
    const response = await client.messages.create({
        model: 'claude-3-sonnet-20240229',
        max_tokens: 2000,
        messages: [
            {
                role: 'user',
                content: content
            }
        ]
    });
    
    return response.content[0].text;
}

// Helper function to convert file to base64
function fileToBase64(file) {
    return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.readAsDataURL(file);
        reader.onload = () => {
            const base64 = reader.result.split(',')[1];
            resolve(base64);
        };
        reader.onerror = error => reject(error);
    });
}

// Example usage with file input
document.getElementById('imageInput').addEventListener('change', async (event) => {
    const file = event.target.files[0];
    if (file) {
        const result = await analyzeImageWithClaude(
            file,
            "Describe what you see in this image and provide any relevant insights."
        );
        console.log(result);
    }
});
</code></pre>
</Card>

### 4. **LangChain Integration**

<Card title="LangChain Multimodal">
  <h4>LangChain with Claude Vision:</h4>
  <pre><code>from langchain_anthropic import ChatAnthropic
from langchain.schema import HumanMessage
import base64

class ClaudeVisionChain:
    def __init__(self, api_key: str):
        self.llm = ChatAnthropic(
            model="claude-3-sonnet-20240229",
            temperature=0.1
        )
    
    def analyze_image(self, image_path: str, prompt: str) -> str:
        """
        Analyze an image using Claude through LangChain
        """
        
        # Read and encode image
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Create message with image
        message = HumanMessage(
            content=[
                \{\{
                    "type": "image",
                    "source": \{\{
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data
                    \}\}
                \}\},
                \{\{
                    "type": "text",
                    "text": prompt
                \}\}
            ]
        )
        
        # Get response
        response = self.llm.invoke([message])
        return response.content

# Example usage
vision_chain = ClaudeVisionChain("your-api-key")
result = vision_chain.analyze_image(
    "screenshot.png",
    "Analyze this UI screenshot and provide feedback on the design and user experience."
)
print(result)
</code></pre>
</Card>

### 5. **CrewAI Multimodal Agents**

<Card title="Multimodal CrewAI Agents">
  <h4>Vision-Enabled Agents:</h4>
  <pre><code>from crewai import Agent, Task, Crew
from langchain_anthropic import ChatAnthropic
import base64

class MultimodalAgent:
    def __init__(self, api_key: str):
        self.llm = ChatAnthropic(
            model="claude-3-sonnet-20240229",
            temperature=0.1
        )
    
    def create_vision_agent(self):
        """
        Create an agent specialized in visual analysis
        """
        return Agent(
            role="Visual Analyst",
            goal="Analyze visual content and provide detailed insights",
            backstory="You are an expert at analyzing images, charts, diagrams, and visual content. You can understand complex visual information and provide detailed, accurate analysis.",
            llm=self.llm,
            verbose=True
        )
    
    def create_multimodal_task(self, image_path: str, analysis_prompt: str):
        """
        Create a task for multimodal analysis
        """
        
        # Read and encode image
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Create task description with image
        task_description = f"""
        Analyze the provided image and answer the following question:
        
        Question: {analysis_prompt}
        
        [Image data would be included here in the actual implementation]
        
        Provide a comprehensive analysis including:
        1. Visual elements identified
        2. Key insights and observations
        3. Recommendations or conclusions
        4. Any relevant context or background information
        """
        
        return Task(
            description=task_description,
            agent=self.create_vision_agent()
        )
    
    def analyze_with_crew(self, image_path: str, prompt: str):
        """
        Analyze image using CrewAI with vision capabilities
        """
        agent = self.create_vision_agent()
        task = self.create_multimodal_task(image_path, prompt)
        
        crew = Crew(
            agents=[agent],
            tasks=[task],
            verbose=True
        )
        
        result = crew.kickoff()
        return result

# Example usage
multimodal_agent = MultimodalAgent("your-api-key")
result = multimodal_agent.analyze_with_crew(
    "dashboard.png",
    "Analyze this dashboard and provide insights about the data visualization and user experience."
)
print(result)
</code></pre>
</Card>

## Real-World Applications

### 1. **Document Analysis**

<Card title="Document Processing with Claude">
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Form Processing:</strong> Extract data from forms and documents</li>
    <li><strong>Receipt Analysis:</strong> Automatically categorize and extract expense data</li>
    <li><strong>Contract Review:</strong> Analyze legal documents and identify key terms</li>
    <li><strong>Report Generation:</strong> Create summaries from complex documents</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  <pre><code>def analyze_document_claude(document_path, analysis_type):
    """
    Analyze documents using Claude's vision capabilities
    """
    
    prompts = {
        "form": "Extract all form fields and their values from this document.",
        "receipt": "Extract the vendor, date, items, and total amount from this receipt.",
        "contract": "Identify key terms, dates, parties, and obligations in this contract.",
        "report": "Summarize the main points and key findings from this report."
    }
    
    prompt = prompts.get(analysis_type, "Analyze this document and provide insights.")
    
    return analyze_image_with_claude(document_path, prompt)

# Example usage
form_data = analyze_document_claude("application_form.png", "form")
receipt_data = analyze_document_claude("receipt.jpg", "receipt")
</code></pre>
</Card>

### 2. **Data Visualization Analysis**

<Card title="Chart and Graph Analysis">
  <h4>Capabilities:</h4>
  <ul>
    <li><strong>Chart Interpretation:</strong> Understand various chart types and data</li>
    <li><strong>Trend Analysis:</strong> Identify patterns and trends in data</li>
    <li><strong>Insight Generation:</strong> Provide business insights from visualizations</li>
    <li><strong>Recommendation Creation:</strong> Suggest actions based on data</li>
  </ul>
  
  <h4>Example Implementation:</h4>
  <pre><code>def analyze_chart_claude(chart_path, business_context=""):
    """
    Analyze charts and provide business insights
    """
    
    prompt = f"""
    Analyze this chart and provide insights about:
    1. What the data shows
    2. Key trends and patterns
    3. Notable observations
    4. Business implications
    5. Recommendations
    
    Business Context: {business_context}
    """
    
    return analyze_image_with_claude(chart_path, prompt)

# Example usage
sales_insights = analyze_chart_claude(
    "sales_chart.png",
    "This is a quarterly sales performance chart for our e-commerce platform."
)
</code></pre>
</Card>

### 3. **UI/UX Design Analysis**

<Card title="Design Feedback with Claude">
  <h4>Design Analysis Capabilities:</h4>
  <ul>
    <li><strong>Layout Analysis:</strong> Evaluate visual hierarchy and organization</li>
    <li><strong>Usability Assessment:</strong> Identify potential usability issues</li>
    <li><strong>Accessibility Review:</strong> Check for accessibility concerns</li>
    <li><strong>Design Recommendations:</strong> Suggest improvements and alternatives</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def analyze_ui_design_claude(design_path, context=""):
    """
    Analyze UI/UX designs and provide feedback
    """
    
    prompt = f"""
    Analyze this UI/UX design and provide feedback on:
    
    1. Visual Design
       - Layout and visual hierarchy
       - Color scheme and typography
       - Brand consistency
    
    2. User Experience
       - Navigation and information architecture
       - User flow and interactions
       - Potential usability issues
    
    3. Accessibility
       - Color contrast and readability
       - Screen reader compatibility
       - Keyboard navigation
    
    4. Recommendations
       - Specific improvements
       - Best practices to follow
       - Alternative approaches
    
    Design Context: {context}
    """
    
    return analyze_image_with_claude(design_path, prompt)

# Example usage
design_feedback = analyze_ui_design_claude(
    "mobile_app_mockup.png",
    "This is a mobile app design for a food delivery service."
)
</code></pre>
</Card>

### 4. **Code Review and Analysis**

<Card title="Code Analysis with Claude">
  <h4>Code Review Capabilities:</h4>
  <ul>
    <li><strong>Code Reading:</strong> Understand code from screenshots</li>
    <li><strong>Bug Detection:</strong> Identify potential issues and bugs</li>
    <li><strong>Best Practices:</strong> Suggest improvements and optimizations</li>
    <li><strong>Documentation:</strong> Generate documentation from code</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def review_code_claude(code_screenshot_path, language=""):
    """
    Review code from screenshots using Claude
    """
    
    prompt = f"""
    Review this code and provide feedback on:
    
    1. Code Quality
       - Readability and structure
       - Naming conventions
       - Code organization
    
    2. Potential Issues
       - Bugs or errors
       - Performance concerns
       - Security vulnerabilities
    
    3. Best Practices
       - Language-specific recommendations
       - Design patterns
       - Optimization opportunities
    
    4. Suggestions
       - Specific improvements
       - Alternative approaches
       - Additional features
    
    Programming Language: {language}
    """
    
    return analyze_image_with_claude(code_screenshot_path, prompt)

# Example usage
code_review = review_code_claude(
    "python_code.png",
    "Python"
)
</code></pre>
</Card>

## Best Practices for Multimodal Design

### 1. **Modality Selection**

<Card title="Choosing the Right Modality">
  <h4>Selection Criteria:</h4>
  <ul>
    <li><strong>Information Type:</strong> Choose modality that best represents the information</li>
    <li><strong>User Context:</strong> Consider user environment and preferences</li>
    <li><strong>Accessibility:</strong> Ensure multiple modalities for accessibility</li>
    <li><strong>Performance:</strong> Balance quality with processing speed</li>
  </ul>
  
  <h4>Claude-Specific Considerations:</h4>
  <ul>
    <li><strong>Image Quality:</strong> Ensure images are clear and well-lit</li>
    <li><strong>Context Provision:</strong> Provide relevant text context with images</li>
    <li><strong>Specific Questions:</strong> Ask specific questions about visual content</li>
    <li><strong>Follow-up Analysis:</strong> Use Claude's responses to guide further analysis</li>
  </ul>
</Card>

### 2. **Integration Strategies**

<Card title="Effective Integration">
  <h4>Integration Approaches:</h4>
  <ul>
    <li><strong>Complementary Use:</strong> Use modalities to complement each other</li>
    <li><strong>Progressive Disclosure:</strong> Reveal information through multiple modalities</li>
    <li><strong>Redundancy:</strong> Provide key information in multiple formats</li>
    <li><strong>Context Switching:</strong> Allow users to switch between modalities</li>
  </ul>
  
  <h4>Claude Integration Tips:</h4>
  <ul>
    <li><strong>Clear Prompts:</strong> Provide specific, detailed prompts for visual analysis</li>
    <li><strong>Iterative Analysis:</strong> Use follow-up questions to dive deeper</li>
    <li><strong>Context Provision:</strong> Include relevant background information</li>
    <li><strong>Output Formatting:</strong> Structure responses for easy consumption</li>
  </ul>
</Card>

### 3. **Error Handling**

<Card title="Robust Error Handling">
  <h4>Error Scenarios:</h4>
  <ul>
    <li><strong>Image Processing Failures:</strong> Handle corrupted or unsupported images</li>
    <li><strong>API Limitations:</strong> Manage rate limits and quotas</li>
    <li><strong>Quality Issues:</strong> Handle low-quality or unclear images</li>
    <li><strong>Context Loss:</strong> Maintain context when modalities fail</li>
  </ul>
  
  <h4>Implementation Strategies:</h4>
  <ul>
    <li><strong>Fallback Mechanisms:</strong> Provide alternative analysis methods</li>
    <li><strong>Quality Validation:</strong> Check image quality before processing</li>
    <li><strong>Graceful Degradation:</strong> Continue with available modalities</li>
    <li><strong>User Feedback:</strong> Inform users of processing status and issues</li>
  </ul>
</Card>

## Evaluation and Testing

### 1. **Performance Metrics**

<Card title="Multimodal Performance">
  <h4>Key Metrics:</h4>
  <ul>
    <li><strong>Accuracy:</strong> Correctness of multimodal understanding</li>
    <li><strong>Latency:</strong> Processing time for different modalities</li>
    <li><strong>User Satisfaction:</strong> User feedback on multimodal interactions</li>
    <li><strong>Accessibility:</strong> Effectiveness for users with different needs</li>
  </ul>
  
  <h4>Claude-Specific Testing:</h4>
  <ul>
    <li><strong>Image Analysis Accuracy:</strong> Test with various image types</li>
    <li><strong>Text-Vision Alignment:</strong> Verify correct understanding of visual content</li>
    <li><strong>Response Quality:</strong> Evaluate depth and relevance of analysis</li>
    <li><strong>Processing Speed:</strong> Measure response times for different image sizes</li>
  </ul>
</Card>

### 2. **Testing Strategies**

<Card title="Comprehensive Testing">
  <h4>Testing Approaches:</h4>
  <ul>
    <li><strong>Unit Testing:</strong> Test individual modality processing</li>
    <li><strong>Integration Testing:</strong> Test multimodal fusion and coordination</li>
    <li><strong>User Testing:</strong> Test with real users and scenarios</li>
    <li><strong>Edge Case Testing:</strong> Test with unusual or challenging inputs</li>
  </ul>
  
  <h4>Test Data:</h4>
  <ul>
    <li><strong>Diverse Images:</strong> Test with various image types and qualities</li>
    <li><strong>Complex Scenarios:</strong> Test with multiple images and complex prompts</li>
    <li><strong>Error Conditions:</strong> Test with corrupted or invalid inputs</li>
    <li><strong>Performance Limits:</strong> Test with large images and long prompts</li>
  </ul>
</Card>

## Related Concepts

<CardGroup cols={2}>
  <Card title="Vision AI" icon="eye" href="../prompting-techniques/multimodal-cot">
    Computer vision and image understanding
  </Card>
  <Card title="Natural Language Processing" icon="message-circle" href="../prompting-techniques/chain-of-thought">
    Text processing and understanding
  </Card>
  <Card title="User Experience Design" icon="users" href="../collaboration-with-engineers">
    Designing for multimodal interactions
  </Card>
  <Card title="Accessibility" icon="heart" href="../safety-security">
    Ensuring inclusive multimodal experiences
  </Card>
  <Card title="Data Visualization" icon="bar-chart" href="../evaluation-observability">
    Creating effective visual representations
  </Card>
  <Card title="API Integration" icon="link" href="../prompting-techniques/react">
    Integrating multimodal services
  </Card>
</CardGroup>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Using Multimodal AI Models For Your Applications (Part 3)](https://www.smashingmagazine.com/2024/10/using-multimodal-ai-models-applications-part3/)  
> for internal educational use only (non-profit).

# Using Multimodal AI Models For Your Applications (Part 3)

In this third and final part of a three-part series, we're taking a more streamlined approach to an application that supports vision-language (VLM) and text-to-speech (TTS). This time, we'll use different models that are designed for all three modalities — images or videos, text, and audio (including speech-to-text) — in one model. These "any-to-any" models make things easier by allowing us to avoid switching between models.

Specifically, we'll focus on two powerful models: Reka and Gemini 1.5 Pro.

Both models take things to the next level compared to the tools we used earlier. They eliminate the need for separate speech recognition models, providing a unified solution for multimodal tasks. With this in mind, our goal in this article is to explore how Reka and Gemini simplify building advanced applications that handle images, text, and audio all at once.

## Overview Of Multimodal AI Models

The architecture of multimodal models has evolved to enable seamless handling of various inputs, including text, images, and audio, among others. Traditional models often require separate components for each modality, but recent advancements in "any-to-any" models like Next-GPT or 4M allow developers to build systems that process multiple modalities within a unified architecture.

Gato, for instance, utilizes a 1.2 billion parameter decoder-only transformer architecture with 24 layers, embedding sizes of 2048 and a hidden size of 8196 in its feed-forward layers. This structure is optimized for general tasks across various inputs, but it still relies on extensive task-specific fine-tuning.

GPT-4o, on the other hand, takes a different approach with training on multiple media types within a single architecture. This means it's a single model trained to handle a variety of inputs (e.g., text, images, code) without the need for separate systems for each. This training method allows for smoother task-switching and better generalization across tasks.

Similarly, CoDi employs a multistage training scheme to handle a linear number of tasks while supporting input-output combinations across different modalities. CoDi's architecture builds a shared multimodal space, enabling synchronized generation for intertwined modalities like video and audio, making it ideal for more dynamic multimedia tasks.

Most "any-to-any" models, including the ones we've discussed, rely on a few key concepts to handle different tasks and inputs smoothly:

- **Shared representation space**: These models convert different types of inputs — text, images, audio — into a common feature space. Text is encoded into vectors, images into feature maps, and audio into spectrograms or embeddings. This shared space allows the model to process various inputs in a unified way.
- **Attention mechanisms**: Attention layers help the model focus on the most relevant parts of each input, whether it's understanding the text, generating captions from images, or interpreting audio.
- **Cross-modal interaction**: In many models, inputs from one modality (e.g., text) can guide the generation or interpretation of another modality (e.g., images), allowing for more integrated and cohesive outputs.
- **Pre-training and fine-tuning**: Models are typically pre-trained on large datasets across different types of data and then fine-tuned for specific tasks, enhancing their performance in real-world applications.

## Reka Models

Reka is an AI research company that helps developers build powerful applications by offering models for a range of tasks. These tasks include generating text from videos and images, translating speech, and answering complex questions from long multimodal documents. Reka's models can even write and execute code, providing flexible, real-world solutions for developers.

These are the three main models Reka offers:

1. **Reka Core**: A 67-billion-parameter multimodal language model designed for complex tasks. It supports inputs like images, videos, and texts while excelling in advanced reasoning and coding.
2. **Reka Flash**: A faster model with a 21-billion-parameter, designed for flexibility and rapid performance in multimodal settings.
3. **Reka Edge (PDF)**: A smaller 7-billion-parameter model was built for on-device and low-latency applications, making it efficient for local use and local or latency-sensitive applications.

Reka's models can be fine-tuned and deployed securely, whether on the cloud, on-premises, or even on-device. Let's start by testing Reka's capabilities directly through its playground. This allows us to experiment with its multimodal features without writing any code, providing a hands-on way to see how the models handle various tasks, such as image and video comprehension.

Alright, we'll kick things off by uploading an image of a diagram outline of the PaliGemma architecture and ask Reka for a detailed explanation.

> Can you provide a more detailed explanation of this image?

![A detailed explanation by Reka of a diagram outline of the PaliGemma architecture](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Here's what we get from Reka Core:

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Integrating Image-To-Text And Text-To-Speech Models (Part 2)](https://www.smashingmagazine.com/2024/08/integrating-image-to-text-and-text-to-speech-models-part2/)  
> for internal educational use only (non-profit).

# Integrating Image-To-Text And Text-To-Speech Models (Part 2)

In Part 1 of this brief two-part series, we developed an application that turns images into audio descriptions using vision-language and text-to-speech models. We combined an image-to-text that analyses and understands images, generating description, with a text-to-speech model to create an audio description, helping people with sight challenges. We also discussed how to choose the right model to fit your needs.

Now, we are taking things a step further. Instead of just providing audio descriptions, we are building that can have interactive conversations about images or videos. This is known as Conversational AI — a technology that lets users talk to systems much like chatbots, virtual assistants, or agents.

While the first iteration of the app was great, the output still lacked some details. For example, if you upload an image of a dog, the description might be something like "a dog sitting on a rock in front of a pool," and the app might produce something close but miss additional details such as the dog's breed, the time of the day, or location.

![The interface for an app with an uploaded photo of a golden retriever puppy on the left and an audio extraction on the right represented by sound waves.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The aim here is simply to build a more advanced version of the previously built app so that it not only describes images but also provides more in-depth information and engages users in meaningful conversations about them.

We'll use LLaVA, a model that combines understanding images and conversational capabilities. After building our tool, we'll explore multimodal models that can handle images, videos, text, audio, and more, all at once to give you even more options and easiness for your applications.

## Visual Instruction Tuning and LLaVA

We are going to look at visual instruction tuning and the multimodal capabilities of LLaVA. We'll first explore how visual instruction tuning can enhance the large language models to understand and follow instructions that include visual information. After that, we'll dive into LLaVA, which brings its own set of tools for image and video processing.

### Visual Instruction Tuning

Visual instruction tuning is a technique that helps large language models (LLMs) understand and follow instructions based on visual inputs. This approach connects language and vision, enabling AI systems to understand and respond to human instructions that involve both text and images. For example, Visual IT enables a model to describe an image or answer questions about a scene in a photograph. This fine-tuning method makes the model more capable of handling these complex interactions effectively.

There's a new training approach called LLaVAR that has been developed, and you can think of it as a tool for handling tasks related to PDFs, invoices, and text-heavy images. It's pretty exciting, but we won't dive into that since it is outside the scope of the app we're making.

### Examples of Visual Instruction Tuning Datasets

To build good models, you need good data — rubbish in, rubbish out. So, here are two datasets that you might want to use to train or evaluate your multimodal models. Of course, you can always add your own datasets to the two I'm going to mention.

**Vision-CAIR**

- Instruction datasets: English;
- Multi-task: Datasets containing multiple tasks;
- Mixed dataset: Contains both human and machine-generated data.

![Vision-CAIR](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Vision-CAIR provides a high-quality, well-aligned image-text dataset created using conversations between two bots. This dataset was initially introduced in a paper titled "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models," and it provides more detailed image descriptions and can be used with predefined instruction templates for image-instruction-answer fine-tuning.

**LLaVA Visual Instruct 150K**

- Instruction datasets: English;
- Multi-task: Datasets containing multiple tasks;
- Mixed dataset: Contains both human and machine-generated data.

LLaVA Visual Instruct 150K is a set of GPT-generated multimodal instruction-following data. It is built for visual instruction tuning and aims to achieve GPT-4 level vision and language capabilities.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Integrating Image-To-Text And Text-To-Speech Models (Part 1)](https://www.smashingmagazine.com/2024/07/integrating-image-to-text-and-text-to-speech-models-part1/)  
> for internal educational use only (non-profit).

# Integrating Image-To-Text And Text-To-Speech Models (Part 1)

Audio descriptions involve narrating contextual visual information in images or videos, improving user experiences, especially for those who rely on audio cues.

At the core of audio description technology are two crucial components: the description and the audio. The description involves understanding and interpreting the visual content of an image or video, which includes details such as actions, settings, expressions, and any other relevant visual information. Meanwhile, the audio component converts these descriptions into spoken words that are clear, coherent, and natural-sounding.

So, here's something we can do: build an app that generates and announces audio descriptions. The app can integrate a pre-trained vision-language model to analyze image inputs, extract relevant information, and generate accurate descriptions. These descriptions are then converted into speech using text-to-speech technology, providing a seamless and engaging audio experience.

![Image audio captioning app. The application provides audio descriptions for images. A file upload field is displayed on the left, and a space for generated audio is displayed on the right.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/1-image-audio-captioning-app.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

By the end of this tutorial, you will gain a solid grasp of the components that are used to build audio description tools. We'll spend time discussing what VLM and TTS models are, as well as many examples of them and tooling for integrating them into your work.

When we finish, you will be ready to follow along with a second tutorial in which we level up and build a chatbot assistant that you can interact with to get more insights about your images or videos.

## Vision-Language Models: An Introduction

VLMs are a form of artificial intelligence that can understand and learn from visuals and linguistic modalities.

![Image illustrating four different tasks a vision-language model can handle, such as Visual QA, object localization in an image.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/2-four-tasks-vision-language-model-can-handle.jpg)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

They are trained on vast amounts of data that include images, videos, and text, allowing them to learn patterns and relationships between these modalities. In simple terms, a VLM can look at an image or video and generate a corresponding text description that accurately matches the visual content.

VLMs typically consist of three main components:

1. An image model that extracts meaningful visual information,
2. A text model that processes and understands natural language,
3. A fusion mechanism that combines the representations learned by the image and text models, enabling cross-modal interactions.

Generally speaking, the image model — also known as the vision encoder — extracts visual features from input images and maps them to the language model's input space, creating visual tokens. The text model then processes and understands natural language by generating text embeddings. Lastly, these visual and textual representations are combined through the fusion mechanism, allowing the model to integrate visual and textual information.

VLMs bring a new level of intelligence to applications by bridging visual and linguistic understanding. Here are some of the applications where VLMs shine:

- **Image captions**: VLMs can provide automatic descriptions that enrich user experiences, improve searchability, and even enhance visuals for vision impairments.
- **Visual answers to questions**: VLMs could be integrated into educational tools to help students learn more deeply by allowing them to ask questions about visuals they encounter in learning materials, such as complex diagrams and illustrations.
- **Document analysis**: VLMs can streamline document review processes, identifying critical information in contracts, reports, or patents much faster than reviewing them manually.
- **Image search**: VLMs could open up the ability to perform reverse image searches. For example, an e-commerce site might allow users to upload image files that are processed to identify similar products that are available for purchase.
- **Content moderation**: Social media platforms could benefit from VLMs by identifying and removing harmful or sensitive content automatically before publishing it.
- **Robotics**: In industrial settings, robots equipped with VLMs can perform quality control tasks by understanding visual cues and describing defects accurately.

This is merely an overview of what VLMs are and the pieces that come together to generate audio descriptions. To get a clearer idea of how VLMs work, let's look at a few real-world examples that leverage VLM processes.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Generating Real-Time Audio Sentiment Analysis With AI](https://www.smashingmagazine.com/2023/09/generating-real-time-audio-sentiment-analysis-ai/)  
> for internal educational use only (non-profit).

# Generating Real-Time Audio Sentiment Analysis With AI

In the previous article, we developed a sentiment analysis tool that could detect and score emotions hidden within audio files. We're taking it to the next level in this article by integrating real-time analysis and multilingual support. Imagine analyzing the sentiment of your audio content in real-time as the audio file is transcribed. In other words, the tool we are building offers immediate insights as an audio file plays.

So, how does it all come together? Meet Whisper and Gradio — the two resources that sit under the hood. Whisper is an advanced automatic speech recognition and language detection library. It swiftly converts audio files to text and identifies the language. Gradio is a UI framework that happens to be designed for interfaces that utilize machine learning, which is ultimately what we are doing in this article. With Gradio, you can create user-friendly interfaces without complex installations, configurations, or any machine learning experience — the perfect tool for a tutorial like this.

By the end of this article, we will have created a fully-functional app that:

- Records audio from the user's microphone,
- Transcribes the audio to plain text,
- Detects the language,
- Analyzes the emotional qualities of the text, and
- Assigns a score to the result.

Note: You can peek at the final product in the live demo.

## Automatic Speech Recognition And Whisper

Let's delve into the fascinating world of automatic speech recognition and its ability to analyze audio. In the process, we'll also introduce Whisper, an automated speech recognition tool developed by the OpenAI team behind ChatGPT and other emerging artificial intelligence technologies. Whisper has redefined the field of speech recognition with its innovative capabilities, and we'll closely examine its available features.

## Automatic Speech Recognition (ASR)

ASR technology is a key component for converting speech to text, making it a valuable tool in today's digital world. Its applications are vast and diverse, spanning various industries. ASR can efficiently and accurately transcribe audio files into plain text. It also powers voice assistants, enabling seamless interaction between humans and machines through spoken language. It's used in myriad ways, such as in call centers that automatically route calls and provide callers with self-service options.

By automating audio conversion to text, ASR significantly saves time and boosts productivity across multiple domains. Moreover, it opens up new avenues for data analysis and decision-making.

That said, ASR does have its fair share of challenges. For example, its accuracy is diminished when dealing with different accents, background noises, and speech variations — all of which require innovative solutions to ensure accurate and reliable transcription. The development of ASR systems capable of handling diverse audio sources, adapting to multiple languages, and maintaining exceptional accuracy is crucial for overcoming these obstacles.

## Whisper: A Speech Recognition Model

Whisper is a speech recognition model also developed by OpenAI. This powerful model excels at speech recognition and offers language identification and translation across multiple languages. It's an open-source model available in five different sizes, four of which have an English-only variant that performs exceptionally well for single-language tasks.

What sets Whisper apart is its robust ability to overcome ASR challenges. Whisper achieves near state-of-the-art performance and even supports zero-shot translation from various languages to English. Whisper has been trained on a large corpus of data that characterizes ASR's challenges. The training data consists of approximately 680,000 hours of multilingual and multitask supervised data collected from the web.

The model is available in multiple sizes. The following table outlines these model characteristics:

- `tiny.en`
- `base.en`
- `small.en`
- `medium.en`

For developers working with English-only applications, it's essential to consider the performance differences among the .en models — specifically, tiny.en and base.en, both of which offer better performance than the other models.

Whisper utilizes a Seq2seq (i.e., transformer encoder-decoder) architecture commonly employed in language-based models. This architecture's input consists of audio frames, typically 30-second segment pairs. The output is a sequence of the corresponding text. Its primary strength lies in transcribing audio into text, making it ideal for "audio-to-text" use cases.

![Diagram of Whisper's ASR architecture](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/generating-real-time-audio-sentiment-analysis-ai/1-ai-real-time-audio-analysis-whisper-architecure.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Using AI To Detect Sentiment In Audio Files](https://www.smashingmagazine.com/2023/06/ai-detect-sentiment-audio-files/)  
> for internal educational use only (non-profit).

# Using AI To Detect Sentiment In Audio Files

I don't know if you've ever used Grammarly's service for writing and editing content. But if you have, then you no doubt have seen the feature that detects the tone of your writing.

It's an extremely helpful tool! It can be hard to know how something you write might be perceived by others, and this can help affirm or correct you. Sure, it's some algorithm doing the work, and we know that not all AI-driven stuff is perfectly accurate. But as a gut check, it's really useful.

![Grammarly tone detector](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/ai-detect-sentiment-audio-files/ai-audio-grammarly.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Now imagine being able to do the same thing with audio files. How neat would it be to understand the underlying sentiments captured in audio recordings? Podcasters especially could stand to benefit from a tool like that, not to mention customer service teams and many other fields.

> An audio sentiment analysis has the potential to transform the way we interact with data.

That's what we are going to accomplish in this article.

![A screenshot of the audio sentiment analyzer built in this tutorial](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/ai-detect-sentiment-audio-files/ai-audio-ui.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The idea is fairly straightforward:

- Upload an audio file.
- Convert the content from speech to text.
- Generate a score that indicates the type of sentiment it communicates.

But how do we actually build an interface that does all that? I'm going to introduce you to three tools and show how they work together to create an audio sentiment analyzer.

## But First: Why Audio Sentiment Analysis?

By harnessing the capabilities of an audio sentiment analysis tool, developers and data professionals can uncover valuable insights from audio recordings, revolutionizing the way we interpret emotions and sentiments in the digital age. Customer service, for example, is crucial for businesses aiming to deliver personable experiences. We can surpass the limitations of text-based analysis to get a better idea of the feelings communicated by verbal exchanges in a variety of settings, including:

- **Call centers**: Call center agents can gain real-time insights into customer sentiment, enabling them to provide personalized and empathetic support.
- **Voice assistants**: Companies can improve their natural language processing algorithms to deliver more accurate responses to customer questions.
- **Surveys**: Organizations can gain valuable insights and understand customer satisfaction levels, identify areas of improvement, and make data-driven decisions to enhance overall customer experience.

And that is just the tip of the iceberg for one industry. Audio sentiment analysis offers valuable insights across various industries. Consider healthcare as another example. Audio analysis could enhance patient care and improve doctor-patient interactions. Healthcare providers can gain a deeper understanding of patient feedback, identify areas for improvement, and optimize the overall patient experience.

Market research is another area that could benefit from audio analysis. Researchers can leverage sentiments to gain valuable insights into a target audience's reactions that could be used in everything from competitor analyses to brand refreshes with the use of audio speech data from interviews, focus groups, or even social media interactions where audio is used.

I can also see audio analysis being used in the design process. Like, instead of asking stakeholders to write responses, how about asking them to record their verbal reactions and running those through an audio analysis tool? The possibilities are endless!

> **Note:** The following article is reproduced verbatim from  
> SingleStore Team, *SingleStore* (2025):  
> [Claude 3 Multimodal With LlamaIndex and SingleStore](https://www.singlestore.com/blog/claude-3-multimodal-with-llamaindex-and-singlestore/)  
> for internal educational use only (non-profit).

# Claude 3 Multimodal With LlamaIndex and SingleStore

### Claude 3

### Claude 3 multimodal tutorial

### Multimodal scenario using an image

### Index into a vector store

### Multimodal LLMs: Industry specific use cases

Large Language Models (LLMs) have been the talk of the town due to their various capabilities that can almost replace humans. While models from OpenAI and Google have made a significant impact, there is another contender for the throne: Anthropic.

Anthropic is an AI company that is building some amazing models to compete with popular models from major providers. Anthropic recently released Claude 3, a multimodal LLM that is getting attention in the AI market. We'll take a closer look at how the Claude 3 model works, and its multimodal capabilities.

What is a multimodal model?

We all agree that the LLMs have limitations in terms of creating/providing accurate information, which can be tracked through different techniques. But, what if we have models that take not just natural language as input, but also images and videos to provide accurate information for users? Isn't this a fantastic idea? Models that take more than one form of input and understand different modalities (text, image, audio, video, etc.) are known as multimodal models. Google, OpenAI and Anthropic each have their multimodal models with better capabilities already in the market.

OpenAI's GPT-4 Vision, Google's Gemini and Anthropic's Claude 3 series are some notable examples of multimodal models that are revolutionizing the AI industry.

Anthropic's Claude 3 is a family of three AI models: Claude 3 Opus, Sonnet and Haiku. These models have the multimodal capabilities that closely compete with GPT-4 and Gemini-ultra. These Claude 3 models have the right balance of cost, safety and performance.

The image displays a comparison chart of various LLMs' performance across different tasks, including math problem-solving, coding and knowledge-based question answering. The models include Claude variants, GPT-4, GPT-3.5 and Gemini, with their respective scores in percentages. As you can see, the Claude 3 models perform robustly across a range of tasks, often outperforming other AI models presented in the comparison.

Claude's Haiku is the most affordable — yet effective — model that can be used for real-time response generation in customer service use cases, content moderation, etc. with higher efficiency.

You can easily access Claude models from Anthropic's official website to see how it works for your queries. You can transcribe handwritten notes, understanding how objects are used and complex equations.

I tried an example by sharing an image I found online to see if the Claude model can respond accurately.

Amazing, that is an impressive response with all the minute details about the image shared.

We will be using the SingleStore database as our vector store, and SingleStore Notebooks (just like Jupyter notebooks) to run all our code.

Also we will be using LlamaIndex, an AI framework to build LLM-powered applications. It provides a complete toolkit for developers building AI applications by providing them with different APIs and the ability to integrate data sources, various file formats, databases to store vector data, applications, etc.

We will be importing the libraries required, running the multimodal model from Anthropic [claude-3-haiku-20240307], storing the data in the SingleStore and retrieving it to see the power of multimodality through text and image.

Let's explore the capabilities of the model on text and vision tasks. Get started by signing up for SingleStore. We are going to use SingleStore's Notebook feature to run our commands, and SingleStore as the database to store our data.

Once you sign up for the first time, you need to create a workspace and a database to store your data. Later in this tutorial, you will also see how you can store data as embeddings.

Creating a database is easy. Just go to your workspace and click Create Database as shown here.

Go back to the main SingleStore dashboard. Click on Develop to create a Notebook.

Next, click on New Notebook > New Notebook.

Create a new Notebook and name it whatever you'd like.

Now, get started with the notebook code shared here. Make sure to select your workspace and database you created.

Start adding all the code shown below step-by-step in the notebook you just created.

Install LlamaIndex and other libraries if required.

Python

Set API keys. Add both OpenAI and Anthropic API keys.

Python

Let's use the Claude 3's Haiku model for chat completion

Python

Let's download the image first

Python

Load the image

Python

Test the query on the image

Python

You can test many image examples

Python

Ask the model to describe the mentioned image

Python

In this section, we'll show you how to use Claude 3 to build a RAG pipeline over image data. We first use Claude to extract text from a set of images, then index the text with an embedding model. Finally, we build a query pipeline over the data.

Python

Testing the query

Python

The complete notebook code is here for your reference.

There are many use cases where multimodal plays an important role and enhances LLM applications — here are the most top of mind:

Multimodal models are revolutionizing the AI industry, and we will only see more powerful models in the future. While unimodal models have the limitations of processing only one type of input (mostly text), multimodal models gain an added advantage here by processing various input types (text, image, audio, video, etc). Multimodal models are paving the future for building LLM-powered applications for more context and high performance.

Start your free SingleStore trial today.

- Healthcare. Since multimodal models can process various input types, these can be highly effective in the healthcare industry. The models can take prescriptions, procedures,  X-rays and other imagery as input, combining both the text and image to come up with high quality responses and remedies.
- Customer support chatbots. Multimodal models can be highly effective as sometimes  text might not be sufficient in resolving an issue, so companies can ask for screenshots and images. This way, using a multimodal-powered application can enhance answering capabilities.
- Social media sentiment analysis. This can be cleverly done by multimodal-powered applications since they can identify images and text as well.
- eCommerce. Product search and recommendations can be simplified using multimodal models, since they get more contextual information from the user with many input capabilities.

> **Note:** The following article is reproduced verbatim from  
> Google Team, *Google Cloud* (2025):  
> [Get multimodal embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)  
> for internal educational use only (non-profit).

# Get multimodal embeddings

## Supported models

## Best practices

## API usage

### API limits

### Before you begin

### Java

To see an example of multimodal embeddings, run the "Intro to multimodal embeddings" notebook in one of the following environments:

The multimodal embeddings model generates 1408-dimension vectors based on the input you provide, which can include a combination of image, text, and video data. The embedding vectors can then be used for subsequent tasks like image classification or video content moderation.

The image embedding vector and text embedding vector are in the same semantic space with the same dimensionality. Consequently, these vectors can be used interchangeably for use cases like searching image by text, or searching video by image.

For text-only embedding use cases, we recommend using the Vertex AI text-embeddings API instead. For example, the text-embeddings API might be better for text-based semantic search, clustering, long-form document analysis, and other text retrieval or question-answering use cases. For more information, see Get text embeddings.

## Supported models

You can get multimodal embeddings by using the following model:

- **multimodalembedding**

## Best practices

Consider the following input aspects when using the multimodal embeddings model:

- **Text in images** - The model can distinguish text in images, similar to optical character recognition (OCR). If you need to distinguish between a description of the image content and the text within an image, consider using prompt engineering to specify your target content.
- **Image quality** - Higher quality images generally produce better embeddings
- **Content relevance** - Ensure the input content is relevant to your use case

## API usage

The multimodal embeddings API allows you to generate embeddings from various input types including images, text, and video data.

### API limits

- Maximum input size varies by modality
- Rate limiting applies based on your quota
- Concurrent request limits may apply

### Before you begin

1. Set up your Google Cloud project
2. Enable the Vertex AI API
3. Configure authentication
4. Install the required client libraries

### Implementation examples

The API supports multiple programming languages including Java, Python, and Node.js for generating multimodal embeddings.

> **Note:** The following article is reproduced verbatim from  
> Cohere Team, *Cohere* (2025):  
> [Multimodal LLMs explained: Different data sources, smarter AI](https://cohere.com/blog/multimodal-llm)  
> for internal educational use only (non-profit).

# Multimodal LLMs explained: Different data sources, smarter AI

Learn how multimodal LLMs combine text, images, and other diverse data sources to help improve AI performance across many different applications and industries.

![What Is a Multimodal LLM?](https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2FMultimodal-LLM_v2.png&w=1920&q=75)
<sub>Source: *Cohere*, Cohere Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> Neptune.ai Team, *Neptune.ai* (2025):  
> [Multimodal Large Language Models](https://neptune.ai/blog/multimodal-large-language-models)  
> for internal educational use only (non-profit).

# Multimodal Large Language Models

### TL;DR

## What is a multimodal large language model?

### Why do we need multimodal LLMs?

### How do multimodal LLMs work?

## Examples of multimodal LLMs

### Microsoft: Kosmos-1

#### Architecture and training

#### Performance

### DeepMind: Flamingo

### LLM Guardrails: Secure and Controllable Deployment

### LLaVA

### Google: PaLM-E

#### Architecture and training

#### Performance

## Challenges, limitations, and future directions of MLLMs

Multimodal Large Language Models (MLLMs) process data from different modalities like text, audio, image, and video.

Compared to text-only models, MLLMs achieve richer contextual understanding and can integrate information across modalities, unlocking new areas of application. Prime use cases of MLLMs include content creation, personalized recommendations, and human-machine interaction.

Examples of MLLMs that process image and text data include Microsoft's Kosmos-1, DeepMind's Flamingo, and the open-source LLaVA. Google's PaLM-E additionally handles information about a robot's state and surroundings.

Combining different modalities and dealing with different types of data comes with some challenges and limitations, such as alignment of heterogeneous data, inherited biases from pre-trained models, and lack of robustness.

How would you translate this sentence: "The glasses are broken." into French: "Les verres sont cases." or  "Les lunettes sont cases."? What if you have an image? Will you be able to choose the correct translation? As humans, we use different modalities daily to enhance communication. Machines can do the same.

While Large Language Models (LLMs) have shown impressive capabilities in understanding complex text, they are limited to a single data modality. However, many tasks span several modalities.

This article explores Multimodal Large Language Models, exploring their core functionalities, challenges, and potential for various machine-learning domains.

Let's break down the concept of Multimodal Large Language Models (MLLMs) by first understanding the terms "modal" and "multimodal:"

"Modal" refers to a particular way of communicating or perceiving information. It's like a channel through which we receive and express ourselves. Some of the common modalities are:

"Multimodal" refers to incorporating various modalities to create a richer understanding of the task, e.g., as on a website or in a blog post that integrates text with visuals.

MLLMs can process not just text but other modalities as well. They are trained on samples containing different modalities, which allows them to develop joint representations and utilize multimodal information to solve tasks.

Many industries heavily rely on multimodality, particularly those that handle a blend of data modalities. For example, MLLMs can be used in a healthcare setting to process patient reports comprising doctor notes (text), treatment plans (structured data), and X-rays or MRI scans (images).

MLLMs process and integrate information from different modalities (i.e., text, image, video, and audio), essential to solving many tasks. Some prominent applications are:

A typical multimodal LLM has three primary modules:

Kosmos-1 (GitHub) is a multimodal LLM created by Microsoft for natural language and perception-intensive tasks. It can perform visual dialogue, visual explanation, visual question answering, image captioning, math equations, OCR, and zero-shot image classification with and without descriptions.

> **Note:** The following article is reproduced verbatim from  
> Sebastian Raschka, *Sebastian Raschka* (2025):  
> [Understanding Multimodal LLMs](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms)  
> for internal educational use only (non-profit).

# Understanding Multimodal LLMs

### An introduction to the main techniques and latest models

## 1. Use cases of multimodal LLMs

## 2. Common approaches to building multimodal LLMs

### 2.1 Method A: Unified Embedding Decoder Architecture

#### 2.1.1 Understanding Image encoders

#### 2.1.2 The role of the linear projection module

#### 2.1.3 Image vs text tokenization

### 2.2 Method B: Cross-Modality Attention Architecture

## 3. Unified decoder and cross-attention model training

## 4. Recent multimodal models and methods

### 4.1 The Llama 3 Herd of Models

### 4.2 Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models

### 4.3 NVLM: Open Frontier-Class Multimodal LLMs

### 4.4 Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution

### 4.5 Pixtral 12B

### 4.6 MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning

### 4.7 Aria: An Open Multimodal Native Mixture-of-Experts Model

### 4.8 Baichuan-Omni

### 4.9 Emu3: Next-Token Prediction is All You Need

### 4.10 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation

## Conclusion

It was a wild two months. There have once again been many developments in AI research, with two Nobel Prizes awarded to AI and several interesting research papers published.

Among others, Meta AI released their latest Llama 3.2 models, which include open-weight versions for the 1B and 3B large language models and two multimodal models.

In this article, I aim to explain how multimodal LLMs function. Additionally, I will review and summarize roughly a dozen other recent multimodal papers and models published in recent weeks (including Llama 3.2) to compare their approaches.

What are multimodal LLMs? As hinted at in the introduction, multimodal LLMs are large language models capable of processing multiple types of inputs, where each "modality" refers to a specific type of data—such as text (like in traditional LLMs), sound, images, videos, and more. For simplicity, we will primarily focus on the image modality alongside text inputs.

A classic and intuitive application of multimodal LLMs is image captioning: you provide an input image, and the model generates a description of the image, as shown in the figure below.

Of course, there are many other use cases. For example, one of my favorites is extracting information from a PDF table and converting it into LaTeX or Markdown.

There are two main approaches to building multimodal LLMs:

- **Method A: Unified Embedding Decoder Architecture approach**
- **Method B: Cross-modality Attention Architecture approach**

As shown in the figure above, the Unified Embedding-Decoder Architecture utilizes a single decoder model, much like an unmodified LLM architecture such as GPT-2 or Llama 3.2. In this approach, images are converted into tokens with the same embedding size as the original text tokens, allowing the LLM to process both text and image input tokens together after concatenation.

The Cross-Modality Attention Architecture employs a cross-attention mechanism to integrate image and text embeddings directly within the attention layer.

In the following sections, we will explore how these methods work on a conceptual level. Then, we will look at recent research papers on multimodal LLMs to see how they are applied in practice.

Let's begin with the unified embedding decoder architecture, illustrated again in the figure below.

In the unified embedding-decoder architecture, an image is converted into embedding vectors, similar to how input text is converted into embeddings in a standard text-only LLM.

For a typical text-only LLM that processes text, the text input is usually tokenized (e.g., using Byte-Pair Encoding) and then passed through an embedding layer, as shown in the figure below.

Analogous to the tokenization and embedding of text, image embeddings are generated using an image encoder module (instead of a tokenizer), as shown in the figure below.

> **Note:** The following article is reproduced verbatim from  
> Giancarlo Mori, *Giancarlo Mori* (2025):  
> [The Future of AI is Multimodal](https://giancarlomori.substack.com/p/the-future-of-ai-is-multimodal)  
> for internal educational use only (non-profit).

# The Future of AI is Multimodal

### Multimodal AI brings machines a step closer to mirroring the intricacies of human intelligence.

## Decoding Multimodal AI: Concept and Components

### Unimodal vs. Multimodal AI

### Key Components of Multimodal AI

## The Superior Edge of Multimodal AI

## Case Studies: Multimodal AI in Action

## Embracing a Multimodal Future

From AI's early days of simple, rule-based algorithms, it has journeyed through remarkable advancements, each milestone bringing it a step closer to mirroring the intricacies of human intelligence. A significant leap in this journey is the emergence of multimodal AI - a paradigm shift from the traditional unimodal systems that once dominated the field.

Taking a step back, let's start with a definition of "multimodal AI": Multi-modal AI refers to artificial intelligence systems that can process and interpret data from multiple different modes or types of input, such as text, images, audio, and video. These systems can analyze and understand the information from these diverse sources simultaneously, enabling more comprehensive and nuanced decision-making and interactions. For example, a multi-modal AI could analyze a video by understanding the spoken words (audio), recognizing objects and actions (visual), and interpreting the text in subtitles or captions (textual), all at the same time.

Unlike its predecessors, which relied on single data input types like text or images, multimodal AI represents a more holistic approach to machine learning. By integrating multiple types of data inputs – text, images, and sounds – it offers a richer, more nuanced understanding of the world. This integration mirrors the human ability to process and interpret diverse sensory information simultaneously, marking a crucial step towards developing AI systems that can interact with and understand the world in a more human-like manner.

The rise of multimodal AI is not just a technical improvement; it's a transformation that paves the way for more sophisticated, context-aware, and intuitive AI systems. These advancements hold the promise of revolutionizing various sectors, from healthcare and education to customer service and security, by providing solutions that are more aligned with the complexities of human communication and understanding.

To fully grasp the essence of multimodal AI, it's essential to first understand what sets it apart from traditional unimodal AI systems and the key components that constitute its framework.

Unimodal AI systems, the initial stepping stones in AI development, are designed to understand and process information from a single type of data input, be it textual, visual, or auditory. For instance, a text-based AI excels in processing and generating language, while an image recognition AI specializes in analyzing visual data. However, their scope is limited to their specific domain of data, restricting their understanding and application to scenarios that require integration of multiple forms of data.

Multimodal AI, in contrast, breaks through these limitations by simultaneously processing and interpreting diverse types of data inputs. This approach mirrors the human cognitive process, where multiple senses are utilized to gain a comprehensive understanding of our surroundings. In essence, multimodal AI systems can integrate text, images, sounds, and potentially other sensory data, providing a more holistic and nuanced view of their input.

The sophisticated functionality of multimodal AI systems is underpinned by three primary components:

- **Input Module**: This is where the multimodal AI receives and processes different types of data. In this stage, each data type, whether it's text, image, or audio, is processed using specialized sub-models tailored for each modality.

- **Fusion Module**: The crux of multimodal AI lies in its ability to fuse the processed data from various modalities. This fusion can happen early in the process (early fusion), integrating raw data, or at a later stage (late fusion), combining the outputs of each sub-model. The fusion process is crucial as it synthesizes the diverse information into a unified representation that the AI can analyze and interpret.

- **Output Module**: Based on the fused data, the AI system generates outputs that are more context-aware and accurate. The output could be in various forms – a decision, a prediction, a synthesized image or text, or even a combination of these, depending on the application and the input data.

One of the most profound advantages of multimodal AI is its enhanced capability to interpret context with a depth and complexity that unimodal systems cannot achieve. By processing and analyzing multiple data types simultaneously, these systems can discern subtleties and nuances in information, leading to a more comprehensive understanding of their environment. For example, in a social media setting, while a unimodal text-based AI might interpret a post's sentiment based on language alone, a multimodal system can enrich this understanding by analyzing the accompanying images and audio tones. This holistic approach enables the AI to capture the post's true intent, whether it's irony, sarcasm, or genuine sentiment, which might otherwise be missed by a text-only analysis.

The integration of diverse data types in multimodal AI is not just about gathering more information; it's about creating a synergistic effect that leads to more informed, accurate, and nuanced decision-making. In critical sectors like healthcare, this ability can have life-altering implications. Consider a multimodal diagnostic system that evaluates a patient's condition by integrating data from various sources – medical imaging provides visual insights, lab results offer quantitative analysis, and patient history adds contextual depth. This comprehensive approach leads to diagnoses that are not only more accurate but also tailored to the individual patient, significantly improving treatment outcomes.

Another key benefit of multimodal AI is its adaptability to complex, real-world environments. Traditional unimodal systems often struggle in situations where data is incomplete, ambiguous, or noisy. Multimodal AI, by contrast, can leverage its multi-sensory data processing to fill in the gaps, make sense of ambiguity, and filter out noise. For instance, in an autonomous vehicle, a unimodal vision-based system might be hindered by poor visibility conditions. However, a multimodal system that combines visual data with radar and audio signals can navigate more reliably, ensuring safety even in challenging conditions.

Furthermore, multimodal AI is bridging the gap between human-AI interactions. By processing language, visual cues, and even emotional tones, these systems can engage with users in a way that feels more natural and human-like. This capability is transforming customer service bots, virtual assistants, and interactive educational tools into entities that understand and respond not just to what we say, but also to how we say it and what we show.

To illustrate the power of multimodal AI, let's explore a few examples:

- **Healthcare Diagnostics**: Multimodal AI systems are revolutionizing diagnostics by integrating patient history, imaging, and genomic data, leading to more accurate and personalized treatment plans.

- **Retail Personalization**: In retail, multimodal AI analyzes customer behavior, preferences, and feedback across various channels – textual reviews, visual content, and browsing patterns – to tailor product recommendations and enhance the shopping experience.

- **Customer Service Bots**: AI-powered customer service agents now respond to queries not just by analyzing textual data but also by interpreting the customer's tone and sentiment through voice or even facial expressions, offering a more empathetic and effective response.

- **Advanced Educational Tools**: In the field of education, multimodal AI is bringing about a revolution in personalized learning experiences. By integrating textual, visual, and auditory data, these systems can adapt to different learning styles.

- **Enhanced Security and Surveillance**: Security systems powered by multimodal AI are changing the landscape of public safety and private security. These systems can analyze video footage, audio recordings, and sensor data to detect potential threats with greater accuracy.

- **Smart Home Integration**: In the realm of smart homes, multimodal AI is enabling more intuitive and responsive interactions between users and their home environments. By processing voice commands, recognizing facial expressions, and understanding physical gestures, these AI systems can control home appliances, adjust environmental settings, and even detect unusual behaviors or emergencies.

- **Automotive Innovations**: The automotive industry is leveraging multimodal AI to enhance the driving experience and safety features in vehicles. Modern cars equipped with AI can process visual data from cameras, audio signals, and sensor information to provide advanced driver assistance, hazard warnings, and even fully autonomous driving capabilities.

- **Agricultural Optimization**: Multimodal AI is also making significant strides in agriculture. By combining data from satellite images, soil sensors, and weather forecasts, these systems can offer farmers insights into crop health, soil quality, and optimal harvesting times.

- **Financial Services and Fraud Detection**: In financial services, multimodal AI is being used for enhanced risk assessment and fraud detection. By analyzing transaction data, customer communication, and behavioral patterns, these AI systems can identify fraudulent activities and assess credit risks with higher precision.

- **Creative Industries and Media**: In creative industries and media, multimodal AI is transforming content creation and curation. AI systems can analyze text, images, and user preferences to generate personalized content recommendations, assist in digital art creation, and even help in writing scripts and composing music.

As we stand on the brink of a new era in artificial intelligence, the promise of multimodal AI is not just in its advanced technology, but in its potential to fundamentally transform our interaction with machines and, by extension, with the world around us. The journey ahead for multimodal AI is teeming with possibilities, challenges, and the promise of a more interconnected and intuitive future.

The development and application of multimodal AI are set to usher in a new age of technological sophistication. We are likely to witness AI systems that not only understand and process complex data from multiple sources but also interact with us in ways that are profoundly human-like. This evolution will bring about a paradigm shift in numerous fields, from healthcare and education to entertainment and security, enhancing both the efficiency and quality of services.

The broader impact of multimodal AI on society is immense. It holds the potential to solve some of the most pressing challenges we face today, be it in advancing healthcare, personalizing education, or contributing to environmental sustainability. However, this transformative power comes with a significant responsibility to ensure that the development of AI is guided by ethical principles and a commitment to the greater good. It is imperative to navigate the challenges of privacy, bias, and ethical use to maintain trust and acceptance of AI technologies.

The future of multimodal AI is a canvas of limitless possibilities. It invites us to reimagine the role of AI in our lives, not as a mere tool, but as a partner in shaping a more intelligent, empathetic, and connected world. The journey ahead is as exciting as it is crucial, and it calls for a collective effort to harness the power of AI in a way that enriches our lives and uplifts our societies.

> **Note:** The following article is reproduced verbatim from  
> Datalife360 Team, *Datalife360* (2025):  
> [What is Multimodal AI exactly?](https://datalife360.substack.com/p/what-is-multimodal-ai-exactly)  
> for internal educational use only (non-profit).

# What is Multimodal AI exactly?

### An overview for the beginners

## So how we define Multimodal AI?

## What sort of responses (outputs) can multimodal AI have?

## What are the (Potential) Ways Can Multimodal AI is Used?

## What are the Strengths and Challenges of Multimodal AI?

## Final Thoughts

Recently, I've been seeing more posts on multimodal AI recently - its becoming a popular topic. GPT4 has integrated it into its interface, while Hugging Face is increasingly releasing more multimodal models.

Multimodal holds great potential and fascination for a simple reason. It's a version of how we humans perceive, remember, and think about the world around us.

Our experience of the world is multimodal – we see objects, hear sounds, feel textures, smell scents, and taste flavors, and then we make decisions.

Before we dive into what is multimodal AI, let's decode what 'modality' means.

Humans are a blend of various sensory inputs – we're essentially multimodal creatures. We've got touch, sight, hearing, smell – all different channels of absorbing information.

You can refer to each source or form of information as a modality. Various mediums can convey information. These include speech, video, text, and so on. There are also various sensors like radar, infrared, accelerometers, and more. Each sensor represents a modality.

Modality has a pretty broad definition. For example, we can actually consider two different languages as two modalities. We can see even datasets collected under different conditions as two modalities.

Let's break it down the types of AI modalities.

**Unimodal AI: A single source** - Unimodal AI is akin to an expert in a singular discipline. It's designed to process one type of data input — be it text, image, or sound. Basic ML models are usually unimodal.

For instance, a unimodal text-based AI uses natural language processing (NLP). It analyzes and interprets textual data. But, it lacks the capability to process visual or auditory information. In its domain, it operates with high efficiency, but its context has limitations.

**Bimodal AI: two sources** - Bimodal AI combines two different types of data modalities. It fuses two or more distinct data modalities (inputs) to do inference. The term "bimodal" comes from combining two distinct inputs into one system.

This requires a fusion module. It means merging features after they have been preprocessed independently. It's akin to cooking components separately and then combining them.

In an image captioning application, a bimodal AI uses computer vision to interpret the image. For example, it does this by using computer vision. It also uses NLP to generate relevant textual descriptions. This creates a more comprehensive data interpretation.

**Multimodal AI: A Multi-Data Type Convergence** - Multimodal AI uses different data types (text, images, audio) to make predictions. It also extracts features. Each modality adds unique context. This involves more than one fusion module that handle and interpret preprocessed data.

Think of it like making a smoothie. Each ingredient (input) has its own flavor (data type), which must be cleaned before use.

They then go through a blender (deep learning module) to extract their predictions (inference). With fusion, the different flavors combine to create a delicious smoothie (final predictions). This fusion leverages the strengths of different data types for better prediction or generative tasks.

When we discuss data from different modalities, it's important to note that, while they each have unique characteristics (e.g., text data is discrete, while image and video data are continuous), there is also a certain level of association between them.

For instance, a photo might have text associated with it (like a caption or tags), which provides context or description. Similarly, in a video, the visual elements (what you see) are often closely associated with the audio elements (what you hear).

These relationships and associations allow multimodal models to interpret complex data holistically. Modeling relationships and labeling is extremely crucial for business and teams building these models. Not understanding the relationship between modalities and good labeling will result in failure. It may be even better just to use a simple unimodal model until the data and relationships are understood.

Understanding these is important if you want predictions and generative outputs.

Data quality is important for modalities. But you need to determine the output of the multimodal model. Not all outputs of a multimodal model can be text - it can text, images, audio, etc. In more advanced cases it can even be a generative output.

**Single Modal Output**: This is a straightforward type of output. It's an output in single modality and may have one or more modalities (image, text, audio) as inputs. For example, if you ask a text-based AI a question, the output will be a text response. In the case of an video model? Modal inputs may be the image and audio data, while the output might be text summarizing the video.

**Multimodal Output**: In this case, the AI can produce outputs of different modalities. For example, and model might take a text input and generate an image and a text summary. Or it might analyze a video (which combines visual and audio data) and provide a text summary, illustrated picture, and an audio summary. These may even be the same data type, but different modalities - a text summary and a text description for example.

**Generative Output**: Multimodal models can also be use generative models. It is quite possible for them create new content or data, which could be in any modality. For instance, a generative AI could write a new piece of music, create a realistic image, or even generate a webpage. This output is 'generative' because it's not just analyzing or categorizing existing data but actually creating something new. This can be in a single modality (like text or images) or multimodal (like a video with both visual and audio components).

Data science teams and business must be clear what outputs they need before they build. The desired output can drastically alter any data or AI product requirements and development time.

The modalities you put into model, and the output you want matter. Otherwise, you risk building a very expensive paperweight.

We've covered what modality is and ways models can respond. But how do we apply this to use cases? The list of examples here could fill multiple articles, so let's boil it down to a few examples.

**Ecommerce** - Multimodal AI can analyze customer interactions, sale history, and preferences. Based on this data, it can offer personalized product recommendations. This includes visual recommendations based on the style or color preferences seen in images or videos that the customer has interacted with. It holds great potential, since recommendation systems can be truly personalized.

**Searching Using Video** - Google's video-to-text research enhances accessibility. It converts video content to text. This benefits those with hearing impairments or a preference for reading. It provides richer data extraction by analyzing both spoken words and visual cues. This improves the searchability and organization of video content. It makes it easier to locate specific information. It aids in content localization and translation. This expands the reach to diverse linguistic and cultural audiences.

> **Note:** The following article is reproduced verbatim from  
> IBM Team, *IBM* (2025):  
> [What is a multimodal LLM (MLLM)?](https://www.ibm.com/think/topics/multimodal-llm)  
> for internal educational use only (non-profit).

# What is a multimodal LLM (MLLM)?

Multimodal large language models (MLLMs) work by encoding, aligning and fusing data from multiple modalities like text, images and audio into a shared representation for cross-modal understanding and reasoning.

In recent years, AI models like GPT and Gemini have transformed how we interact with artificial intelligence through natural language. But human communication isn't limited to just words. We understand the world through images, sounds, gestures and more. This space is where multimodal AI comes in. These state-of-the-art multimodal large language models (MLLMs) can process and reason across multiple types of data or modalities such as text, images and audio. This ability allows them to describe images, answer questions about videos, interpret charts, perform optical character recognition (OCR) tasks or even engage in real-time conversations that involve vision and speech.

Each modality has its own structure and requires different ways to represent and interpret information. For example, text is a sequence of words, an image is a grid of pixels and audio is a continuous waveform or spectrogram. Combining multiple modalities in a single AI system is powerful because it mirrors how humans understand the world. We don't look at a picture without context, we describe it with words, relate it to sounds or connect it to actions. When AI systems combine these different streams of information, they gain richer context and better reasoning skills. This capability makes it possible to describe images in natural language, answer questions about videos or follow text instructions with visual input. A multimodal approach pushes AI beyond unimodal text-only chat, helping machines see, listen and communicate more like we do.

## How Multimodal LLMs Work

The first step in any multimodal language model is to convert raw input data from different sources into machine-understandable features. Each type of data: text, visual data (image, videos and more), audio or sensor data has its own unique structure and requires a dedicated encoder to capture its meaning.

Because machines operate in binary, various techniques are used to translate multimedia content into a format that computers can process and understand, as outlined below. For text, tokenization breaks sentences into smaller units that are then embedded by using pretrained models like BERT (bidirectional encoder representations from transformers) or other language understanding transformer-based encoders. This approach produces dense vector representations that capture semantic information. For image inputs, advanced architectures such as vision transformers (ViT) or convolutional neural networks (CNNs) extract visual features such as shapes, colors and spatial patterns. This method is used in BLIP-2, which combines a ViT-based image encoder with a Q-Former to link vision and language. For audio, specialized encoders like wav2vec or HuBERT process raw waveforms to produce representations of speech or sound cues, as seen in models designed for audio-visual tasks like VideoCoCa. This modular approach ensures that each modality's encoder is tailored to preserve the critical information needed for the next stages.

Once the model has encoded each type of input such as breaking down text into semantic word embeddings or analyzing an image for shapes and objects, it produces high-level features. These features are patterns or summaries that capture the key meaning or structure of the original data. The next step is to align these features by mapping them into a shared space so they can interact meaningfully across modalities.

This is done through a projection step, where the abstract features from each encoder are mapped into a shared embedding space. An embedding space is a common numerical representation where text, image or audio features are converted into vectors that the model can compare and combine meaningfully. Projection is typically done by using linear transformations, learned projection heads or small neural layers. This step reshapes each modality's feature vector into a compatible size and format. Thus, the model ensures that features from text, images and audio can interact meaningfully when they are fused.

For example, frozen uses a visual encoder to transform images into embeddings, which are then concatenated with text embeddings before feeding into a frozen LLM for in-context multimodal learning. This model uses the combined inputs directly, without extra training, to generate answers or predictions based on the new context. Similarly, LLaMA-Adapter uses lightweight adapter modules to project image recognition features from a frozen encoder and integrate them with the language model without retraining the whole system. Projection ensures that features from text, images and audio can interact meaningfully when fused.

Once features from each modality are projected into a common or compatible space, the model combines them to form a unified multimodal representation. This step can be done by using simple strategies like concatenation, which stacks feature vectors side by side. More sophisticated approaches involve learned interactions between modalities, for example, through attention mechanisms. Cross-attention allows one modality (such as text) to selectively focus on relevant parts of another (such as an image), helping the model dynamically align and integrate information. In modern multimodal models, such mechanisms are central to representation learning, not just a final fusion step.

For instance, Flamingo and BLIP-2 employ cross-attention to align descriptive words with objects in an image. Some models use hierarchical fusion, merging features in stages. ALLaVA, for example, goes further with graph-based fusion, constructing explicit relation graphs to represent structured cross-modal links. This shared semantic space allows the model to reason across modalities for a comprehensive understanding.

Once fused, the combined features need to be refined and deeply processed to capture subtle cross-modal dependencies. For example, the layers in a transformer play a crucial role here, stacking self-attention and feedforward operations to model complex relationships. Self-attention helps refine context within the same modality, for instance, understanding the relationship between words in a sentence. Cross-attention goes a step further by allowing elements from one modality, such as text tokens, to directly interact with elements from another, like image regions. This mechanism enables the model to answer questions about an image, generate a caption or relate audio cues to visual scenes. LXMERT uses cross-attention for visual question answering (VQA) by aligning objects in an image with language tokens. VideoCoCa takes this a step further by connecting visual frames with spoken or written text, making it better at understanding videos.

After the fused features are processed, the model must produce an output that solves a specific task handled by an output decoder. For tasks like image captioning or video description, the decoder generates coherent text that describes visual or audio inputs. For example, MiniGPT-4 can create captions and follow instructions by turning combined features into natural, easy-to-understand text. Visual ChatGPT, built on OpenAI technology, uses an integrated prompt manager and multiple computer vision models. It can handle complex multistep tasks such as describing images, answering visual questions and generating new visuals from text instructions. For classification tasks, like emotion recognition or object detection, decoders map multimodal features to labels or decisions, ensuring the model's understanding is delivered in a usable format.

Behind the scenes, the power of multimodal models comes from large-scale pretraining and task-specific prompting. These systems are trained on huge paired datasets such as image-text pairs like in CLIP, video transcripts or audio-text pairs. CLIP famously uses contrastive learning to align images and captions, while LLaVA and MiniGPT-4 leverage synthetic instruction-following datasets generated by using GPT-4 to expand their understanding of how language and vision relate. Pretraining with tasks like masked modeling and contrastive matching builds broad cross-modal knowledge, while prompting and fine-tuning adapt these general skills to specific applications with little extra data. This method makes MLLMs capable of impressive zero-shot and few-shot performance, like describing images they have never seen or generating visuals from text.

This full pipeline of encoding, projecting, fusing, processing, decoding and pretraining is what enables modern multimodal capabilities to understand and generate rich outputs. By bringing multiple forms of information together, MLLMs bridge the gap between how machines and humans perceive the world.

## Real-World Applications

A practical use case of MLLMs in action is the CONCH model (contrastive learning from captions for histopathology) in healthcare. CONCH is a vision-language model trained on a large, domain-specific dataset to analyze medical slides, including special stains like immunohistochemistry. By using a ChatGPT-like interface, CONCH can match pathology images with diagnostic text prompts in a zero-shot setting, This helps pathologists retrieve relevant information for conditions such as invasive carcinoma or colitis without relying on massive general datasets.

GITMol is another example of an advanced MLLM designed to handle complex molecular data by integrating various modalities such as text descriptions, molecular images and graphs representing molecular structures. This model can perform tasks like predicting chemical reactions, recognizing compound names and providing insights into molecular properties. It combines textual and visual information and facilitates a deeper understanding of molecular interactions, assists in drug discovery and accelerates research in the chemical and biological sciences. Its ability to process multimodal data allows researchers to make more accurate predictions about molecular behavior and interactions, which is crucial in developing new therapeutics and understanding biological mechanisms.

## Training Multimodal LLMs

Training multimodal LLMs involves carefully designing data collection processes, choosing suitable training strategies and ensuring data quality to develop models capable of understanding and reasoning across multiple modalities.

Gathering instruction data for multimodal learning is complex and costly due to the diverse formats and tasks involved. Three main methods help scale this efficiently:

- **Data adaptation**: Existing high-quality datasets, such as VQA (images with questions and answers) or image-caption corpora like COCO (common objects in context dataset), are transformed into instruction-response formats. This step is done manually or semiautomatically, for example, by using language models like GPT to expand short answers into richer instructions.
- **Self-instruction**: Models can generate additional instruction data themselves by prompting auxiliary models or by using chain-of-thought reasoning to create new tasks and answers. This boosts dataset size without heavy manual labeling.
- **Data mixture**: Instruction data is often blended with large-scale language-only conversation data. This ability enables models to build strong reasoning and dialog skills alongside multimodal understanding.

Quality matters as much as scale. Noisy or repetitive data can weaken performance. Therefore, teams emphasize prompt diversity, writing instructions in varied ways to prevent overfitting. And task coverage ensures that the model learns not just captioning but also more complex tasks like visual reasoning. Filtering, deduplication and checks for alignment between modalities all help maintain data integrity.

The full training pipeline typically has three phases:

1. **Pretraining**: The model learns to align different modalities and acquire broad knowledge from large, paired datasets like image-caption pairs (for example, COCO, LAION-5B). Tasks can include predicting masked words or aligning images and text, by using objectives like contrastive learning.
2. **Instruction-tuning**: The model trains on explicit instruction-response pairs. For example, an image-question pair might be rephrased as "Describe the image in detail" to teach the model to follow natural prompts. This stage helps models like LLaVA or MiniGPT-4 handle real-world queries.
3. **Alignment tuning**: Finally, the model is aligned with human preferences by using techniques like reinforcement learning with human feedback (RLHF) or direct preference optimization (DPO). Human feedback improves model performance by refining answers for relevance, factuality and tone, and reduces hallucination.

## Challenges and Future Directions

Despite rapid progress, multimodal LLMs still face several critical challenges that limit their performance and practical deployment.

- **Handling long contexts**: Many MLLMs struggle to process long, complex sequences that mix text, images or videos. This challenge makes tasks like understanding long videos or documents with rich visuals, where end-to-end comprehension is necessary, more difficult.
- **Complex instruction following**: Current open models often fall short when asked to handle nuanced or multistep instructions. High-quality instruction following often still relies on proprietary systems like GPT-4V.
- **Cross-modal reasoning**: Techniques like multimodal in-context learning (M-ICL) and chain of thought reasoning (M-CoT) are still in their early stages, leaving models with limited cross-modal reasoning abilities.
- **Expansion to new modalities**: Future models need to handle more diverse data types, for example, combining audio, visual and physiological signals in emotion recognition or multiple medical imaging techniques for diagnosis.
- **High computation costs**: Training large multimodal models demands massive computational resources including GPUs, distributed systems and careful scheduling, making it costly and time-consuming.
- **Lifelong learning**: Most MLLMs still rely on static training. Building models that can learn continuously, adapt to new tasks and retain previous knowledge without forgetting, is an open challenge.
- **Safety and robustness**: Like text-only LLMs, multimodal models can generate biased or misleading outputs if not properly safeguarded.

Multimodal LLMs are expanding what AI can understand and generate, but the next advancements won't come from just adding more parameters. New architectures promise faster, more efficient sequence processing by breaking away from the attention-heavy bottlenecks of traditional transformers. These advancements mean more tokens, less computation and better scalability for long-context, richly multimodal tasks.

At the same time, smarter prompt engineering is reducing the need for costly fine-tuning by letting us guide models with better instructions instead of more training. And instead of endlessly scaling up model size, many researchers now focus on using larger, more diverse datasets enabling open-source and task-specific models to thrive.

With organizations like OpenAI, Microsoft and the open source community leading the charge, the future of multimodal AI is incredibly exciting. Innovations like retrieval-augmented generation (RAG), in context learning (ICL) and multimodal reasoning are setting new benchmarks for advancements in robotics, image recognition and language understanding. By effectively processing and integrating diverse multimodal inputs, these models are moving beyond unimodal systems to deliver richer, more context-aware experiences. Through advanced machine learning techniques and streamlined training processes, multimodal AI models are enabling smarter interactions, whether it's through conversational chatbots, intuitive image analysis or decision-making systems powered by autoregressive architectures. These systems are becoming more intuitive, context-aware and human-like, with the potential to make a real difference in industries ranging from healthcare to automation and beyond.

> **Note:** The following article is reproduced verbatim from  
> Determined.ai Team, *Determined.ai* (2025):  
> [How Multimodal LLMs Work](https://determined.ai/blog/multimodal-llms)  
> for internal educational use only (non-profit).

# How Multimodal LLMs Work

Large Language Models (LLMs) have shown an impressive ability to understand text. But there are many cases where we'd like LLMs to understand more than just text. We might want them to receive or generate multiple modalities of data, like text, images, and audio. LLMs with this capability are called multimodal LLMs, and in this post, we'll give a high-level overview of three multimodal LLMs in the vision-language domain. As we'll see, all three LLMs have the following components in common:

- A vision-only model.
- A text-only model (the LLM).
- One or more components that convert the output of the vision model into a format that can be fed into the LLM.

Let's dive in.

## Flamingo

Flamingo is a multimodal LLM that was presented in 2022. Here's how the vision and language components work:

- The vision encoder converts images or videos into embeddings (lists of numbers). These embeddings vary in size depending on the dimensions of the input images, or the lengths of the input videos, so another component called the Perceiver Resampler converts these embeddings to a common fixed length.
- The language model takes in text, and the fixed-length vision embeddings from the Percever Resampler. The vision embeddings are used in multiple "cross-attention" blocks, which learn to weigh the importance of different parts of the vision embedding, given the current text.

Training occurs in three steps:

1. The vision encoder is pretrained using CLIP (see the next section for an explanation). CLIP actually trains both a vision encoder and a text encoder, so the text encoder from this step is discarded.
2. The language model is a Chinchilla model pretrained on next-token prediction, i.e. predicting the next group of characters given a series of previous characters. This is how most LLMs like GPT-4 are trained. You might hear this type of model referred to as "autoregressive", which means the model predicts future values based on past values.
3. In the third stage, untrained cross-attention blocks are inserted into the language model, and an untrained Perceiver Resampler is inserted between the vision encoder and the language model. This is the complete Flamingo model, but the cross-attention blocks and the Perceiver Resampler still need to be trained. To do this, the entire Flamingo model is used to compute tokens in the next-token prediction task, but the inputs now contain images interleaved with text. Furthermore, the weights of the vision encoder and the language model are frozen. In other words, only the Perceiver Resampler and cross-attention blocks are actually updated and trained.

After training, Flamingo is able to perform a variety of vision-language tasks, including answering questions about images in a conversational format.

## What is CLIP?

As mentioned in the previous section, Flamingo uses CLIP in its pretraining stage. CLIP is not a multimodal LLM. Instead, it is a training methodology that produces separate vision and text models with powerful downstream capabilities. CLIP stands for Contrastive Language-Image Pre-training, and it is conceptually very simple:

- The model architecture consists of an image encoder and text encoder. The image encoder converts images to embeddings (lists of numbers), and the text encoder converts text to embeddings.
- The two encoders are trained on batches of image-text pairs, in which the text describes the image. The encoders are trained such that:
  - For each image-text pair, the image and text embeddings are "close" to each other.
  - For all non-matching image-text pairs, the image and text embeddings are far from each other.

Note that there are many ways to measure the distance between two embeddings. Some common ways are Euclidean distance and cosine similarity. CLIP uses the latter.

At a high level, CLIP learns a joint image-text embedding space, which basically means that the similarity between images and text can be directly computed. It turns out that training models with this goal makes them generally very useful, including in the context of multimodal LLMs.

## BLIP-2

BLIP-2 is a multimodal LLM that was released in early 2023. Like Flamingo, it contains a pretrained image encoder and LLM. But unlike Flamingo, both the image encoder and LLM are left untouched (after pretraining).

Instead, BLIP-2 introduces a new component called the Q-Former, which acts as a bridge between the vision encoder and the LLM. The Q-Former is a transformer that takes in the output of the vision encoder and produces a fixed-length sequence of embeddings that can be fed into the LLM.

The Q-Former is trained using a combination of two objectives:

1. **Image-Text Contrastive Learning**: Similar to CLIP, the Q-Former learns to align image and text representations.
2. **Image-Grounded Text Generation**: The Q-Former learns to generate text that is grounded in the image content.

The key innovation of BLIP-2 is that it keeps the vision encoder and LLM frozen, only training the Q-Former. This makes the training much more efficient and allows BLIP-2 to work with a variety of different vision encoders and LLMs.

## LLaVA

LLaVA is a multimodal LLM that was released in 2023. The architecture is quite simple:

- It uses a pretrained vision encoder (typically CLIP ViT-L/14) to encode images into embeddings.
- These embeddings are projected to the same dimension as the LLM's input embeddings using a simple linear layer.
- The projected embeddings are then concatenated with the text embeddings and fed into the LLM.

The key insight of LLaVA is that by using a simple linear projection, it can effectively bridge the gap between vision and language representations. This approach is much simpler than the more complex architectures used in Flamingo and BLIP-2, but it still achieves impressive performance on vision-language tasks.

LLaVA is trained using a combination of:

1. **Pretraining**: The model is trained on a large dataset of image-text pairs to learn basic vision-language understanding.
2. **Instruction tuning**: The model is fine-tuned on instruction-following data to improve its ability to follow user instructions.

## Summary

In this post, we briefly covered three important multimodal LLMs, plus CLIP. Here's a quick summary of the key differences:

- **Flamingo**: Uses cross-attention blocks and a Perceiver Resampler to integrate vision and language. Trains the integration components while keeping the vision encoder and LLM frozen.
- **BLIP-2**: Uses a Q-Former as a bridge between vision and language. Only trains the Q-Former, keeping both the vision encoder and LLM frozen.
- **LLaVA**: Uses a simple linear projection to align vision and language representations. Trains the entire model end-to-end.

All three approaches have their strengths and have contributed to the rapid advancement of multimodal AI capabilities.

> **Note:** The following article is reproduced verbatim from  
> BentoML Team, *BentoML* (2025):  
> [Multimodal AI: A Guide to Open-Source Vision Language Models](https://www.bentoml.com/blog/open-source-vision-language-models)  
> for internal educational use only (non-profit).

# Multimodal AI: A Guide to Open-Source Vision Language Models

You wake up in the morning, and there is another new AI model making headlines. It's not surprising anymore, right? These days, it feels like a new model drops every other day, each promising more powerful than the last.

Take Llama 3.2 Vision as an example, the first multimodal models in Meta's open-source Llama series. They push the boundaries beyond text understanding to now include images. But don't get it twisted. Multimodal AI is about more than just images and text. These models are capable of processing multiple types of information, from images and audio to video and text. And not just open-source AI, proprietary models like GPT-4 already expanded their capabilities by integrating these modalities.

Compared with proprietary models, open-source models remain a favorite for those looking for more secure, affordable, and customizable solutions. In this blog post, we're introducing some of the most popular open-source multimodal models available today.

Since the world of multimodal AI is broad, we will be focused on vision language models (VLMs). These models are designed to understand and process both visual and text information. At the same time, we will explore some FAQs about VLMs.

## Gemma 3

Gemma 3 is a family of lightweight, state-of-the-art open models developed by Google, built on the same research behind Gemini 2.0. It supports advanced text, image, and short video understanding, with strong reasoning capabilities across tasks and languages.

Available in 1B, 4B, 12B, and 27B sizes, Gemma 3 offers flexibility for a range of hardware, from laptops to cloud clusters. With a 128K-token context window (32K for 1B), it can handle long-form input for more complex tasks.

**Key features:**

- **Multilingual support**: Pretrained on data spanning over 140 languages, Gemma 3 offers out-of-the-box support for 35+ widely used languages.
- **Portable and efficient**: The small sizes make Gemma 3 ideal for deployment in constrained environments such as laptops, desktops, and edge devices. It also comes with official quantized versions that reduce resource demands while maintaining strong performance.
- **Agentic workflows**: Gemma 3 supports function calling and structured output, enabling automation and integration into complex application pipelines.

**Points to be cautious about:**

- **Limited video comprehension**: While some visual reasoning is supported (especially short videos/images), Gemma 3 does not appear to support long-form or audio-visual video analysis.
- **Image input only normalized**: Images are resized to 896×896 and encoded into 256 tokens. This may limit fine-grained image understanding or work with non-standard resolutions/aspect ratios.

## GLM-4.1V-Thinking

GLM-4.1V-Thinking is an open-source VLM developed by Z.ai, the team behind the GLM family of LLMs. With just 9 billion parameters and a 64K-token context window, it delivers exceptional multimodal reasoning performance in a compact, efficient architecture.

Trained using a "thinking paradigm" and reinforcement learning, GLM-4.1V-9B-Thinking is specifically designed for tasks that require deep reasoning, complex visual understanding, and long-context comprehension.

**Key features:**

- **Advanced visual reasoning**: Achieves state-of-the-art performance among 10B-class models, surpassing even 72B models like Qwen2.5-VL on 18 benchmark tasks.
- **Flexible image handling**: Supports images up to 4K resolution with arbitrary aspect ratios, ideal for detailed visual inputs.
- **Fully open-source**: Both the base model and thinking variant are released under the permissive MIT license, supporting research, experimentation, and commercial use.

**Points to be cautious about:**

- **Limited language support**: Currently, the model provides bilingual support for English and Chinese only.

## Llama 3.2 Vision

Llama 3.2 Vision, developed by Meta, is a collection of multimodal LLM designed to process both text and images. Available in 11B and 90B parameter sizes, Llama 3.2 Vision outperforms many open-source and proprietary models in image-text tasks.

To support image input, Meta integrates a pre-trained image encoder into the language model using adapters, which connect image data to the text-processing layers. This allows the models to handle both image and text inputs simultaneously.

**Key features:**

- **Multimodal capabilities**: Llama 3.2 Vision can perform image-text tasks, including generating captions, answering image-based questions, and complex visual reasoning.
- **Strong performance**: Both the 11B and 90B versions outperform proprietary models like Claude 3 Haiku in tasks involving chart and diagram understanding.
- **Customizability**: You can fine-tune Llama 3.2 Vision models for custom applications using Torchtune.

**Points to be cautious about:**

- **Math reasoning**: According to Meta's benchmark, Llama 3.2 Vision still shows room for improvement in math-heavy tasks, especially the 11B version.
- **Language support**: Although it supports multiple languages like German, French and Italian for text-only tasks, for image+text applications, only English is supported.

## NVLM 1.0

NVLM is a family of multimodal LLMs developed by NVIDIA, representing a frontier-class approach to VLMs. It achieves state-of-the-art results in tasks that require a deep understanding of both text and images. The first public iteration, NVLM 1.0, rivals top proprietary models like GPT-4o, as well as open-access models like Llama 3-V 405B.

**Key features:**

- **Distinct architectures**: The NVLM 1.0 family consists of three unique architectures for different use cases:
  - **NVLM-D**: A decoder-only architecture that provides unified multimodal reasoning and performs better at OCR-related tasks.
  - **NVLM-X**: A cross-attention-based architecture that is computationally efficient, particularly when handling high-resolution images.
  - **NVLM-H**: A hybrid architecture that combines the strengths of both the decoder-only and cross-attention approaches. It delivers superior performance in multimodal reasoning and image processing.

- **Powerful image reasoning**: NVLM 1.0 surpasses many proprietary and open-source models in tasks such as OCR, multimodal reasoning, and high-resolution image handling. It demonstrates exceptional scene understanding capability.

- **Improved text-only performance**: NVIDIA researchers observed that while open multimodal LLMs often achieve strong results in vision language tasks, their performance tends to degrade in text-only tasks. Therefore, they developed "production-grade multimodality" for the NVLM models. This enables NVLM models to excel in both vision language tasks and text-only tasks.

**Points to be cautious about:**

- **Non-commercial use**: NVLM 1.0 is shared by NVIDIA strictly for research purposes and hobbyist experimentation. It is not licensed for commercial deployment.
- **Limited variant release**: NVIDIA has only made the NVLM-1.0-D-72B (decoder-only) weights and code publicly available so far. Additional architectures or model sizes may be released in the future.

## Molmo

Molmo is a family of open-source VLMs developed by the Allen Institute for AI. Available in 1B, 7B, and 72B parameters, Molmo models deliver state-of-the-art performance for their class. According to the benchmarks, they can perform on a par with proprietary models like GPT-4V, Gemini 1.5 Pro, and Claude 3.5 Sonnet.

The key to Molmo's performance lies in its unique training data, **PixMo**. This highly curated dataset consists of 1 million image-text pairs and includes two main types of data:

- Dense captioning data for multimodal pre-training
- Supervised fine-tuning data to enable various user interactions, such as question answering, document reading, and even pointing to objects in images.

Interestingly, Molmo researchers used an innovative approach to data collection: **Asked annotators to provide spoken descriptions of images within 60 to 90 seconds**. Specifically, these detailed descriptions included everything visible, even the spatial positioning and relationships among objects. The results show that annotators provided detailed captions far more efficiently than traditional methods (writing them down). Overall, they collected high-quality audio descriptions for 712k images that were sampled from 50 high-level topics.

**Key features:**

- **State-of-the-art performance**: Molmo's 72B model outperforms proprietary models like Gemini 1.5 Pro and Claude 3.5 Sonnet on academic benchmarks. Even the smaller 7B and 1B models rival GPT-4V in several tasks.
- **Pointing capabilities**: Molmo can "point" to one or more visual elements in the image. Pointing provides a natural explanation grounded in image pixels. Molmo researchers believe that in the future pointing will be an important communication channel between VLMs and agents. For example, a web agent could query the VLM for the location of specific objects.
- **Open architecture**: The original developers promise to release all artifacts used in creating Molmo, including the PixMo dataset, training code, evaluations, and intermediate checkpoints. This offers a new standard for building high-performing multimodal systems from scratch and promotes reproducibility.

**Points to be cautious about:**

- **Transparent images**: Molmo may struggle with transparent images. It's recommended to add a white or dark background to images before processing them with the model.
- **Broadcast errors**: If you encounter a broadcast error while processing images, ensure your image is in RGB format.

## Qwen2.5-VL

Qwen2.5-VL is a powerful vision-language model developed by Alibaba Cloud. It represents a significant advancement in multimodal AI capabilities, offering strong performance across a wide range of vision-language tasks.

**Key features:**

- **Comprehensive multimodal understanding**: Qwen2.5-VL excels at understanding and reasoning about images, text, and their relationships.
- **High-resolution image support**: The model can handle high-resolution images effectively, making it suitable for detailed visual analysis tasks.
- **Strong reasoning capabilities**: Qwen2.5-VL demonstrates excellent performance in complex reasoning tasks that require both visual and textual understanding.

**Differences between Qwen2.5-VL and Qwen2.5-Omni:**

- **Qwen2.5-VL**: Focused specifically on vision-language tasks, optimized for image understanding and text generation based on visual input.
- **Qwen2.5-Omni**: A more general-purpose multimodal model that can handle multiple modalities including text, images, audio, and video.

## Sources

<Card title="Reference Materials">
  <ul>
    <li><strong>Anthropic Claude Documentation:</strong> <a href="https://docs.anthropic.com/en/docs/overview">https://docs.anthropic.com/en/docs/overview</a></li>
    <li><strong>Claude Vision Guide:</strong> <a href="https://docs.anthropic.com/en/docs/vision">https://docs.anthropic.com/en/docs/vision</a></li>
    <li><strong>Multimodal AI Research:</strong> <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a></li>
    <li><strong>Computer Vision Fundamentals:</strong> <a href="https://arxiv.org/abs/1409.0575">https://arxiv.org/abs/1409.0575</a></li>
    <li><strong>Multimodal UX Design:</strong> <a href="https://www.interaction-design.org/literature/topics/multimodal-interfaces">https://www.interaction-design.org/literature/topics/multimodal-interfaces</a></li>
  </ul>
</Card>

## Figures

<Card title="Multimodal Processing Pipeline">
  <Frame>
    <img src="/images/multimodal-pipeline.png" alt="Diagram showing how multiple modalities are processed and fused in AI systems" />
  </Frame>
</Card>

<Card title="Modality Interaction Patterns">
  <Frame>
    <img src="/images/modality-interactions.png" alt="Visualization of different ways modalities can interact and complement each other" />
  </Frame>
</Card>

