---
title: "Multimodality in AI"
description: "Explore how AI systems process and understand multiple types of data simultaneously, from text and images to audio and video, enabling richer user experiences"
slug: "modules-multimodality"
updatedAt: "2025-08-19"
tags: [module, multimodality, multimodal, ai, text, image, audio, video, claude, anthropic]
---

# Multimodality in AI

<Callout type="info">
  **Learning Objective**: Understand how AI systems process multiple data types simultaneously and how to design interfaces that leverage multimodal capabilities for richer user experiences, with special focus on Claude's vision and multimodal capabilities.
</Callout>

## Overview

Multimodality in AI refers to systems that can understand, process, and generate multiple types of data simultaneously - including text, images, audio, and video. This capability enables more natural and intuitive interactions that mirror how humans perceive and communicate with the world.

<CardGroup cols={2}>
  <Card title="Natural Interaction" icon="users">
    Multimodal AI enables interactions that feel more natural and human-like, combining different senses and communication modes.
  </Card>
  <Card title="Richer Context" icon="layers">
    Multiple data types provide richer context and understanding, leading to more accurate and relevant responses.
  </Card>
</CardGroup>

## Claude's Multimodal Capabilities

<Callout type="info">
  **Claude's Vision**: Claude 3 models feature sophisticated vision capabilities that enable the model to understand and analyze images, charts, diagrams, and visual content alongside text.
</Callout>

### Claude's Vision Features

<Card title="Claude's Visual Understanding">
  <p>Claude can process and understand visual content in various formats:</p>
  
  <h4>Supported Image Types:</h4>
  <ul>
    <li><strong>Photographs:</strong> Real-world images, objects, scenes</li>
    <li><strong>Charts and Graphs:</strong> Data visualizations, analytics</li>
    <li><strong>Diagrams:</strong> Technical drawings, flowcharts, schematics</li>
    <li><strong>Documents:</strong> Screenshots, forms, handwritten text</li>
    <li><strong>Code:</strong> Screenshots of code, terminal output</li>
    <li><strong>UI/UX Elements:</strong> Interface mockups, wireframes</li>
  </ul>
  
  <h4>Visual Analysis Capabilities:</h4>
  <ul>
    <li><strong>Object Recognition:</strong> Identify objects, people, scenes</li>
    <li><strong>Text Extraction:</strong> Read text from images (OCR)</li>
    <li><strong>Data Interpretation:</strong> Analyze charts and graphs</li>
    <li><strong>Code Understanding:</strong> Read and analyze code screenshots</li>
    <li><strong>Document Processing:</strong> Extract information from forms and documents</li>
    <li><strong>Visual Reasoning:</strong> Understand spatial relationships and context</li>
  </ul>
</Card>

### Claude's Multimodal Processing

<Card title="Text + Vision Integration">
  <h4>Combined Processing:</h4>
  <ul>
    <li><strong>Contextual Understanding:</strong> Claude can reference images while discussing text</li>
    <li><strong>Cross-Modal Reasoning:</strong> Connect visual elements with textual concepts</li>
    <li><strong>Enhanced Responses:</strong> Provide more accurate and detailed answers</li>
    <li><strong>Visual Explanations:</strong> Explain visual content in natural language</li>
  </ul>
  
  <h4>Example Use Cases:</h4>
  <ul>
    <li><strong>Document Analysis:</strong> Extract and summarize information from complex documents</li>
    <li><strong>Data Visualization:</strong> Analyze charts and provide insights</li>
    <li><strong>Code Review:</strong> Review code screenshots and suggest improvements</li>
    <li><strong>Design Feedback:</strong> Analyze UI mockups and provide design suggestions</li>
    <li><strong>Educational Content:</strong> Explain diagrams and visual concepts</li>
  </ul>
</Card>

### Claude Vision Implementation

<Card title="Using Claude's Vision">
  <h4>Basic Implementation:</h4>
  <pre><code>import anthropic
import base64

client = anthropic.Anthropic(api_key="your-api-key")

def analyze_image_with_claude(image_path, question):
    """
    Analyze an image using Claude's vision capabilities
    """
    
    # Read and encode image
    with open(image_path, "rb") as image_file:
        image_data = base64.b64encode(image_file.read()).decode('utf-8')
    
    # Create message with image
    response = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=1000,
        messages=[
            \{\{
                "role": "user",
                "content": [
                    \{\{
                        "type": "image",
                        "source": \{\{
                            "type": "base64",
                            "media_type": "image/jpeg",
                            "data": image_data
                        \}\}
                    \}\},
                    \{\{
                        "type": "text",
                        "text": question
                    \}\}
                ]
            \}\}
        ]
    )
    
    return response.content[0].text

# Example usage
result = analyze_image_with_claude(
    "chart.png", 
    "What insights can you draw from this data visualization?"
)
print(result)
</code></pre>
</Card>

## Why It's Important for Designers to Know

### 1. **Enhanced User Experience**

<Card title="Multimodal UX Benefits">
  <ul>
    <li><strong>Natural Communication:</strong> Users can interact using multiple modalities simultaneously</li>
    <li><strong>Reduced Cognitive Load:</strong> Information can be presented in the most appropriate format</li>
    <li><strong>Accessibility:</strong> Multiple input/output options accommodate different user needs</li>
    <li><strong>Engagement:</strong> Rich, dynamic interactions increase user engagement</li>
    <li><strong>Efficiency:</strong> Faster and more accurate information exchange</li>
  </ul>
</Card>

### 2. **Design Opportunities**

<Callout type="warning">
  **Design Challenge**: Multimodal interfaces require careful consideration of how different modalities work together and when to use each one.
</Callout>

<Table>
  <TableHead>
    <TableRow>
      <TableHeader>Modality</TableHeader>
      <TableHeader>Strengths</TableHeader>
      <TableHeader>Design Considerations</TableHeader>
      <TableHeader>Best Use Cases</TableHeader>
    </TableRow>
  </TableHead>
  <TableBody>
    <TableRow>
      <TableCell><strong>Text</strong></TableCell>
      <TableCell>Precise, searchable, accessible</TableCell>
      <TableCell>Readability, character limits, language support</TableCell>
      <TableCell>Detailed information, instructions, search</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Images</strong></TableCell>
      <TableCell>Visual, immediate, emotional impact</TableCell>
      <TableCell>Resolution, loading times, alt text</TableCell>
      <TableCell>Visual content, demonstrations, branding</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Audio</strong></TableCell>
      <TableCell>Hands-free, natural, emotional</TableCell>
      <TableCell>Clarity, background noise, accessibility</TableCell>
      <TableCell>Voice commands, music, notifications</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Video</strong></TableCell>
      <TableCell>Dynamic, engaging, comprehensive</TableCell>
      <TableCell>Bandwidth, autoplay, mobile optimization</TableCell>
      <TableCell>Tutorials, entertainment, live content</TableCell>
    </TableRow>
  </TableBody>
</Table>

### 3. **Technical Understanding**

<Card title="Technical Implications">
  <ul>
    <li><strong>Model Complexity:</strong> Multimodal models are more complex and resource-intensive</li>
    <li><strong>Data Processing:</strong> Different modalities require different preprocessing</li>
    <li><strong>Integration Challenges:</strong> Coordinating multiple input/output streams</li>
    <li><strong>Performance Trade-offs:</strong> Balancing quality vs. speed vs. cost</li>
    <li><strong>Error Handling:</strong> Managing failures across different modalities</li>
  </ul>
</Card>

## Core Multimodal Concepts

### 1. **Modality Types and Characteristics**

<Card title="Understanding Modalities">
  <h4>Primary Modalities:</h4>
  <ul>
    <li><strong>Text:</strong> Written language, symbols, structured data</li>
    <li><strong>Visual:</strong> Images, videos, graphics, charts</li>
    <li><strong>Audio:</strong> Speech, music, sounds, voice</li>
    <li><strong>Temporal:</strong> Time-series data, sequences, events</li>
  </ul>
  
  <h4>Modality Characteristics:</h4>
  <ul>
    <li><strong>Dimensionality:</strong> Text is 1D, images are 2D, video is 3D (2D + time)</li>
    <li><strong>Information Density:</strong> Visual content often contains more information per unit</li>
    <li><strong>Processing Speed:</strong> Text is fastest to process, video is slowest</li>
    <li><strong>Storage Requirements:</strong> Visual and audio content require more storage</li>
  </ul>
</Card>

### 2. **Multimodal Fusion**

<Card title="Combining Modalities">
  <h4>Fusion Strategies:</h4>
  <ul>
    <li><strong>Early Fusion:</strong> Combine modalities at the input level</li>
    <li><strong>Late Fusion:</strong> Process each modality separately, then combine results</li>
    <li><strong>Intermediate Fusion:</strong> Combine at intermediate processing stages</li>
    <li><strong>Cross-Modal Attention:</strong> Use attention mechanisms to focus on relevant modalities</li>
  </ul>
  
  <h4>Claude's Approach:</h4>
  <ul>
    <li><strong>Unified Processing:</strong> Claude processes text and images together</li>
    <li><strong>Contextual Integration:</strong> Visual context enhances textual understanding</li>
    <li><strong>Cross-Reference Capability:</strong> Can reference visual elements in text responses</li>
    <li><strong>Seamless Interaction:</strong> Natural conversation about visual content</li>
  </ul>
</Card>

### 3. **Multimodal Understanding**

<Card title="Understanding Across Modalities">
  <h4>Key Capabilities:</h4>
  <ul>
    <li><strong>Cross-Modal Translation:</strong> Convert between modalities (text to image, etc.)</li>
    <li><strong>Modality Alignment:</strong> Understand relationships between different modalities</li>
    <li><strong>Context Preservation:</strong> Maintain context across modality switches</li>
    <li><strong>Semantic Understanding:</strong> Extract meaning regardless of modality</li>
  </ul>
  
  <h4>Claude's Strengths:</h4>
  <ul>
    <li><strong>Visual Comprehension:</strong> Deep understanding of image content</li>
    <li><strong>Text-Vision Alignment:</strong> Seamless integration of text and visual information</li>
    <li><strong>Contextual Reasoning:</strong> Can reason about visual content in context</li>
    <li><strong>Detailed Analysis:</strong> Provide comprehensive analysis of visual elements</li>
  </ul>
</Card>

## Claude-Specific Implementation

### 1. **Basic Vision Integration**

<Card title="Simple Image Analysis">
  <h4>Python Implementation:</h4>
  <pre><code>import anthropic
import base64
from PIL import Image
import io

def analyze_image_claude(image_path, prompt):
    """
    Analyze an image using Claude's vision capabilities
    """
    
    # Load and prepare image
    with Image.open(image_path) as img:
        # Convert to RGB if necessary
        if img.mode != 'RGB':
            img = img.convert('RGB')
        
        # Save to bytes
        img_byte_arr = io.BytesIO()
        img.save(img_byte_arr, format='JPEG')
        img_byte_arr = img_byte_arr.getvalue()
        
        # Encode to base64
        image_data = base64.b64encode(img_byte_arr).decode('utf-8')
    
    # Create Claude client
    client = anthropic.Anthropic(api_key="your-api-key")
    
    # Send request
    response = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=2000,
        messages=[
            \{\{
                "role": "user",
                "content": [
                    \{\{
                        "type": "image",
                        "source": \{\{
                            "type": "base64",
                            "media_type": "image/jpeg",
                            "data": image_data
                        \}\}
                    \}\},
                    \{\{
                        "type": "text",
                        "text": prompt
                    \}\}
                ]
            \}\}
        ]
    )
    
    return response.content[0].text

# Example usage
result = analyze_image_claude(
    "dashboard.png",
    "Analyze this dashboard and provide insights about the data shown."
)
print(result)
</code></pre>
</Card>

### 2. **Advanced Multimodal Processing**

<Card title="Complex Multimodal Analysis">
  <h4>Multiple Images and Text:</h4>
  <pre><code>def multimodal_analysis_claude(images, text_prompt):
    """
    Analyze multiple images with text using Claude
    """
    
    client = anthropic.Anthropic(api_key="your-api-key")
    
    # Prepare content array
    content = []
    
    # Add images
    for image_path in images:
        with open(image_path, "rb") as img_file:
            image_data = base64.b64encode(img_file.read()).decode('utf-8')
        
        content.append(\{\{
            "type": "image",
            "source": \{\{
                "type": "base64",
                "media_type": "image/jpeg",
                "data": image_data
            \}\}
        \}\})
    
    # Add text prompt
    content.append(\{\{
        "type": "text",
        "text": text_prompt
    \}\})
    
    # Send request
    response = client.messages.create(
        model="claude-3-sonnet-20240229",
        max_tokens=3000,
        messages=[
            \{\{
                "role": "user",
                "content": content
            \}\}
        ]
    )
    
    return response.content[0].text

# Example usage
images = ["chart1.png", "chart2.png", "dashboard.png"]
prompt = """
Compare these three visualizations and provide insights about:
1. Data trends across the charts
2. Key differences and similarities
3. Recommendations based on the data
"""

result = multimodal_analysis_claude(images, prompt)
print(result)
</code></pre>
</Card>

### 3. **JavaScript Implementation**

<Card title="JavaScript Vision Integration">
  <h4>Browser-based Implementation:</h4>
  <pre><code>import Anthropic from '@anthropic-ai/sdk';

async function analyzeImageWithClaude(imageFile, prompt) {
    const client = new Anthropic({
        apiKey: 'your-api-key',
    });
    
    // Convert image to base64
    const base64Image = await fileToBase64(imageFile);
    
    // Create content array
    const content = [
        {
            type: 'image',
            source: {
                type: 'base64',
                media_type: imageFile.type,
                data: base64Image
            }
        },
        {
            type: 'text',
            text: prompt
        }
    ];
    
    // Send request
    const response = await client.messages.create({
        model: 'claude-3-sonnet-20240229',
        max_tokens: 2000,
        messages: [
            {
                role: 'user',
                content: content
            }
        ]
    });
    
    return response.content[0].text;
}

// Helper function to convert file to base64
function fileToBase64(file) {
    return new Promise((resolve, reject) => {
        const reader = new FileReader();
        reader.readAsDataURL(file);
        reader.onload = () => {
            const base64 = reader.result.split(',')[1];
            resolve(base64);
        };
        reader.onerror = error => reject(error);
    });
}

// Example usage with file input
document.getElementById('imageInput').addEventListener('change', async (event) => {
    const file = event.target.files[0];
    if (file) {
        const result = await analyzeImageWithClaude(
            file,
            "Describe what you see in this image and provide any relevant insights."
        );
        console.log(result);
    }
});
</code></pre>
</Card>

### 4. **LangChain Integration**

<Card title="LangChain Multimodal">
  <h4>LangChain with Claude Vision:</h4>
  <pre><code>from langchain_anthropic import ChatAnthropic
from langchain.schema import HumanMessage
import base64

class ClaudeVisionChain:
    def __init__(self, api_key: str):
        self.llm = ChatAnthropic(
            model="claude-3-sonnet-20240229",
            temperature=0.1
        )
    
    def analyze_image(self, image_path: str, prompt: str) -> str:
        """
        Analyze an image using Claude through LangChain
        """
        
        # Read and encode image
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Create message with image
        message = HumanMessage(
            content=[
                \{\{
                    "type": "image",
                    "source": \{\{
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_data
                    \}\}
                \}\},
                \{\{
                    "type": "text",
                    "text": prompt
                \}\}
            ]
        )
        
        # Get response
        response = self.llm.invoke([message])
        return response.content

# Example usage
vision_chain = ClaudeVisionChain("your-api-key")
result = vision_chain.analyze_image(
    "screenshot.png",
    "Analyze this UI screenshot and provide feedback on the design and user experience."
)
print(result)
</code></pre>
</Card>

### 5. **CrewAI Multimodal Agents**

<Card title="Multimodal CrewAI Agents">
  <h4>Vision-Enabled Agents:</h4>
  <pre><code>from crewai import Agent, Task, Crew
from langchain_anthropic import ChatAnthropic
import base64

class MultimodalAgent:
    def __init__(self, api_key: str):
        self.llm = ChatAnthropic(
            model="claude-3-sonnet-20240229",
            temperature=0.1
        )
    
    def create_vision_agent(self):
        """
        Create an agent specialized in visual analysis
        """
        return Agent(
            role="Visual Analyst",
            goal="Analyze visual content and provide detailed insights",
            backstory="You are an expert at analyzing images, charts, diagrams, and visual content. You can understand complex visual information and provide detailed, accurate analysis.",
            llm=self.llm,
            verbose=True
        )
    
    def create_multimodal_task(self, image_path: str, analysis_prompt: str):
        """
        Create a task for multimodal analysis
        """
        
        # Read and encode image
        with open(image_path, "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # Create task description with image
        task_description = f"""
        Analyze the provided image and answer the following question:
        
        Question: \{analysis_prompt\}
        
        [Image data would be included here in the actual implementation]
        
        Provide a comprehensive analysis including:
        1. Visual elements identified
        2. Key insights and observations
        3. Recommendations or conclusions
        4. Any relevant context or background information
        """
        
        return Task(
            description=task_description,
            agent=self.create_vision_agent()
        )
    
    def analyze_with_crew(self, image_path: str, prompt: str):
        """
        Analyze image using CrewAI with vision capabilities
        """
        agent = self.create_vision_agent()
        task = self.create_multimodal_task(image_path, prompt)
        
        crew = Crew(
            agents=[agent],
            tasks=[task],
            verbose=True
        )
        
        result = crew.kickoff()
        return result

# Example usage
multimodal_agent = MultimodalAgent("your-api-key")
result = multimodal_agent.analyze_with_crew(
    "dashboard.png",
    "Analyze this dashboard and provide insights about the data visualization and user experience."
)
print(result)
</code></pre>
</Card>

## Real-World Applications

### 1. **Document Analysis**

<Card title="Document Processing with Claude">
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Form Processing:</strong> Extract data from forms and documents</li>
    <li><strong>Receipt Analysis:</strong> Automatically categorize and extract expense data</li>
    <li><strong>Contract Review:</strong> Analyze legal documents and identify key terms</li>
    <li><strong>Report Generation:</strong> Create summaries from complex documents</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  <pre><code>def analyze_document_claude(document_path, analysis_type):
    """
    Analyze documents using Claude's vision capabilities
    """
    
    prompts = {
        "form": "Extract all form fields and their values from this document.",
        "receipt": "Extract the vendor, date, items, and total amount from this receipt.",
        "contract": "Identify key terms, dates, parties, and obligations in this contract.",
        "report": "Summarize the main points and key findings from this report."
    }
    
    prompt = prompts.get(analysis_type, "Analyze this document and provide insights.")
    
    return analyze_image_with_claude(document_path, prompt)

# Example usage
form_data = analyze_document_claude("application_form.png", "form")
receipt_data = analyze_document_claude("receipt.jpg", "receipt")
</code></pre>
</Card>

### 2. **Data Visualization Analysis**

<Card title="Chart and Graph Analysis">
  <h4>Capabilities:</h4>
  <ul>
    <li><strong>Chart Interpretation:</strong> Understand various chart types and data</li>
    <li><strong>Trend Analysis:</strong> Identify patterns and trends in data</li>
    <li><strong>Insight Generation:</strong> Provide business insights from visualizations</li>
    <li><strong>Recommendation Creation:</strong> Suggest actions based on data</li>
  </ul>
  
  <h4>Example Implementation:</h4>
  <pre><code>def analyze_chart_claude(chart_path, business_context=""):
    """
    Analyze charts and provide business insights
    """
    
    prompt = f"""
    Analyze this chart and provide insights about:
    1. What the data shows
    2. Key trends and patterns
    3. Notable observations
    4. Business implications
    5. Recommendations
    
    Business Context: \{business_context\}
    """
    
    return analyze_image_with_claude(chart_path, prompt)

# Example usage
sales_insights = analyze_chart_claude(
    "sales_chart.png",
    "This is a quarterly sales performance chart for our e-commerce platform."
)
</code></pre>
</Card>

### 3. **UI/UX Design Analysis**

<Card title="Design Feedback with Claude">
  <h4>Design Analysis Capabilities:</h4>
  <ul>
    <li><strong>Layout Analysis:</strong> Evaluate visual hierarchy and organization</li>
    <li><strong>Usability Assessment:</strong> Identify potential usability issues</li>
    <li><strong>Accessibility Review:</strong> Check for accessibility concerns</li>
    <li><strong>Design Recommendations:</strong> Suggest improvements and alternatives</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def analyze_ui_design_claude(design_path, context=""):
    """
    Analyze UI/UX designs and provide feedback
    """
    
    prompt = f"""
    Analyze this UI/UX design and provide feedback on:
    
    1. Visual Design
       - Layout and visual hierarchy
       - Color scheme and typography
       - Brand consistency
    
    2. User Experience
       - Navigation and information architecture
       - User flow and interactions
       - Potential usability issues
    
    3. Accessibility
       - Color contrast and readability
       - Screen reader compatibility
       - Keyboard navigation
    
    4. Recommendations
       - Specific improvements
       - Best practices to follow
       - Alternative approaches
    
    Design Context: \{context\}
    """
    
    return analyze_image_with_claude(design_path, prompt)

# Example usage
design_feedback = analyze_ui_design_claude(
    "mobile_app_mockup.png",
    "This is a mobile app design for a food delivery service."
)
</code></pre>
</Card>

### 4. **Code Review and Analysis**

<Card title="Code Analysis with Claude">
  <h4>Code Review Capabilities:</h4>
  <ul>
    <li><strong>Code Reading:</strong> Understand code from screenshots</li>
    <li><strong>Bug Detection:</strong> Identify potential issues and bugs</li>
    <li><strong>Best Practices:</strong> Suggest improvements and optimizations</li>
    <li><strong>Documentation:</strong> Generate documentation from code</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def review_code_claude(code_screenshot_path, language=""):
    """
    Review code from screenshots using Claude
    """
    
    prompt = f"""
    Review this code and provide feedback on:
    
    1. Code Quality
       - Readability and structure
       - Naming conventions
       - Code organization
    
    2. Potential Issues
       - Bugs or errors
       - Performance concerns
       - Security vulnerabilities
    
    3. Best Practices
       - Language-specific recommendations
       - Design patterns
       - Optimization opportunities
    
    4. Suggestions
       - Specific improvements
       - Alternative approaches
       - Additional features
    
    Programming Language: \{language\}
    """
    
    return analyze_image_with_claude(code_screenshot_path, prompt)

# Example usage
code_review = review_code_claude(
    "python_code.png",
    "Python"
)
</code></pre>
</Card>

## Best Practices for Multimodal Design

### 1. **Modality Selection**

<Card title="Choosing the Right Modality">
  <h4>Selection Criteria:</h4>
  <ul>
    <li><strong>Information Type:</strong> Choose modality that best represents the information</li>
    <li><strong>User Context:</strong> Consider user environment and preferences</li>
    <li><strong>Accessibility:</strong> Ensure multiple modalities for accessibility</li>
    <li><strong>Performance:</strong> Balance quality with processing speed</li>
  </ul>
  
  <h4>Claude-Specific Considerations:</h4>
  <ul>
    <li><strong>Image Quality:</strong> Ensure images are clear and well-lit</li>
    <li><strong>Context Provision:</strong> Provide relevant text context with images</li>
    <li><strong>Specific Questions:</strong> Ask specific questions about visual content</li>
    <li><strong>Follow-up Analysis:</strong> Use Claude's responses to guide further analysis</li>
  </ul>
</Card>

### 2. **Integration Strategies**

<Card title="Effective Integration">
  <h4>Integration Approaches:</h4>
  <ul>
    <li><strong>Complementary Use:</strong> Use modalities to complement each other</li>
    <li><strong>Progressive Disclosure:</strong> Reveal information through multiple modalities</li>
    <li><strong>Redundancy:</strong> Provide key information in multiple formats</li>
    <li><strong>Context Switching:</strong> Allow users to switch between modalities</li>
  </ul>
  
  <h4>Claude Integration Tips:</h4>
  <ul>
    <li><strong>Clear Prompts:</strong> Provide specific, detailed prompts for visual analysis</li>
    <li><strong>Iterative Analysis:</strong> Use follow-up questions to dive deeper</li>
    <li><strong>Context Provision:</strong> Include relevant background information</li>
    <li><strong>Output Formatting:</strong> Structure responses for easy consumption</li>
  </ul>
</Card>

### 3. **Error Handling**

<Card title="Robust Error Handling">
  <h4>Error Scenarios:</h4>
  <ul>
    <li><strong>Image Processing Failures:</strong> Handle corrupted or unsupported images</li>
    <li><strong>API Limitations:</strong> Manage rate limits and quotas</li>
    <li><strong>Quality Issues:</strong> Handle low-quality or unclear images</li>
    <li><strong>Context Loss:</strong> Maintain context when modalities fail</li>
  </ul>
  
  <h4>Implementation Strategies:</h4>
  <ul>
    <li><strong>Fallback Mechanisms:</strong> Provide alternative analysis methods</li>
    <li><strong>Quality Validation:</strong> Check image quality before processing</li>
    <li><strong>Graceful Degradation:</strong> Continue with available modalities</li>
    <li><strong>User Feedback:</strong> Inform users of processing status and issues</li>
  </ul>
</Card>

## Evaluation and Testing

### 1. **Performance Metrics**

<Card title="Multimodal Performance">
  <h4>Key Metrics:</h4>
  <ul>
    <li><strong>Accuracy:</strong> Correctness of multimodal understanding</li>
    <li><strong>Latency:</strong> Processing time for different modalities</li>
    <li><strong>User Satisfaction:</strong> User feedback on multimodal interactions</li>
    <li><strong>Accessibility:</strong> Effectiveness for users with different needs</li>
  </ul>
  
  <h4>Claude-Specific Testing:</h4>
  <ul>
    <li><strong>Image Analysis Accuracy:</strong> Test with various image types</li>
    <li><strong>Text-Vision Alignment:</strong> Verify correct understanding of visual content</li>
    <li><strong>Response Quality:</strong> Evaluate depth and relevance of analysis</li>
    <li><strong>Processing Speed:</strong> Measure response times for different image sizes</li>
  </ul>
</Card>

### 2. **Testing Strategies**

<Card title="Comprehensive Testing">
  <h4>Testing Approaches:</h4>
  <ul>
    <li><strong>Unit Testing:</strong> Test individual modality processing</li>
    <li><strong>Integration Testing:</strong> Test multimodal fusion and coordination</li>
    <li><strong>User Testing:</strong> Test with real users and scenarios</li>
    <li><strong>Edge Case Testing:</strong> Test with unusual or challenging inputs</li>
  </ul>
  
  <h4>Test Data:</h4>
  <ul>
    <li><strong>Diverse Images:</strong> Test with various image types and qualities</li>
    <li><strong>Complex Scenarios:</strong> Test with multiple images and complex prompts</li>
    <li><strong>Error Conditions:</strong> Test with corrupted or invalid inputs</li>
    <li><strong>Performance Limits:</strong> Test with large images and long prompts</li>
  </ul>
</Card>

## Related Concepts

<CardGroup cols={2}>
  <Card title="Vision AI" icon="eye" href="../prompting-techniques/multimodal-cot">
    Computer vision and image understanding
  </Card>
  <Card title="Natural Language Processing" icon="message-circle" href="../prompting-techniques/chain-of-thought">
    Text processing and understanding
  </Card>
  <Card title="User Experience Design" icon="users" href="../collaboration-with-engineers">
    Designing for multimodal interactions
  </Card>
  <Card title="Accessibility" icon="heart" href="../safety-security">
    Ensuring inclusive multimodal experiences
  </Card>
  <Card title="Data Visualization" icon="bar-chart" href="../evaluation-observability">
    Creating effective visual representations
  </Card>
  <Card title="API Integration" icon="link" href="../prompting-techniques/react">
    Integrating multimodal services
  </Card>
</CardGroup>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Using Multimodal AI Models For Your Applications (Part 3)](https://www.smashingmagazine.com/2024/10/using-multimodal-ai-models-applications-part3/)  
> for internal educational use only (non-profit).

# Using Multimodal AI Models For Your Applications (Part 3)

In this third and final part of a three-part series, we're taking a more streamlined approach to an application that supports vision-language (VLM) and text-to-speech (TTS). This time, we'll use different models that are designed for all three modalities — images or videos, text, and audio (including speech-to-text) — in one model. These "any-to-any" models make things easier by allowing us to avoid switching between models.

Specifically, we'll focus on two powerful models: Reka and Gemini 1.5 Pro.

Both models take things to the next level compared to the tools we used earlier. They eliminate the need for separate speech recognition models, providing a unified solution for multimodal tasks. With this in mind, our goal in this article is to explore how Reka and Gemini simplify building advanced applications that handle images, text, and audio all at once.

## Overview Of Multimodal AI Models

The architecture of multimodal models has evolved to enable seamless handling of various inputs, including text, images, and audio, among others. Traditional models often require separate components for each modality, but recent advancements in "any-to-any" models like Next-GPT or 4M allow developers to build systems that process multiple modalities within a unified architecture.

Gato, for instance, utilizes a 1.2 billion parameter decoder-only transformer architecture with 24 layers, embedding sizes of 2048 and a hidden size of 8196 in its feed-forward layers. This structure is optimized for general tasks across various inputs, but it still relies on extensive task-specific fine-tuning.

GPT-4o, on the other hand, takes a different approach with training on multiple media types within a single architecture. This means it's a single model trained to handle a variety of inputs (e.g., text, images, code) without the need for separate systems for each. This training method allows for smoother task-switching and better generalization across tasks.

Similarly, CoDi employs a multistage training scheme to handle a linear number of tasks while supporting input-output combinations across different modalities. CoDi's architecture builds a shared multimodal space, enabling synchronized generation for intertwined modalities like video and audio, making it ideal for more dynamic multimedia tasks.

Most "any-to-any" models, including the ones we've discussed, rely on a few key concepts to handle different tasks and inputs smoothly:

- **Shared representation space**: These models convert different types of inputs — text, images, audio — into a common feature space. Text is encoded into vectors, images into feature maps, and audio into spectrograms or embeddings. This shared space allows the model to process various inputs in a unified way.
- **Attention mechanisms**: Attention layers help the model focus on the most relevant parts of each input, whether it's understanding the text, generating captions from images, or interpreting audio.
- **Cross-modal interaction**: In many models, inputs from one modality (e.g., text) can guide the generation or interpretation of another modality (e.g., images), allowing for more integrated and cohesive outputs.
- **Pre-training and fine-tuning**: Models are typically pre-trained on large datasets across different types of data and then fine-tuned for specific tasks, enhancing their performance in real-world applications.

## Reka Models

Reka is an AI research company that helps developers build powerful applications by offering models for a range of tasks. These tasks include generating text from videos and images, translating speech, and answering complex questions from long multimodal documents. Reka's models can even write and execute code, providing flexible, real-world solutions for developers.

These are the three main models Reka offers:

1. **Reka Core**: A 67-billion-parameter multimodal language model designed for complex tasks. It supports inputs like images, videos, and texts while excelling in advanced reasoning and coding.
2. **Reka Flash**: A faster model with a 21-billion-parameter, designed for flexibility and rapid performance in multimodal settings.
3. **Reka Edge (PDF)**: A smaller 7-billion-parameter model was built for on-device and low-latency applications, making it efficient for local use and local or latency-sensitive applications.

Reka's models can be fine-tuned and deployed securely, whether on the cloud, on-premises, or even on-device. Let's start by testing Reka's capabilities directly through its playground. This allows us to experiment with its multimodal features without writing any code, providing a hands-on way to see how the models handle various tasks, such as image and video comprehension.

Alright, we'll kick things off by uploading an image of a diagram outline of the PaliGemma architecture and ask Reka for a detailed explanation.

> Can you provide a more detailed explanation of this image?

![A detailed explanation by Reka of a diagram outline of the PaliGemma architecture](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/using-multimodal-ai-models-applications-part3/1-diagram-outline-reka-explanation.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Here's what we get from Reka Core:

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Integrating Image-To-Text And Text-To-Speech Models (Part 2)](https://www.smashingmagazine.com/2024/08/integrating-image-to-text-and-text-to-speech-models-part2/)  
> for internal educational use only (non-profit).

# Integrating Image-To-Text And Text-To-Speech Models (Part 2)

In Part 1 of this brief two-part series, we developed an application that turns images into audio descriptions using vision-language and text-to-speech models. We combined an image-to-text that analyses and understands images, generating description, with a text-to-speech model to create an audio description, helping people with sight challenges. We also discussed how to choose the right model to fit your needs.

Now, we are taking things a step further. Instead of just providing audio descriptions, we are building that can have interactive conversations about images or videos. This is known as Conversational AI — a technology that lets users talk to systems much like chatbots, virtual assistants, or agents.

While the first iteration of the app was great, the output still lacked some details. For example, if you upload an image of a dog, the description might be something like "a dog sitting on a rock in front of a pool," and the app might produce something close but miss additional details such as the dog's breed, the time of the day, or location.

![The interface for an app with an uploaded photo of a golden retriever puppy on the left and an audio extraction on the right represented by sound waves.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/1-app-interface.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The aim here is simply to build a more advanced version of the previously built app so that it not only describes images but also provides more in-depth information and engages users in meaningful conversations about them.

We'll use LLaVA, a model that combines understanding images and conversational capabilities. After building our tool, we'll explore multimodal models that can handle images, videos, text, audio, and more, all at once to give you even more options and easiness for your applications.

## Visual Instruction Tuning and LLaVA

We are going to look at visual instruction tuning and the multimodal capabilities of LLaVA. We'll first explore how visual instruction tuning can enhance the large language models to understand and follow instructions that include visual information. After that, we'll dive into LLaVA, which brings its own set of tools for image and video processing.

### Visual Instruction Tuning

Visual instruction tuning is a technique that helps large language models (LLMs) understand and follow instructions based on visual inputs. This approach connects language and vision, enabling AI systems to understand and respond to human instructions that involve both text and images. For example, Visual IT enables a model to describe an image or answer questions about a scene in a photograph. This fine-tuning method makes the model more capable of handling these complex interactions effectively.

There's a new training approach called LLaVAR that has been developed, and you can think of it as a tool for handling tasks related to PDFs, invoices, and text-heavy images. It's pretty exciting, but we won't dive into that since it is outside the scope of the app we're making.

### Examples of Visual Instruction Tuning Datasets

To build good models, you need good data — rubbish in, rubbish out. So, here are two datasets that you might want to use to train or evaluate your multimodal models. Of course, you can always add your own datasets to the two I'm going to mention.

**Vision-CAIR**

- Instruction datasets: English;
- Multi-task: Datasets containing multiple tasks;
- Mixed dataset: Contains both human and machine-generated data.

![Vision-CAIR](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part2/2-vision-cair.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Vision-CAIR provides a high-quality, well-aligned image-text dataset created using conversations between two bots. This dataset was initially introduced in a paper titled "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models," and it provides more detailed image descriptions and can be used with predefined instruction templates for image-instruction-answer fine-tuning.

**LLaVA Visual Instruct 150K**

- Instruction datasets: English;
- Multi-task: Datasets containing multiple tasks;
- Mixed dataset: Contains both human and machine-generated data.

LLaVA Visual Instruct 150K is a set of GPT-generated multimodal instruction-following data. It is built for visual instruction tuning and aims to achieve GPT-4 level vision and language capabilities.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Integrating Image-To-Text And Text-To-Speech Models (Part 1)](https://www.smashingmagazine.com/2024/07/integrating-image-to-text-and-text-to-speech-models-part1/)  
> for internal educational use only (non-profit).

# Integrating Image-To-Text And Text-To-Speech Models (Part 1)

Audio descriptions involve narrating contextual visual information in images or videos, improving user experiences, especially for those who rely on audio cues.

At the core of audio description technology are two crucial components: the description and the audio. The description involves understanding and interpreting the visual content of an image or video, which includes details such as actions, settings, expressions, and any other relevant visual information. Meanwhile, the audio component converts these descriptions into spoken words that are clear, coherent, and natural-sounding.

So, here's something we can do: build an app that generates and announces audio descriptions. The app can integrate a pre-trained vision-language model to analyze image inputs, extract relevant information, and generate accurate descriptions. These descriptions are then converted into speech using text-to-speech technology, providing a seamless and engaging audio experience.

![Image audio captioning app. The application provides audio descriptions for images. A file upload field is displayed on the left, and a space for generated audio is displayed on the right.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/1-image-audio-captioning-app.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

By the end of this tutorial, you will gain a solid grasp of the components that are used to build audio description tools. We'll spend time discussing what VLM and TTS models are, as well as many examples of them and tooling for integrating them into your work.

When we finish, you will be ready to follow along with a second tutorial in which we level up and build a chatbot assistant that you can interact with to get more insights about your images or videos.

## Vision-Language Models: An Introduction

VLMs are a form of artificial intelligence that can understand and learn from visuals and linguistic modalities.

![Image illustrating four different tasks a vision-language model can handle, such as Visual QA, object localization in an image.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/integrating-image-to-text-and-text-to-speech-models-part1/2-four-tasks-vision-language-model-can-handle.jpg)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

They are trained on vast amounts of data that include images, videos, and text, allowing them to learn patterns and relationships between these modalities. In simple terms, a VLM can look at an image or video and generate a corresponding text description that accurately matches the visual content.

VLMs typically consist of three main components:

1. An image model that extracts meaningful visual information,
2. A text model that processes and understands natural language,
3. A fusion mechanism that combines the representations learned by the image and text models, enabling cross-modal interactions.

Generally speaking, the image model — also known as the vision encoder — extracts visual features from input images and maps them to the language model's input space, creating visual tokens. The text model then processes and understands natural language by generating text embeddings. Lastly, these visual and textual representations are combined through the fusion mechanism, allowing the model to integrate visual and textual information.

VLMs bring a new level of intelligence to applications by bridging visual and linguistic understanding. Here are some of the applications where VLMs shine:

- **Image captions**: VLMs can provide automatic descriptions that enrich user experiences, improve searchability, and even enhance visuals for vision impairments.
- **Visual answers to questions**: VLMs could be integrated into educational tools to help students learn more deeply by allowing them to ask questions about visuals they encounter in learning materials, such as complex diagrams and illustrations.
- **Document analysis**: VLMs can streamline document review processes, identifying critical information in contracts, reports, or patents much faster than reviewing them manually.
- **Image search**: VLMs could open up the ability to perform reverse image searches. For example, an e-commerce site might allow users to upload image files that are processed to identify similar products that are available for purchase.
- **Content moderation**: Social media platforms could benefit from VLMs by identifying and removing harmful or sensitive content automatically before publishing it.
- **Robotics**: In industrial settings, robots equipped with VLMs can perform quality control tasks by understanding visual cues and describing defects accurately.

This is merely an overview of what VLMs are and the pieces that come together to generate audio descriptions. To get a clearer idea of how VLMs work, let's look at a few real-world examples that leverage VLM processes.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Generating Real-Time Audio Sentiment Analysis With AI](https://www.smashingmagazine.com/2023/09/generating-real-time-audio-sentiment-analysis-ai/)  
> for internal educational use only (non-profit).

# Generating Real-Time Audio Sentiment Analysis With AI

In the previous article, we developed a sentiment analysis tool that could detect and score emotions hidden within audio files. We're taking it to the next level in this article by integrating real-time analysis and multilingual support. Imagine analyzing the sentiment of your audio content in real-time as the audio file is transcribed. In other words, the tool we are building offers immediate insights as an audio file plays.

So, how does it all come together? Meet Whisper and Gradio — the two resources that sit under the hood. Whisper is an advanced automatic speech recognition and language detection library. It swiftly converts audio files to text and identifies the language. Gradio is a UI framework that happens to be designed for interfaces that utilize machine learning, which is ultimately what we are doing in this article. With Gradio, you can create user-friendly interfaces without complex installations, configurations, or any machine learning experience — the perfect tool for a tutorial like this.

By the end of this article, we will have created a fully-functional app that:

- Records audio from the user's microphone,
- Transcribes the audio to plain text,
- Detects the language,
- Analyzes the emotional qualities of the text, and
- Assigns a score to the result.

Note: You can peek at the final product in the live demo.

## Automatic Speech Recognition And Whisper

Let's delve into the fascinating world of automatic speech recognition and its ability to analyze audio. In the process, we'll also introduce Whisper, an automated speech recognition tool developed by the OpenAI team behind ChatGPT and other emerging artificial intelligence technologies. Whisper has redefined the field of speech recognition with its innovative capabilities, and we'll closely examine its available features.

## Automatic Speech Recognition (ASR)

ASR technology is a key component for converting speech to text, making it a valuable tool in today's digital world. Its applications are vast and diverse, spanning various industries. ASR can efficiently and accurately transcribe audio files into plain text. It also powers voice assistants, enabling seamless interaction between humans and machines through spoken language. It's used in myriad ways, such as in call centers that automatically route calls and provide callers with self-service options.

By automating audio conversion to text, ASR significantly saves time and boosts productivity across multiple domains. Moreover, it opens up new avenues for data analysis and decision-making.

That said, ASR does have its fair share of challenges. For example, its accuracy is diminished when dealing with different accents, background noises, and speech variations — all of which require innovative solutions to ensure accurate and reliable transcription. The development of ASR systems capable of handling diverse audio sources, adapting to multiple languages, and maintaining exceptional accuracy is crucial for overcoming these obstacles.

## Whisper: A Speech Recognition Model

Whisper is a speech recognition model also developed by OpenAI. This powerful model excels at speech recognition and offers language identification and translation across multiple languages. It's an open-source model available in five different sizes, four of which have an English-only variant that performs exceptionally well for single-language tasks.

What sets Whisper apart is its robust ability to overcome ASR challenges. Whisper achieves near state-of-the-art performance and even supports zero-shot translation from various languages to English. Whisper has been trained on a large corpus of data that characterizes ASR's challenges. The training data consists of approximately 680,000 hours of multilingual and multitask supervised data collected from the web.

The model is available in multiple sizes. The following table outlines these model characteristics:

- `tiny.en`
- `base.en`
- `small.en`
- `medium.en`

For developers working with English-only applications, it's essential to consider the performance differences among the .en models — specifically, tiny.en and base.en, both of which offer better performance than the other models.

Whisper utilizes a Seq2seq (i.e., transformer encoder-decoder) architecture commonly employed in language-based models. This architecture's input consists of audio frames, typically 30-second segment pairs. The output is a sequence of the corresponding text. Its primary strength lies in transcribing audio into text, making it ideal for "audio-to-text" use cases.

![Diagram of Whisper's ASR architecture](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/generating-real-time-audio-sentiment-analysis-ai/1-ai-real-time-audio-analysis-whisper-architecure.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Using AI To Detect Sentiment In Audio Files](https://www.smashingmagazine.com/2023/06/ai-detect-sentiment-audio-files/)  
> for internal educational use only (non-profit).

# Using AI To Detect Sentiment In Audio Files

I don't know if you've ever used Grammarly's service for writing and editing content. But if you have, then you no doubt have seen the feature that detects the tone of your writing.

It's an extremely helpful tool! It can be hard to know how something you write might be perceived by others, and this can help affirm or correct you. Sure, it's some algorithm doing the work, and we know that not all AI-driven stuff is perfectly accurate. But as a gut check, it's really useful.

![Grammarly tone detector](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/ai-detect-sentiment-audio-files/ai-audio-grammarly.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

Now imagine being able to do the same thing with audio files. How neat would it be to understand the underlying sentiments captured in audio recordings? Podcasters especially could stand to benefit from a tool like that, not to mention customer service teams and many other fields.

> An audio sentiment analysis has the potential to transform the way we interact with data.

That's what we are going to accomplish in this article.

![A screenshot of the audio sentiment analyzer built in this tutorial](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/ai-detect-sentiment-audio-files/ai-audio-ui.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The idea is fairly straightforward:

- Upload an audio file.
- Convert the content from speech to text.
- Generate a score that indicates the type of sentiment it communicates.

But how do we actually build an interface that does all that? I'm going to introduce you to three tools and show how they work together to create an audio sentiment analyzer.

## But First: Why Audio Sentiment Analysis?

By harnessing the capabilities of an audio sentiment analysis tool, developers and data professionals can uncover valuable insights from audio recordings, revolutionizing the way we interpret emotions and sentiments in the digital age. Customer service, for example, is crucial for businesses aiming to deliver personable experiences. We can surpass the limitations of text-based analysis to get a better idea of the feelings communicated by verbal exchanges in a variety of settings, including:

- **Call centers**: Call center agents can gain real-time insights into customer sentiment, enabling them to provide personalized and empathetic support.
- **Voice assistants**: Companies can improve their natural language processing algorithms to deliver more accurate responses to customer questions.
- **Surveys**: Organizations can gain valuable insights and understand customer satisfaction levels, identify areas of improvement, and make data-driven decisions to enhance overall customer experience.

And that is just the tip of the iceberg for one industry. Audio sentiment analysis offers valuable insights across various industries. Consider healthcare as another example. Audio analysis could enhance patient care and improve doctor-patient interactions. Healthcare providers can gain a deeper understanding of patient feedback, identify areas for improvement, and optimize the overall patient experience.

Market research is another area that could benefit from audio analysis. Researchers can leverage sentiments to gain valuable insights into a target audience's reactions that could be used in everything from competitor analyses to brand refreshes with the use of audio speech data from interviews, focus groups, or even social media interactions where audio is used.

I can also see audio analysis being used in the design process. Like, instead of asking stakeholders to write responses, how about asking them to record their verbal reactions and running those through an audio analysis tool? The possibilities are endless!

> **Note:** The following article is reproduced verbatim from  
> SingleStore Team, *SingleStore* (2025):  
> [Claude 3 Multimodal With LlamaIndex and SingleStore](https://www.singlestore.com/blog/claude-3-multimodal-with-llamaindex-and-singlestore/)  
> for internal educational use only (non-profit).

# Claude 3 Multimodal With LlamaIndex and SingleStore

### Claude 3

### Claude 3 multimodal tutorial

### Multimodal scenario using an image

### Index into a vector store

### Multimodal LLMs: Industry specific use cases

Large Language Models (LLMs) have been the talk of the town due to their various capabilities that can almost replace humans. While models from OpenAI and Google have made a significant impact, there is another contender for the throne: Anthropic.

Anthropic is an AI company that is building some amazing models to compete with popular models from major providers. Anthropic recently released Claude 3, a multimodal LLM that is getting attention in the AI market. We'll take a closer look at how the Claude 3 model works, and its multimodal capabilities.

What is a multimodal model?

We all agree that the LLMs have limitations in terms of creating/providing accurate information, which can be tracked through different techniques. But, what if we have models that take not just natural language as input, but also images and videos to provide accurate information for users? Isn't this a fantastic idea? Models that take more than one form of input and understand different modalities (text, image, audio, video, etc.) are known as multimodal models. Google, OpenAI and Anthropic each have their multimodal models with better capabilities already in the market.

OpenAI's GPT-4 Vision, Google's Gemini and Anthropic's Claude 3 series are some notable examples of multimodal models that are revolutionizing the AI industry.

Anthropic's Claude 3 is a family of three AI models: Claude 3 Opus, Sonnet and Haiku. These models have the multimodal capabilities that closely compete with GPT-4 and Gemini-ultra. These Claude 3 models have the right balance of cost, safety and performance.

The image displays a comparison chart of various LLMs' performance across different tasks, including math problem-solving, coding and knowledge-based question answering. The models include Claude variants, GPT-4, GPT-3.5 and Gemini, with their respective scores in percentages. As you can see, the Claude 3 models perform robustly across a range of tasks, often outperforming other AI models presented in the comparison.

Claude's Haiku is the most affordable — yet effective — model that can be used for real-time response generation in customer service use cases, content moderation, etc. with higher efficiency.

You can easily access Claude models from Anthropic's official website to see how it works for your queries. You can transcribe handwritten notes, understanding how objects are used and complex equations.

I tried an example by sharing an image I found online to see if the Claude model can respond accurately.

Amazing, that is an impressive response with all the minute details about the image shared.

We will be using the SingleStore database as our vector store, and SingleStore Notebooks (just like Jupyter notebooks) to run all our code.

Also we will be using LlamaIndex, an AI framework to build LLM-powered applications. It provides a complete toolkit for developers building AI applications by providing them with different APIs and the ability to integrate data sources, various file formats, databases to store vector data, applications, etc.

We will be importing the libraries required, running the multimodal model from Anthropic [claude-3-haiku-20240307], storing the data in the SingleStore and retrieving it to see the power of multimodality through text and image.

Let's explore the capabilities of the model on text and vision tasks. Get started by signing up for SingleStore. We are going to use SingleStore's Notebook feature to run our commands, and SingleStore as the database to store our data.

Once you sign up for the first time, you need to create a workspace and a database to store your data. Later in this tutorial, you will also see how you can store data as embeddings.

Creating a database is easy. Just go to your workspace and click Create Database as shown here.

Go back to the main SingleStore dashboard. Click on Develop to create a Notebook.

Next, click on New Notebook > New Notebook.

Create a new Notebook and name it whatever you'd like.

Now, get started with the notebook code shared here. Make sure to select your workspace and database you created.

Start adding all the code shown below step-by-step in the notebook you just created.

Install LlamaIndex and other libraries if required.

Python

Set API keys. Add both OpenAI and Anthropic API keys.

Python

Let's use the Claude 3's Haiku model for chat completion

Python

Let's download the image first

Python

Load the image

Python

Test the query on the image

Python

You can test many image examples

Python

Ask the model to describe the mentioned image

Python

In this section, we'll show you how to use Claude 3 to build a RAG pipeline over image data. We first use Claude to extract text from a set of images, then index the text with an embedding model. Finally, we build a query pipeline over the data.

Python

Testing the query

Python

The complete notebook code is here for your reference.

There are many use cases where multimodal plays an important role and enhances LLM applications — here are the most top of mind:

Multimodal models are revolutionizing the AI industry, and we will only see more powerful models in the future. While unimodal models have the limitations of processing only one type of input (mostly text), multimodal models gain an added advantage here by processing various input types (text, image, audio, video, etc). Multimodal models are paving the future for building LLM-powered applications for more context and high performance.

Start your free SingleStore trial today.

- Healthcare. Since multimodal models can process various input types, these can be highly effective in the healthcare industry. The models can take prescriptions, procedures,  X-rays and other imagery as input, combining both the text and image to come up with high quality responses and remedies.
- Customer support chatbots. Multimodal models can be highly effective as sometimes  text might not be sufficient in resolving an issue, so companies can ask for screenshots and images. This way, using a multimodal-powered application can enhance answering capabilities.
- Social media sentiment analysis. This can be cleverly done by multimodal-powered applications since they can identify images and text as well.
- eCommerce. Product search and recommendations can be simplified using multimodal models, since they get more contextual information from the user with many input capabilities.

> **Note:** The following article is reproduced verbatim from  
> Google Team, *Google Cloud* (2025):  
> [Get multimodal embeddings](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)  
> for internal educational use only (non-profit).

# Get multimodal embeddings

## Supported models

## Best practices

## API usage

### API limits

### Before you begin

### Java

To see an example of multimodal embeddings, run the "Intro to multimodal embeddings" notebook in one of the following environments:

The multimodal embeddings model generates 1408-dimension vectors based on the input you provide, which can include a combination of image, text, and video data. The embedding vectors can then be used for subsequent tasks like image classification or video content moderation.

The image embedding vector and text embedding vector are in the same semantic space with the same dimensionality. Consequently, these vectors can be used interchangeably for use cases like searching image by text, or searching video by image.

For text-only embedding use cases, we recommend using the Vertex AI text-embeddings API instead. For example, the text-embeddings API might be better for text-based semantic search, clustering, long-form document analysis, and other text retrieval or question-answering use cases. For more information, see Get text embeddings.

## Supported models

You can get multimodal embeddings by using the following model:

- **multimodalembedding**

## Best practices

Consider the following input aspects when using the multimodal embeddings model:

- **Text in images** - The model can distinguish text in images, similar to optical character recognition (OCR). If you need to distinguish between a description of the image content and the text within an image, consider using prompt engineering to specify your target content.
- **Image quality** - Higher quality images generally produce better embeddings
- **Content relevance** - Ensure the input content is relevant to your use case

## API usage

The multimodal embeddings API allows you to generate embeddings from various input types including images, text, and video data.

### API limits

- Maximum input size varies by modality
- Rate limiting applies based on your quota
- Concurrent request limits may apply

### Before you begin

1. Set up your Google Cloud project
2. Enable the Vertex AI API
3. Configure authentication
4. Install the required client libraries

### Implementation examples

The API supports multiple programming languages including Java, Python, and Node.js for generating multimodal embeddings.

> **Note:** The following article is reproduced verbatim from  
> Cohere Team, *Cohere* (2025):  
> [Multimodal LLMs explained: Different data sources, smarter AI](https://cohere.com/blog/multimodal-llm)  
> for internal educational use only (non-profit).

# Multimodal LLMs explained: Different data sources, smarter AI

Learn how multimodal LLMs combine text, images, and other diverse data sources to help improve AI performance across many different applications and industries.

![What Is a Multimodal LLM?](https://cohere.com/_next/image?url=https%3A%2F%2Fcohere-ai.ghost.io%2Fcontent%2Fimages%2F2025%2F06%2FMultimodal-LLM_v2.png&w=1920&q=75)
<sub>Source: *Cohere*, Cohere Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> Neptune.ai Team, *Neptune.ai* (2025):  
> [Multimodal Large Language Models](https://neptune.ai/blog/multimodal-large-language-models)  
> for internal educational use only (non-profit).

# Multimodal Large Language Models

### TL;DR

## What is a multimodal large language model?

### Why do we need multimodal LLMs?

### How do multimodal LLMs work?

## Examples of multimodal LLMs

### Microsoft: Kosmos-1

#### Architecture and training

#### Performance

### DeepMind: Flamingo

### LLM Guardrails: Secure and Controllable Deployment

### LLaVA

### Google: PaLM-E

#### Architecture and training

#### Performance

## Challenges, limitations, and future directions of MLLMs

Multimodal Large Language Models (MLLMs) process data from different modalities like text, audio, image, and video.

Compared to text-only models, MLLMs achieve richer contextual understanding and can integrate information across modalities, unlocking new areas of application. Prime use cases of MLLMs include content creation, personalized recommendations, and human-machine interaction.

Examples of MLLMs that process image and text data include Microsoft's Kosmos-1, DeepMind's Flamingo, and the open-source LLaVA. Google's PaLM-E additionally handles information about a robot's state and surroundings.

Combining different modalities and dealing with different types of data comes with some challenges and limitations, such as alignment of heterogeneous data, inherited biases from pre-trained models, and lack of robustness.

How would you translate this sentence: "The glasses are broken." into French: "Les verres sont cases." or  "Les lunettes sont cases."? What if you have an image? Will you be able to choose the correct translation? As humans, we use different modalities daily to enhance communication. Machines can do the same.

While Large Language Models (LLMs) have shown impressive capabilities in understanding complex text, they are limited to a single data modality. However, many tasks span several modalities.

This article explores Multimodal Large Language Models, exploring their core functionalities, challenges, and potential for various machine-learning domains.

Let's break down the concept of Multimodal Large Language Models (MLLMs) by first understanding the terms "modal" and "multimodal:"

"Modal" refers to a particular way of communicating or perceiving information. It's like a channel through which we receive and express ourselves. Some of the common modalities are:

"Multimodal" refers to incorporating various modalities to create a richer understanding of the task, e.g., as on a website or in a blog post that integrates text with visuals.

MLLMs can process not just text but other modalities as well. They are trained on samples containing different modalities, which allows them to develop joint representations and utilize multimodal information to solve tasks.

Many industries heavily rely on multimodality, particularly those that handle a blend of data modalities. For example, MLLMs can be used in a healthcare setting to process patient reports comprising doctor notes (text), treatment plans (structured data), and X-rays or MRI scans (images).

MLLMs process and integrate information from different modalities (i.e., text, image, video, and audio), essential to solving many tasks. Some prominent applications are:

A typical multimodal LLM has three primary modules:

Kosmos-1 (GitHub) is a multimodal LLM created by Microsoft for natural language and perception-intensive tasks. It can perform visual dialogue, visual explanation, visual question answering, image captioning, math equations, OCR, and zero-shot image classification with and without descriptions.

> **Note:** The following article is reproduced verbatim from  
> Sebastian Raschka, *Sebastian Raschka* (2025):  
> [Understanding Multimodal LLMs](https://magazine.sebastianraschka.com/p/understanding-multimodal-llms)  
> for internal educational use only (non-profit).

# Understanding Multimodal LLMs

### An introduction to the main techniques and latest models

## 1. Use cases of multimodal LLMs

## 2. Common approaches to building multimodal LLMs

### 2.1 Method A: Unified Embedding Decoder Architecture

#### 2.1.1 Understanding Image encoders

#### 2.1.2 The role of the linear projection module

#### 2.1.3 Image vs text tokenization

### 2.2 Method B: Cross-Modality Attention Architecture

## 3. Unified decoder and cross-attention model training

## 4. Recent multimodal models and methods

### 4.1 The Llama 3 Herd of Models

### 4.2 Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models

### 4.3 NVLM: Open Frontier-Class Multimodal LLMs

### 4.4 Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution

### 4.5 Pixtral 12B

### 4.6 MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning

### 4.7 Aria: An Open Multimodal Native Mixture-of-Experts Model

### 4.8 Baichuan-Omni

### 4.9 Emu3: Next-Token Prediction is All You Need

### 4.10 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation

## Conclusion

It was a wild two months. There have once again been many developments in AI research, with two Nobel Prizes awarded to AI and several interesting research papers published.

Among others, Meta AI released their latest Llama 3.2 models, which include open-weight versions for the 1B and 3B large language models and two multimodal models.

In this article, I aim to explain how multimodal LLMs function. Additionally, I will review and summarize roughly a dozen other recent multimodal papers and models published in recent weeks (including Llama 3.2) to compare their approaches.

What are multimodal LLMs? As hinted at in the introduction, multimodal LLMs are large language models capable of processing multiple types of inputs, where each "modality" refers to a specific type of data—such as text (like in traditional LLMs), sound, images, videos, and more. For simplicity, we will primarily focus on the image modality alongside text inputs.

A classic and intuitive application of multimodal LLMs is image captioning: you provide an input image, and the model generates a description of the image, as shown in the figure below.

Of course, there are many other use cases. For example, one of my favorites is extracting information from a PDF table and converting it into LaTeX or Markdown.

There are two main approaches to building multimodal LLMs:

- **Method A: Unified Embedding Decoder Architecture approach**
- **Method B: Cross-modality Attention Architecture approach**

As shown in the figure above, the Unified Embedding-Decoder Architecture utilizes a single decoder model, much like an unmodified LLM architecture such as GPT-2 or Llama 3.2. In this approach, images are converted into tokens with the same embedding size as the original text tokens, allowing the LLM to process both text and image input tokens together after concatenation.

The Cross-Modality Attention Architecture employs a cross-attention mechanism to integrate image and text embeddings directly within the attention layer.

In the following sections, we will explore how these methods work on a conceptual level. Then, we will look at recent research papers on multimodal LLMs to see how they are applied in practice.

Let's begin with the unified embedding decoder architecture, illustrated again in the figure below.

In the unified embedding-decoder architecture, an image is converted into embedding vectors, similar to how input text is converted into embeddings in a standard text-only LLM.

For a typical text-only LLM that processes text, the text input is usually tokenized (e.g., using Byte-Pair Encoding) and then passed through an embedding layer, as shown in the figure below.

Analogous to the tokenization and embedding of text, image embeddings are generated using an image encoder module (instead of a tokenizer), as shown in the figure below.

> **Note:** The following article is reproduced verbatim from  
> Giancarlo Mori, *Giancarlo Mori* (2025):  
> [The Future of AI is Multimodal](https://giancarlomori.substack.com/p/the-future-of-ai-is-multimodal)  
> for internal educational use only (non-profit).

# The Future of AI is Multimodal

### Multimodal AI brings machines a step closer to mirroring the intricacies of human intelligence.

## Decoding Multimodal AI: Concept and Components

### Unimodal vs. Multimodal AI

### Key Components of Multimodal AI

## The Superior Edge of Multimodal AI

## Case Studies: Multimodal AI in Action

## Embracing a Multimodal Future

From AI's early days of simple, rule-based algorithms, it has journeyed through remarkable advancements, each milestone bringing it a step closer to mirroring the intricacies of human intelligence. A significant leap in this journey is the emergence of multimodal AI - a paradigm shift from the traditional unimodal systems that once dominated the field.

Taking a step back, let's start with a definition of "multimodal AI": Multi-modal AI refers to artificial intelligence systems that can process and interpret data from multiple different modes or types of input, such as text, images, audio, and video. These systems can analyze and understand the information from these diverse sources simultaneously, enabling more comprehensive and nuanced decision-making and interactions. For example, a multi-modal AI could analyze a video by understanding the spoken words (audio), recognizing objects and actions (visual), and interpreting the text in subtitles or captions (textual), all at the same time.

Unlike its predecessors, which relied on single data input types like text or images, multimodal AI represents a more holistic approach to machine learning. By integrating multiple types of data inputs – text, images, and sounds – it offers a richer, more nuanced understanding of the world. This integration mirrors the human ability to process and interpret diverse sensory information simultaneously, marking a crucial step towards developing AI systems that can interact with and understand the world in a more human-like manner.

The rise of multimodal AI is not just a technical improvement; it's a transformation that paves the way for more sophisticated, context-aware, and intuitive AI systems. These advancements hold the promise of revolutionizing various sectors, from healthcare and education to customer service and security, by providing solutions that are more aligned with the complexities of human communication and understanding.

To fully grasp the essence of multimodal AI, it's essential to first understand what sets it apart from traditional unimodal AI systems and the key components that constitute its framework.

Unimodal AI systems, the initial stepping stones in AI development, are designed to understand and process information from a single type of data input, be it textual, visual, or auditory. For instance, a text-based AI excels in processing and generating language, while an image recognition AI specializes in analyzing visual data. However, their scope is limited to their specific domain of data, restricting their understanding and application to scenarios that require integration of multiple forms of data.

Multimodal AI, in contrast, breaks through these limitations by simultaneously processing and interpreting diverse types of data inputs. This approach mirrors the human cognitive process, where multiple senses are utilized to gain a comprehensive understanding of our surroundings. In essence, multimodal AI systems can integrate text, images, sounds, and potentially other sensory data, providing a more holistic and nuanced view of their input.

The sophisticated functionality of multimodal AI systems is underpinned by three primary components:

- **Input Module**: This is where the multimodal AI receives and processes different types of data. In this stage, each data type, whether it's text, image, or audio, is processed using specialized sub-models tailored for each modality.

- **Fusion Module**: The crux of multimodal AI lies in its ability to fuse the processed data from various modalities. This fusion can happen early in the process (early fusion), integrating raw data, or at a later stage (late fusion), combining the outputs of each sub-model. The fusion process is crucial as it synthesizes the diverse information into a unified representation that the AI can analyze and interpret.

- **Output Module**: Based on the fused data, the AI system generates outputs that are more context-aware and accurate. The output could be in various forms – a decision, a prediction, a synthesized image or text, or even a combination of these, depending on the application and the input data.

One of the most profound advantages of multimodal AI is its enhanced capability to interpret context with a depth and complexity that unimodal systems cannot achieve. By processing and analyzing multiple data types simultaneously, these systems can discern subtleties and nuances in information, leading to a more comprehensive understanding of their environment. For example, in a social media setting, while a unimodal text-based AI might interpret a post's sentiment based on language alone, a multimodal system can enrich this understanding by analyzing the accompanying images and audio tones. This holistic approach enables the AI to capture the post's true intent, whether it's irony, sarcasm, or genuine sentiment, which might otherwise be missed by a text-only analysis.

The integration of diverse data types in multimodal AI is not just about gathering more information; it's about creating a synergistic effect that leads to more informed, accurate, and nuanced decision-making. In critical sectors like healthcare, this ability can have life-altering implications. Consider a multimodal diagnostic system that evaluates a patient's condition by integrating data from various sources – medical imaging provides visual insights, lab results offer quantitative analysis, and patient history adds contextual depth. This comprehensive approach leads to diagnoses that are not only more accurate but also tailored to the individual patient, significantly improving treatment outcomes.

Another key benefit of multimodal AI is its adaptability to complex, real-world environments. Traditional unimodal systems often struggle in situations where data is incomplete, ambiguous, or noisy. Multimodal AI, by contrast, can leverage its multi-sensory data processing to fill in the gaps, make sense of ambiguity, and filter out noise. For instance, in an autonomous vehicle, a unimodal vision-based system might be hindered by poor visibility conditions. However, a multimodal system that combines visual data with radar and audio signals can navigate more reliably, ensuring safety even in challenging conditions.

Furthermore, multimodal AI is bridging the gap between human-AI interactions. By processing language, visual cues, and even emotional tones, these systems can engage with users in a way that feels more natural and human-like. This capability is transforming customer service bots, virtual assistants, and interactive educational tools into entities that understand and respond not just to what we say, but also to how we say it and what we show.

To illustrate the power of multimodal AI, let's explore a few examples:

- **Healthcare Diagnostics**: Multimodal AI systems are revolutionizing diagnostics by integrating patient history, imaging, and genomic data, leading to more accurate and personalized treatment plans.

- **Retail Personalization**: In retail, multimodal AI analyzes customer behavior, preferences, and feedback across various channels – textual reviews, visual content, and browsing patterns – to tailor product recommendations and enhance the shopping experience.

- **Customer Service Bots**: AI-powered customer service agents now respond to queries not just by analyzing textual data but also by interpreting the customer's tone and sentiment through voice or even facial expressions, offering a more empathetic and effective response.

- **Advanced Educational Tools**: In the field of education, multimodal AI is bringing about a revolution in personalized learning experiences. By integrating textual, visual, and auditory data, these systems can adapt to different learning styles.

- **Enhanced Security and Surveillance**: Security systems powered by multimodal AI are changing the landscape of public safety and private security. These systems can analyze video footage, audio recordings, and sensor data to detect potential threats with greater accuracy.

- **Smart Home Integration**: In the realm of smart homes, multimodal AI is enabling more intuitive and responsive interactions between users and their home environments. By processing voice commands, recognizing facial expressions, and understanding physical gestures, these AI systems can control home appliances, adjust environmental settings, and even detect unusual behaviors or emergencies.

- **Automotive Innovations**: The automotive industry is leveraging multimodal AI to enhance the driving experience and safety features in vehicles. Modern cars equipped with AI can process visual data from cameras, audio signals, and sensor information to provide advanced driver assistance, hazard warnings, and even fully autonomous driving capabilities.

- **Agricultural Optimization**: Multimodal AI is also making significant strides in agriculture. By combining data from satellite images, soil sensors, and weather forecasts, these systems can offer farmers insights into crop health, soil quality, and optimal harvesting times.

- **Financial Services and Fraud Detection**: In financial services, multimodal AI is being used for enhanced risk assessment and fraud detection. By analyzing transaction data, customer communication, and behavioral patterns, these AI systems can identify fraudulent activities and assess credit risks with higher precision.

- **Creative Industries and Media**: In creative industries and media, multimodal AI is transforming content creation and curation. AI systems can analyze text, images, and user preferences to generate personalized content recommendations, assist in digital art creation, and even help in writing scripts and composing music.

As we stand on the brink of a new era in artificial intelligence, the promise of multimodal AI is not just in its advanced technology, but in its potential to fundamentally transform our interaction with machines and, by extension, with the world around us. The journey ahead for multimodal AI is teeming with possibilities, challenges, and the promise of a more interconnected and intuitive future.

The development and application of multimodal AI are set to usher in a new age of technological sophistication. We are likely to witness AI systems that not only understand and process complex data from multiple sources but also interact with us in ways that are profoundly human-like. This evolution will bring about a paradigm shift in numerous fields, from healthcare and education to entertainment and security, enhancing both the efficiency and quality of services.

The broader impact of multimodal AI on society is immense. It holds the potential to solve some of the most pressing challenges we face today, be it in advancing healthcare, personalizing education, or contributing to environmental sustainability. However, this transformative power comes with a significant responsibility to ensure that the development of AI is guided by ethical principles and a commitment to the greater good. It is imperative to navigate the challenges of privacy, bias, and ethical use to maintain trust and acceptance of AI technologies.

The future of multimodal AI is a canvas of limitless possibilities. It invites us to reimagine the role of AI in our lives, not as a mere tool, but as a partner in shaping a more intelligent, empathetic, and connected world. The journey ahead is as exciting as it is crucial, and it calls for a collective effort to harness the power of AI in a way that enriches our lives and uplifts our societies.

## Sources

<Card title="Reference Materials">
  <ul>
    <li><strong>Anthropic Claude Documentation:</strong> <a href="https://docs.anthropic.com/en/docs/overview">https://docs.anthropic.com/en/docs/overview</a></li>
    <li><strong>Claude Vision Guide:</strong> <a href="https://docs.anthropic.com/en/docs/vision">https://docs.anthropic.com/en/docs/vision</a></li>
    <li><strong>Multimodal AI Research:</strong> <a href="https://arxiv.org/abs/2303.08774">https://arxiv.org/abs/2303.08774</a></li>
    <li><strong>Computer Vision Fundamentals:</strong> <a href="https://arxiv.org/abs/1409.0575">https://arxiv.org/abs/1409.0575</a></li>
    <li><strong>Multimodal UX Design:</strong> <a href="https://www.interaction-design.org/literature/topics/multimodal-interfaces">https://www.interaction-design.org/literature/topics/multimodal-interfaces</a></li>
  </ul>
</Card>

## Figures

<Card title="Multimodal Processing Pipeline">
  <Frame>
    <img src="/images/multimodal-pipeline.png" alt="Diagram showing how multiple modalities are processed and fused in AI systems" />
  </Frame>
</Card>

<Card title="Modality Interaction Patterns">
  <Frame>
    <img src="/images/modality-interactions.png" alt="Visualization of different ways modalities can interact and complement each other" />
  </Frame>
</Card>

