---
title: "AI UX Behavior & Design Patterns"
slug: "modules-ai-ux-behavior"
updatedAt: "2025-08-18"
tags: [module, ux, design, ai-patterns, claude, anthropic]
---

# AI UX Behavior & Design Patterns

> **Synthesis**: Understanding how users interact with AI systems and designing intuitive, trustworthy, and effective user experiences for AI-powered applications, with special focus on Claude's conversational capabilities.

## Why it's important for designers to know

AI UX behavior is fundamentally different from traditional software UX. Users interact with AI systems through conversation, expect intelligent responses, and need clear feedback about system capabilities and limitations. Designers must understand:

- **Conversational Design**: How to structure AI interactions that feel natural and productive
- **Trust & Transparency**: Building user confidence through clear communication about AI capabilities
- **Error Handling**: Graceful degradation when AI fails or provides unexpected responses
- **Progressive Disclosure**: Revealing AI capabilities gradually to avoid overwhelming users
- **Feedback Loops**: Providing clear signals about what the AI is doing and thinking

<Callout type="info">
  **Claude's UX Advantages**: Claude's constitutional AI principles and natural conversational abilities make it particularly well-suited for creating trustworthy, helpful user experiences.
</Callout>

## Claude's UX Strengths

### Natural Conversation Flow

<Card title="Claude's Conversational Design">
  <p>Claude excels at natural, helpful conversations that feel more human-like than traditional chatbots:</p>
  
  <h4>Conversational Capabilities:</h4>
  <ul>
    <li><strong>Context Awareness:</strong> Maintains conversation context across multiple turns</li>
    <li><strong>Natural Language:</strong> Understands and responds in natural, conversational language</li>
    <li><strong>Helpful Tone:</strong> Consistently helpful and supportive in interactions</li>
    <li><strong>Clarification Skills:</strong> Asks for clarification when needed</li>
    <li><strong>Proactive Assistance:</strong> Offers helpful suggestions and alternatives</li>
  </ul>
  
  <h4>UX Benefits:</h4>
  <ul>
    <li><strong>Reduced Friction:</strong> Users don't need to learn specific commands or syntax</li>
    <li><strong>Increased Trust:</strong> Natural conversation builds user confidence</li>
    <li><strong>Better Engagement:</strong> More engaging than rigid question-answer formats</li>
    <li><strong>Accessibility:</strong> Easier for users with varying technical skills</li>
  </ul>
</Card>

### Trust and Transparency

<Card title="Building Trust with Claude">
  <h4>Trust-Building Features:</h4>
  <ul>
    <li><strong>Honest Communication:</strong> Claude admits when it doesn't know something</li>
    <li><strong>Capability Clarity:</strong> Clear about what it can and cannot do</li>
    <li><strong>Safe Responses:</strong> Built-in safety prevents harmful outputs</li>
    <li><strong>Consistent Behavior:</strong> Predictable, reliable responses</li>
  </ul>
  
  <h4>Transparency Patterns:</h4>
  <ul>
    <li><strong>Confidence Indicators:</strong> Show when Claude is certain vs uncertain</li>
    <li><strong>Source Attribution:</strong> Cite sources when providing information</li>
    <li><strong>Limitation Disclosure:</strong> Clear about capabilities and constraints</li>
    <li><strong>Error Explanation:</strong> Explain why certain requests can't be fulfilled</li>
  </ul>
</Card>

## AI UX Design Patterns

<Tabs>
  <Tab title="Good Design" icon="check-circle">
    ### ‚úÖ Good AI UX Design with Claude
    
    **Progressive Disclosure**: Provides clear options without overwhelming
    - **Graceful Error Handling**: Acknowledges limitations and offers alternatives
    - **Confidence Transparency**: Shows clear confidence levels and reasoning
    - **User Control**: Gives users clear choices and control over complexity
    - **Contextual Awareness**: Maintains conversation context and relevance
    - **Claude Integration**: Leverages Claude's natural conversational abilities
    
    **Example Implementation:**
    ```python
    import anthropic
    
    class GoodClaudeUXSystem:
        def __init__(self):
            self.client = anthropic.Anthropic(api_key="your-api-key")
            self.context_manager = ContextManager()
            self.confidence_detector = ConfidenceDetector()
            self.user_preference_tracker = UserPreferenceTracker()
        
        def generate_response(self, user_input, conversation_history):
            # Progressive disclosure
            if self.is_broad_question(user_input):
                return self.generate_options_response(user_input)
            
            # Graceful error handling
            if not self.can_handle_request(user_input):
                return self.generate_alternative_suggestions(user_input)
            
            # Use Claude's natural conversation abilities
            response = self.client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=1000,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant. Be conversational, honest about your capabilities, and offer alternatives when you can't help."
                    },
                    *conversation_history,
                    {
                        "role": "user",
                        "content": user_input
                    }
                ]
            )
            
            # Add confidence indicators
            confidence = self.confidence_detector.assess(user_input, response.content[0].text)
            
            # User control based on preferences
            if self.user_preference_tracker.prefers_simple(user_input):
                return self.generate_simple_response(response.content[0].text, confidence)
            else:
                return self.generate_detailed_response(response.content[0].text, confidence)
        
        def generate_options_response(self, question):
            return {
                "response": "I'd be happy to help! What specific area are you working on?",
                "options": ["Basic concepts", "Advanced topics", "Practical examples"],
                "confidence": "high",
                "user_control": True,
                "claude_enhanced": True
            }
    ```
  </Tab>
  
  <Tab title="Poor Design" icon="x-circle">
    ### ‚ùå Poor AI UX Design
    
    **Information Overload**: Dumps too much information without guidance
    - **Abrupt Failures**: Provides no recovery path when errors occur
    - **Uncertain Responses**: Shows lack of confidence without explanation
    - **No User Control**: Assumes user wants maximum complexity
    - **Context Loss**: Fails to maintain conversation flow
    - **Ignoring Claude Strengths**: Doesn't leverage Claude's conversational abilities
    
    **Example Implementation:**
    ```python
    class PoorAIUXSystem:
        def __init__(self):
            # No context management
            # No confidence detection
            # No user preference tracking
            # No Claude integration
        
        def generate_response(self, user_input):
            # Information dump without structure
            if self.is_broad_question(user_input):
                return self.dump_all_information(user_input)
            
            # Abrupt failure
            if not self.can_handle_request(user_input):
                return "I cannot help with that."
            
            # No confidence assessment
            # No user control
            # No context consideration
            # No Claude integration
            return self.generate_verbose_response(user_input)
        
        def dump_all_information(self, question):
            return {
                "response": "Here's everything about this topic... [500+ words]",
                "confidence": "unknown",
                "user_control": False,
                "context_aware": False,
                "claude_enhanced": False
            }
    ```
  </Tab>
</Tabs>

## Claude-Specific UX Patterns

### 1. **Conversational Onboarding**

<Card title="Onboarding with Claude">
  <h4>Design Principles:</h4>
  <ul>
    <li><strong>Welcome Message:</strong> Friendly, helpful introduction</li>
    <li><strong>Capability Overview:</strong> Clear explanation of what Claude can do</li>
    <li><strong>Example Interactions:</strong> Show users how to interact effectively</li>
    <li><strong>Progressive Discovery:</strong> Reveal advanced features gradually</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  <pre><code>def create_claude_onboarding():
    welcome_message = """
    Hi! I'm Claude, your AI assistant. I can help you with:
    
    ‚Ä¢ Answering questions and explaining concepts
    ‚Ä¢ Analyzing documents and data
    ‚Ä¢ Writing and editing content
    ‚Ä¢ Solving problems and brainstorming ideas
    
    Just ask me anything! I'm here to help.
    
    üí° Tip: Be specific with your questions for better answers.
    """
    
    return {
        "message": welcome_message,
        "suggestions": [
            "What can you help me with?",
            "Show me an example of your capabilities",
            "How do I get the best results?"
        ],
        "next_steps": "Try asking me a question or uploading a document!"
    }
</code></pre>
</Card>

### 2. **Confidence and Uncertainty**

<Card title="Confidence Indicators with Claude">
  <h4>Confidence Levels:</h4>
  <ul>
    <li><strong>High Confidence:</strong> Clear, direct answers with sources</li>
    <li><strong>Medium Confidence:</strong> Qualified responses with caveats</li>
    <li><strong>Low Confidence:</strong> Honest uncertainty with alternatives</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def generate_confidence_indicator(response_text, confidence_score):
    if confidence_score >= 0.8:
        return {
            "response": response_text,
            "confidence": "high",
            "indicator": "‚úÖ",
            "message": "I'm confident about this information."
        }
    elif confidence_score >= 0.5:
        return {
            "response": response_text,
            "confidence": "medium",
            "indicator": "ü§î",
            "message": "This is my best understanding, but you may want to verify."
        }
    else:
        return {
            "response": response_text,
            "confidence": "low",
            "indicator": "‚ùì",
            "message": "I'm not entirely sure about this. Here are some alternatives..."
        }
</code></pre>
</Card>

### 3. **Error Handling and Recovery**

<Card title="Graceful Error Handling">
  <h4>Error Types and Responses:</h4>
  <ul>
    <li><strong>Unclear Requests:</strong> Ask for clarification</li>
    <li><strong>Out of Scope:</strong> Explain limitations and suggest alternatives</li>
    <li><strong>Technical Issues:</strong> Provide helpful error messages</li>
    <li><strong>Safety Concerns:</strong> Explain why certain requests can't be fulfilled</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def handle_claude_errors(error_type, user_input):
    error_handlers = {
        "unclear": {
            "message": "I'm not sure I understand. Could you rephrase that?",
            "suggestions": ["Can you be more specific?", "What exactly are you looking for?"],
            "help_text": "Try being more specific about what you need help with."
        },
        "out_of_scope": {
            "message": "I can't help with that specific request, but I can help with related topics.",
            "suggestions": ["What's your main goal?", "Is there something else I can help with?"],
            "help_text": "I'm designed to help with information, analysis, and creative tasks."
        },
        "technical": {
            "message": "I'm having trouble processing that. Let's try a different approach.",
            "suggestions": ["Try rephrasing your question", "Break it down into smaller parts"],
            "help_text": "Sometimes simpler, more direct questions work better."
        },
        "safety": {
            "message": "I can't help with that request, but I'm happy to help with other things.",
            "suggestions": ["What's your main goal?", "How else can I assist you?"],
            "help_text": "I'm designed to be helpful while maintaining safety and ethical standards."
        }
    }
    
    handler = error_handlers.get(error_type, error_handlers["unclear"])
    
    return {
        "response": handler["message"],
        "suggestions": handler["suggestions"],
        "help_text": handler["help_text"],
        "error_type": error_type
    }
</code></pre>
</Card>

### 4. **Progressive Disclosure**

<Card title="Progressive Information Disclosure">
  <h4>Disclosure Strategy:</h4>
  <ul>
    <li><strong>Start Simple:</strong> Provide basic information first</li>
    <li><strong>Offer Details:</strong> Give users the option to learn more</li>
    <li><strong>Contextual Help:</strong> Provide help when users need it</li>
    <li><strong>User Control:</strong> Let users choose their level of detail</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def progressive_disclosure_response(user_input, detail_level="basic"):
    if detail_level == "basic":
        return {
            "response": "Here's a simple answer to your question...",
            "show_more": True,
            "more_options": ["Detailed explanation", "Examples", "Related topics"]
        }
    elif detail_level == "detailed":
        return {
            "response": "Here's a comprehensive answer with examples and context...",
            "show_more": False,
            "related_topics": ["Topic 1", "Topic 2", "Topic 3"]
        }
    
    return {
        "response": "I'd be happy to help! What level of detail would you prefer?",
        "options": ["Quick answer", "Detailed explanation", "Step-by-step guide"]
    }
</code></pre>
</Card>

## Advanced UX Patterns

### 1. **Multi-Modal Interactions**

<Card title="Claude's Multi-Modal UX">
  <h4>Interaction Types:</h4>
  <ul>
    <li><strong>Text + Vision:</strong> Analyze images while discussing text</li>
    <li><strong>Document Analysis:</strong> Process and explain documents</li>
    <li><strong>Data Visualization:</strong> Interpret charts and graphs</li>
    <li><strong>Code Review:</strong> Analyze code screenshots</li>
  </ul>
  
  <h4>UX Considerations:</h4>
  <ul>
    <li><strong>Clear Upload Interface:</strong> Easy file upload and drag-and-drop</li>
    <li><strong>Processing Feedback:</strong> Show when Claude is analyzing content</li>
    <li><strong>Context Preservation:</strong> Maintain conversation context with visual content</li>
    <li><strong>Accessibility:</strong> Provide text alternatives for visual content</li>
  </ul>
</Card>

### 2. **Tool Integration UX**

<Card title="Tool Use User Experience">
  <h4>Tool Interaction Patterns:</h4>
  <ul>
    <li><strong>Automatic Tool Selection:</strong> Claude chooses appropriate tools</li>
    <li><strong>Tool Execution Feedback:</strong> Show when tools are being used</li>
    <li><strong>Result Integration:</strong> Seamlessly incorporate tool results</li>
    <li><strong>Error Handling:</strong> Graceful handling of tool failures</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  <pre><code>def tool_integration_ux(tool_name, tool_result, user_query):
    tool_messages = {
        "web_search": "I searched the web for current information about this...",
        "calculator": "I calculated the result for you...",
        "file_reader": "I analyzed the document you provided...",
        "code_executor": "I ran the code and here are the results..."
    }
    
    return {
        "tool_used": tool_name,
        "tool_message": tool_messages.get(tool_name, "I used a tool to help with your request..."),
        "result": tool_result,
        "context": "Based on the " + tool_name + " results, here's what I found...",
        "user_query": user_query
    }
</code></pre>
</Card>

### 3. **Personalization and Adaptation**

<Card title="Personalized UX with Claude">
  <h4>Personalization Features:</h4>
  <ul>
    <li><strong>User Preferences:</strong> Remember user's preferred detail level</li>
    <li><strong>Interaction History:</strong> Adapt based on previous conversations</li>
    <li><strong>Domain Expertise:</strong> Adjust responses based on user's knowledge level</li>
    <li><strong>Communication Style:</strong> Match user's preferred tone and style</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>class PersonalizedClaudeUX:
    def __init__(self):
        self.user_profiles = \{\}
        self.interaction_history = \{\}
    
    def adapt_response(self, user_id, response, user_input):
        profile = self.user_profiles.get(user_id, \{\})
        
        # Adapt detail level
        if profile.get("prefers_simple"):
            response = self.simplify_response(response)
        
        # Adapt tone
        if profile.get("prefers_formal"):
            response = self.make_formal(response)
        elif profile.get("prefers_casual"):
            response = self.make_casual(response)
        
        # Adapt based on expertise level
        expertise_level = profile.get("expertise_level", "intermediate")
        response = self.adjust_for_expertise(response, expertise_level)
        
        return response
    
    def update_user_profile(self, user_id, interaction_data):
        # Update user preferences based on interactions
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = \{\}
        
        # Analyze interaction patterns
        detail_preference = self.analyze_detail_preference(interaction_data)
        tone_preference = self.analyze_tone_preference(interaction_data)
        expertise_level = self.analyze_expertise_level(interaction_data)
        
        self.user_profiles[user_id].update(\{
            "prefers_simple": detail_preference == "simple",
            "prefers_formal": tone_preference == "formal",
            "expertise_level": expertise_level
        \})
</code></pre>
</Card>

## Performance and Responsiveness

### 1. **Streaming Responses**

<Card title="Real-Time Feedback with Claude">
  <h4>Streaming Benefits:</h4>
  <ul>
    <li><strong>Perceived Performance:</strong> Users see immediate feedback</li>
    <li><strong>Engagement:</strong> Keeps users engaged during processing</li>
    <li><strong>Interruption Support:</strong> Users can interrupt if needed</li>
    <li><strong>Progress Indication:</strong> Shows that work is being done</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>async def stream_claude_response(user_input):
    async with anthropic.Anthropic(api_key="your-api-key") as client:
        stream = await client.messages.create(
            model="claude-3-sonnet-20240229",
            max_tokens=1000,
            messages=[\{"role": "user", "content": user_input\}],
            stream=True
        )
        
        async for chunk in stream:
            if chunk.type == "content_block_delta":
                yield \{
                    "type": "content",
                    "text": chunk.delta.text,
                    "partial": True
                \}
            elif chunk.type == "message_stop":
                yield \{
                    "type": "complete",
                    "text": "",
                    "partial": False
                \}
</code></pre>
</Card>

### 2. **Loading States and Feedback**

<Card title="Loading State Design">
  <h4>Loading Patterns:</h4>
  <ul>
    <li><strong>Typing Indicators:</strong> Show Claude is "thinking"</li>
    <li><strong>Progress Bars:</strong> For longer operations</li>
    <li><strong>Status Messages:</strong> Explain what's happening</li>
    <li><strong>Cancel Options:</strong> Allow users to stop processing</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def create_loading_states():
    return {
        "thinking": {
            "message": "Claude is thinking...",
            "icon": "ü§î",
            "animation": "pulse"
        },
        "analyzing": {
            "message": "Analyzing your request...",
            "icon": "üîç",
            "animation": "spin"
        },
        "searching": {
            "message": "Searching for information...",
            "icon": "üîé",
            "animation": "bounce"
        },
        "processing": {
            "message": "Processing your input...",
            "icon": "‚öôÔ∏è",
            "animation": "rotate"
        }
    }
</code></pre>
</Card>

## Accessibility and Inclusion

### 1. **Universal Design Principles**

<Card title="Accessible AI UX">
  <h4>Accessibility Features:</h4>
  <ul>
    <li><strong>Screen Reader Support:</strong> Proper ARIA labels and descriptions</li>
    <li><strong>Keyboard Navigation:</strong> Full keyboard accessibility</li>
    <li><strong>High Contrast:</strong> Support for high contrast modes</li>
    <li><strong>Font Scaling:</strong> Responsive text sizing</li>
    <li><strong>Alternative Input:</strong> Voice input and other modalities</li>
  </ul>
  
  <h4>Implementation Guidelines:</h4>
  <pre><code>def create_accessible_interface():
    return {
        "aria_labels": {
            "chat_input": "Type your message to Claude",
            "send_button": "Send message to Claude",
            "upload_button": "Upload file for Claude to analyze",
            "settings_button": "Open Claude settings"
        },
        "keyboard_shortcuts": {
            "send_message": "Enter",
            "new_conversation": "Ctrl+N",
            "upload_file": "Ctrl+U",
            "settings": "Ctrl+,"
        },
        "focus_management": {
            "auto_focus": "chat_input",
            "focus_after_send": "chat_input",
            "focus_after_upload": "chat_input"
        }
    }
</code></pre>
</Card>

### 2. **Inclusive Language and Content**

<Card title="Inclusive AI Interactions">
  <h4>Inclusive Design Principles:</h4>
  <ul>
    <li><strong>Diverse Examples:</strong> Use examples from various cultures and backgrounds</li>
    <li><strong>Gender-Neutral Language:</strong> Avoid assumptions about gender</li>
    <li><strong>Cultural Sensitivity:</strong> Respect different cultural perspectives</li>
    <li><strong>Age-Appropriate Content:</strong> Adapt content for different age groups</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def create_inclusive_prompts():
    return {
        "system_prompt": """
        You are a helpful, inclusive AI assistant. When providing examples or references:
        - Use diverse examples from various cultures and backgrounds
        - Use gender-neutral language when possible
        - Be culturally sensitive and respectful
        - Consider different perspectives and experiences
        - Avoid stereotypes and assumptions
        """,
        "example_prompts": [
            "Can you provide examples from different cultural perspectives?",
            "How might this apply to people from various backgrounds?",
            "What are some alternative approaches to consider?"
        ]
    }
</code></pre>
</Card>

## Testing and Evaluation

### 1. **UX Testing Methods**

<Card title="AI UX Testing">
  <h4>Testing Approaches:</h4>
  <ul>
    <li><strong>Usability Testing:</strong> Observe real users interacting with Claude</li>
    <li><strong>A/B Testing:</strong> Compare different UX approaches</li>
    <li><strong>Conversation Analysis:</strong> Analyze conversation patterns and flows</li>
    <li><strong>Accessibility Testing:</strong> Test with users who have disabilities</li>
  </ul>
  
  <h4>Testing Metrics:</h4>
  <ul>
    <li><strong>Task Completion Rate:</strong> How often users achieve their goals</li>
    <li><strong>Time to Completion:</strong> How long tasks take</li>
    <li><strong>User Satisfaction:</strong> Subjective ratings and feedback</li>
    <li><strong>Error Rate:</strong> How often users encounter problems</li>
  </ul>
</Card>

### 2. **Continuous Improvement**

<Card title="Iterative UX Improvement">
  <h4>Improvement Process:</h4>
  <ul>
    <li><strong>Data Collection:</strong> Gather usage data and user feedback</li>
    <li><strong>Pattern Analysis:</strong> Identify common issues and opportunities</li>
    <li><strong>Hypothesis Formation:</strong> Develop theories about improvements</li>
    <li><strong>Testing and Validation:</strong> Test improvements with users</li>
    <li><strong>Implementation:</strong> Deploy successful improvements</li>
  </ul>
  
  <h4>Feedback Loops:</h4>
  <pre><code>def create_feedback_system():
    return {
        "user_feedback": {
            "thumbs_up": "Track positive interactions",
            "thumbs_down": "Track negative interactions",
            "text_feedback": "Collect detailed user comments",
            "suggestion_box": "Allow feature requests"
        },
        "analytics": {
            "conversation_length": "Track conversation duration",
            "completion_rate": "Track task completion",
            "error_frequency": "Track common errors",
            "user_satisfaction": "Track satisfaction scores"
        },
        "improvement_cycle": {
            "collect": "Gather data and feedback",
            "analyze": "Identify patterns and issues",
            "design": "Create improvement hypotheses",
            "test": "Validate improvements",
            "implement": "Deploy successful changes"
        }
    }
</code></pre>
</Card>

## Key Design Principles

### 1. **Progressive Disclosure**
- Start simple, add complexity on demand
- Provide clear navigation paths
- Don't overwhelm with options upfront

### 2. **Transparency & Trust**
- Be clear about AI capabilities and limitations
- Show confidence levels when appropriate
- Explain reasoning when possible
- Leverage Claude's built-in honesty

### 3. **Graceful Degradation**
- Always provide a path forward
- Offer alternatives when primary approach fails
- Maintain usefulness even in error states

### 4. **User Control & Freedom**
- Give users clear choices and control
- Allow interruption and redirection
- Provide escape hatches from complex paths

### 5. **Consistent Feedback**
- Show what the AI is doing
- Provide clear success/error states
- Maintain conversation context

### 6. **Natural Conversation**
- Leverage Claude's conversational abilities
- Maintain context across interactions
- Use natural language patterns
- Provide helpful, supportive responses

## Collaboration prompts for engineers

### For Frontend Developers
```
"Can we add a confidence indicator that shows when Claude is certain vs uncertain about its responses? This helps users understand when to trust the AI's advice."
```

### For Backend Developers
```
"We need to implement streaming responses so users see Claude working in real-time. This reduces perceived latency and builds trust."
```

### For UX Researchers
```
"Let's A/B test different error message formats to see which ones help users recover and continue their workflow most effectively."
```

### For Product Managers
```
"We should track user satisfaction after Claude interactions to understand which UX patterns lead to better outcomes and user retention."
```

### For Accessibility Specialists
```
"How can we ensure our Claude interface is fully accessible to users with disabilities? What additional features do we need?"
```

## Related Concepts

<CardGroup cols={2}>
  <Card title="Prompt Engineering" icon="edit" href="../prompting-techniques/chain-of-thought">
    Design effective prompts for Claude
  </Card>
  <Card title="Evaluation & Observability" icon="monitor" href="../evaluation-observability">
    Measure UX effectiveness
  </Card>
  <Card title="Safety & Security" icon="shield" href="../safety-security">
    Build trustworthy AI experiences
  </Card>
  <Card title="Multimodality" icon="image" href="../multimodality">
    Design for multiple interaction types
  </Card>
  <Card title="Tool Use" icon="wrench" href="../prompting-techniques/react">
    Integrate external tools seamlessly
  </Card>
  <Card title="Streaming UX" icon="zap" href="../streaming-ux">
    Real-time interaction patterns
  </Card>
</CardGroup>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [How To Design Effective Conversational AI Experiences: A Comprehensive Guide](https://www.smashingmagazine.com/2024/07/how-design-effective-conversational-ai-experiences-guide/)  
> for internal educational use only (non-profit).

# How To Design Effective Conversational AI Experiences: A Comprehensive Guide

Conversational AI is revolutionizing information access, offering a personalized, intuitive search experience that delights users and empowers businesses. A well-designed conversational agent acts as a knowledgeable guide, understanding user intent and effortlessly navigating vast data, which leads to happier, more engaged users, fostering loyalty and trust. Meanwhile, businesses benefit from increased efficiency, reduced costs, and a stronger bottom line. On the other hand, a poorly designed system can lead to frustration, confusion, and, ultimately, abandonment.

Achieving success with conversational AI requires more than just deploying a chatbot. To truly harness this technology, we must master the intricate dynamics of human-AI interaction. This involves understanding how users articulate needs, explore results, and refine queries, paving the way for a seamless and effective search experience.

This article will decode the three phases of conversational search, the challenges users face at each stage, and the strategies and best practices AI agents can employ to enhance the experience.

## The Three Phases Of Conversational Search

To analyze these complex interactions, Trippas et al. (2018) (PDF) proposed a framework that outlines three core phases in the conversational search process:

1. **Query formulation**: Users express their information needs, often facing challenges in articulating them clearly.
2. **Search results exploration**: Users navigate through presented results, seeking further information and refining their understanding.
3. **Query re-formulation**: Users refine their search based on new insights, adapting their queries and exploring different avenues.

Building on this framework, Azzopardi et al. (2018) (PDF) identified five key user actions within these phases: reveal, inquire, navigate, interrupt, interrogate, and the corresponding agent actions ‚Äî inquire, reveal, traverse, suggest, and explain.

![Table created by the author based on Azzopardi et al.'s paper](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/how-design-effective-conversational-ai-experiences-guide/1-table-three-phases-conversational-search.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

In the following sections, I'll break down each phase of the conversational search journey, delving into the actions users take and the corresponding strategies AI agents can employ, as identified by Azzopardi et al. (2018) (PDF). I'll also share actionable tactics and real-world examples to guide the implementation of these strategies.

## Phase 1: Query Formulation: The Art Of Articulation

In the initial phase of query formulation, users attempt to translate their needs into prompts. This process involves conscious disclosures ‚Äî sharing details they believe are relevant ‚Äî and unconscious non-disclosure ‚Äî omitting information they may not deem important or struggle to articulate.

This process is fraught with challenges. As Jakob Nielsen aptly pointed out,

> "Articulating ideas in written prose is hard. Most likely, half the population can't do it. This is a usability problem for current prompt-based AI user interfaces."‚Äî Jakob Nielsen

This can manifest as:

- **Vague language**: "I need help with my finances." Budgeting? Investing? Debt management?
- **Missing details**: "I need a new pair of shoes." What type of shoes? For what purpose?
- **Limited vocabulary**: Not knowing the right technical terms. "I think I have a sprain in my ankle." The user might not know the difference between a sprain and a strain or the correct anatomical terms.

These challenges can lead to frustration for users and less relevant results from the AI agent.

### AI Agent Strategies: Nudging Users Towards Better Input

To bridge the articulation gap, AI agents can employ three core strategies:

1. **Elicit**: Proactively guide users to provide more information.
2. **Clarify**: Seek to resolve ambiguities in the user's query.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [When Words Cannot Describe: Designing For AI Beyond Conversational Interfaces](https://www.smashingmagazine.com/2024/02/designing-ai-beyond-conversational-interfaces/)  
> for internal educational use only (non-profit).

# When Words Cannot Describe: Designing For AI Beyond Conversational Interfaces

Few technological innovations can completely change the way we interact with computers. Lucky for us, it seems we've won front-row seats to the unfolding of the next paradigm shift.

These shifts tend to unlock a new abstraction layer to hide the working details of a subsystem. Generalizing details allows our complex systems to appear simpler & more intuitive. This streamlines coding programs for computers as well as designing the interfaces to interact with them.

The Command Line Interface, for instance, created an abstraction layer to enable interaction through a stored program. This hid the subsystem details once exposed in earlier computers that were only programmable by inputting 1s & 0s through switches.

Graphical User Interfaces (GUI) further abstracted this notion by allowing us to manipulate computers through visual metaphors. These abstractions made computers accessible to a mainstream of non-technical users.

Despite these advances, we still haven't found a perfectly intuitive interface ‚Äî the troves of support articles across the web make that evident. Yet recent advances in AI have convinced many technologists that the next evolutionary cycle of computing is upon us.

![An animation by Maximillian Piras depicting a history of interface abstraction: three panes of glass float above each other & each one displays a different interface style. The bottom layer is a command line interface represented by green code on a black screen. The middle layer is a GUI represented by a desktop with icons representing folders. The top layer is an AI-powered conversational interface represented by a chatbot asking, "How can I help you today?".](https://files.smashing.media/articles/designing-ai-beyond-conversational-interfaces/1-smashing-abstraction-intro.gif)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

## The Next Layer Of Interface Abstraction

A branch of machine learning called generative AI drives the bulk of recent innovation. It leverages pattern recognition in datasets to establish probabilistic distributions that enable novel constructions of text, media, & code. Bill Gates believes it's "the most important advance in technology since the graphical user interface" because it can make controlling computers even easier. A newfound ability to interpret unstructured data, such as natural language, unlocks new inputs & outputs to enable novel form factors.

Now our universe of information can be instantly invoked through an interface as intuitive as talking to another human. These are the computers we've dreamed of in science fiction, akin to systems like Data from Star Trek. Perhaps computers up to this point were only prototypes & we're now getting to the actual product launch. Imagine if building the internet was laying down the tracks, AIs could be the trains to transport all of our information at breakneck speed & we're about to see what happens when they barrel into town.

> "Soon the pre-AI period will seem as distant as the days when using a computer meant typing at a C:> prompt rather than tapping on a screen."‚Äî Bill Gates in "The Age of AI Has Begun"

If everything is about to change, so must the mental models of software designers. As Luke Wroblewski once popularized mobile-first design, the next zeitgeist is likely AI-first. Only through understanding AI's constraints & capabilities can we craft delight. Its influence on the discourse of interface evolution has already begun.

Large Language Models (LLMs), for instance, are a type of AI utilized in many new applications & their text-based nature leads many to believe a conversational interface, such as a chatbot, is a fitting form for the future. The notion that AI is something you talk to has been permeating across the industry for years. Robb Wilson, the co-owner of UX Magazine, calls conversation "the infinitely scalable interface" in his book The Age of Invisible Machines (2022). Noah Levin, Figma's VP of Product Design, contends that "it's a very intuitive thing to learn how to talk to something." Even a herald of GUIs such as Bill Gates posits that "our main way of controlling a computer will no longer be pointing and clicking."

![An animation displays a new button in Microsoft PowerPoint that allows users to make edits through a chat window.](https://files.smashing.media/articles/designing-ai-beyond-conversational-interfaces/2-smashing-abstraction-mscopilot-800px.gif)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The hope is that conversational computers will flatten learning curves. Jesse Lyu, the founder of Rabbit, asserts that a natural language approach will be "so intuitive that you don't even need to learn how to use it."

After all, it's not as if Data from Stark Trek came with an instruction manual or onboarding tutorial. From this perspective, the evolutionary tale of conversational interfaces superseding GUIs seems logical & echoes the earlier shift away from command lines. But others have opposing opinions, some going as far as Maggie Appleton to call conversational interfaces like chatbots "the lazy solution."

This might seem like a schism at first, but it's more so a symptom of a simplistic framing of interface evolution. Command lines are far from extinct; technical users still prefer them for their greater flexibility & efficiency. For use cases like software development or automation scripting, the added abstraction layer in graphical no-code tools can act as a barrier rather than a bridge.

> GUIs were revolutionary but not a panacea. Yet there is ample research to suggest conversational interfaces won't be one, either. For certain interactions, they can decrease usability, increase cost, & introduce security risk relative to GUIs.

So, what is the right interface for artificially intelligent applications? This article aims to inform that design decision by contrasting the capabilities & constraints of conversation as an interface.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Developing A Chatbot Using Microsoft's Bot Framework, LUIS And Node.js (Part 1)](https://www.smashingmagazine.com/2017/05/chatbot-microsoft-bot-framework-luis-nodejs-part1/)  
> for internal educational use only (non-profit).

# Developing A Chatbot Using Microsoft's Bot Framework, LUIS And Node.js (Part 1)

This tutorial gives you hands-on access to my journey of creating a digital assistant capable of connecting with any system via a RESTful API to perform various tasks.
Here, I'll be demonstrating how to save a user's basic information and create a new project on their behalf via natural language processing (NLP).

Note: I am not associated with Microsoft in any way (either directly or indirectly).

## Choosing A Bot Framework

The rising wave of artificial intelligence (AI) in the last couple of years has given a massive push to the idea of conversational interfaces, commonly known as chatbots. Creating a high-performing chatbot that can understand natural language was a tedious and complex task a few years back, but like all other user interfaces, we saw some promising advancements in chatbot-building platforms as well.

The tech community has joined forces to boost the growth of various aspects of AI, including natural language processing and machine learning. Big players such as Facebook, Microsoft, IBM and Google have tirelessly been creating platforms and tools to help developers and entrepreneurs ease the process of integrating such technology into their own products and businesses.

Platforms such as Slack, Facebook Messenger, Pandorabots, Telegram, Wit.ai and Microsoft's Bot Framework have given thousands of technopreneurs like me a strong foundation on which to create useful chatbots that can be integrated with existing platforms and apps in minutes. Some of these frameworks are confined to their own platforms, like Slack and Facebook Messenger, while others integrate with multiple platforms, which saves a lot of effort if you're aiming for a cross-platform launch. Matching user intent and finding entities from their utterances are the foundation on which these frameworks are built. The most extensive suites of cognitive services that can add true intelligence to your bot are offered by both IBM and Microsoft. Both companies have been investing heavily in this domain, and their services can be used as an extension of the "mind" of your bot.

A few months back, I wrote an article on TechCrunch discussing the role of AI and chatbots in the web development industry. This was the same time when I saw that Bot Framework was picking up speed. Microsoft made its Bot Builder available as an open-source SDK, not just on .NET, but also on Node.js and a REST API.

At this time, I was in discussions with Iflexion and Hema Maps about creating chatbots with extensive natural language and AI capabilities for their future projects. During these meetings, I identified the need to eventually add machine learning as well. So, I thought, why not give Microsoft's products a try?

## The Chatbot Journey

### Signing Up And Creating The App

The first part is pretty convenient if you already have a Microsoft account (a work, school or personal account). Just use that to sign in on https://dev.botframework.com/. After you log in, there's a button to "Register a Bot," which takes you to a page asking you to fill in some information about the bot:

- **name**: A friendly name for your bot.
- **bot handle**: Used in the URL for your bot. Cannot be changed once registered.
- **description**: Displayed on your card in the bot directory, if you choose to list it there.
- **messaging end point**: The URL at which your bot will live. You'll get this URL when setting up the server environment.
- **app ID**: Obtained by clicking the "Create Microsoft App ID and password" button above. While creating your app ID, you'll be asked to create a password. Copy and save this password safely: You'll need it in the bot configuration file.
- **owners**: A comma-separated list of email IDs for people who will have editing rights to this bot.
- **instrumentation key**: The Azure App Insights key, if you want to receive analytics about your bot.

Once you have filled in all the information, you can click on "Register."

### Setting Up The Local Environment

We will first set up our local machine to test the bot via the Bot Framework Emulator on Ubuntu (also available for Mac and Windows). When I first downloaded the Bot Framework Emulator for Linux, it was just a version for the command-line interface. Luckily, it soon got replaced with a GUI-based AppImage version, which runs directly without installation.

Because JavaScript is one of my primary coding languages, I chose the Node.js version of the Bot Builder SDK. To run it, you'll obviously need to install Node.js on your system. I used the Linux package manager to do so, but you can select the method that is convenient for you.

```
$ sudo apt install nodejs
```

Once Node.js is installed, create a new folder and a blank JavaScript file in a location of your choice. I'll use /var/www/aplostestbot as my root folder and create a file, aplostestbot.js, inside it. Open a new command-line editor and change the current working directory to the bot's folder. Now, run the following commands to fetch the two dependencies that we need to build our bot:

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [How AI Succeeds (and Fails) to Help People Find Information](https://www.nngroup.com/articles/ai-information-seeking-keyword-foraging/)  
> for internal educational use only (non-profit).

# How AI Succeeds (and Fails) to Help People Find Information

AI chatbots and AI-driven search can help people find what they're looking for ‚Äî even if they don't know what they're looking for. In this way, generative AI tools can greatly alleviate a longstanding major pain point of a search-driven web. However, AI's ability to help with this problem is still limited. In particular, many consumers are simply not aware of how powerful generative AI tools can be in information-seeking, and don't know how to prompt to achieve better outcomes.

## In This Article:

- Keyword Foraging in Traditional Search
- AI Streamlines Simple Keyword Foraging
- Articulation Challenges in Searching and Prompting: An Example from User Testing
- AI Doesn't Yet Eliminate Information-Seeking Challenges

## Keyword Foraging in Traditional Search

As one of our participants in recent qualitative studies on AI told us, "You don't know what you don't know."

That's an eternal challenge when humans seek to answer a question or learn about a topic. With traditional web search, users are severely limited by their ability to articulate their need. They have to know what they're looking for in order to provide keywords to a search engine.

This challenge leads to a user behavior I've named keyword foraging ‚Äî the search before the search.

> When keyword foraging, a user conducts a preliminary search (usually in a web search engine like Google) to determine the right keywords for their information need.

For example, a shopper may want to buy that Y-shaped peeler that bartenders use to remove strips of citrus peel for cocktails but doesn't know it's called a "channel knife." Before she can find the term that will help her get what she wants, the user will need to do some awkward failing through search engines and websites.

## AI Streamlines Simple Keyword Foraging

Because generative AI chats and AI-powered search engines can accept wordy, complex questions, keywords are less of a barrier than they used to be. The ability to express an information need in full sentences, without proper terminology, is extremely powerful.

In the channel-knife example, the shopper can simply explain her goal and describe the thing she wants ‚Äî it doesn't matter if she doesn't know the actual term.

![Google's AI Mode: By articulating the information need in natural language, users can easily work around the keyword-foraging problem.](https://media.nngroup.com/media/editor/2025/08/13/keyword-channel-knife-gemini.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

For simple situations like this one, where the user knows what they need but does not know the correct keyword, generative AI greatly facilitates keyword foraging.

However, keyword foraging still represents an extra initial step before the user can search for channel knives to purchase (her actual goal). You'd expect Google's AI Mode to pull in some options from Google Shopping in this instance, but it failed to.

In contrast, when I gave the same prompt to Amazon's Rufus (its AI-powered shopping assistant), Rufus provided the correct term and some links to purchase options.

![Amazon Rufus provided the term the user was looking for, but it also proactively offered some product options.](https://media.nngroup.com/media/editor/2025/08/13/keyword-channel-knife-rufus.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Articulation Challenges in Searching and Prompting: An Example from User Testing

It can be annoying and tedious to look for the right name of the thing you want to search for. But it's even more difficult when you don't know or can't explain what you need in the first place. This situation goes beyond simple keyword foraging ‚Äî before you can even begin to forage for keywords, you need to explain the problem and identify the potential solution.

For example, one participant in our recent study had noticed that a drainage pipe outside his home was leaking and wanted to fix it by himself. He faced several challenges with this information need: to find a repair video, he needed to search for the right terms, but he had no plumbing vocabulary and no clear diagnosis of his issue.

He started at classic Google (his "search engine of choice") and tried a traditional search query: repair outside drainage pipes uk. He immediately checked the resulting AI overview but was disappointed. He noticed the word "trenchless" and realized that the system was not accurately understanding his situation ‚Äî it was advising on repairing underground wastewater pipes, while he was dealing with an above-ground pipe problem. "It's going off in the wrong direction," he told us.

![Google AI overview: The participant struggled to communicate his plumbing problem. Google's resulting AI overview gave tips for repairing a different type of pipe, which was not helpful for the participant.](https://media.nngroup.com/media/editor/2025/08/13/keyword-drain-uk.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

He reflected that he probably wasn't accurately describing the problem in his query, saying:

> "The terms in my prompt might be wrong. I need to change my terms."

He tried three iterations of his search query but kept getting similar results.

- repair outside drainage pipes uk
- repair outside drainage pipes fixed to exterior wall uk
- repair outside waste water pipes fixed to exterior wall uk

"At this point, I'm getting quite frustrated," he said. He proceeded to visit websites, including Reddit and YouTube, vainly looking for information scent to indicate he was moving in the right direction. After multiple disappointments over nearly 15 minutes, he told the facilitator that he might give up and just call a plumber to fix the problem.

The facilitator asked him to attempt his task with Gemini. This was the user's first time using AI chat for something other than help writing emails and documents. His first prompt simply rephrased his previous queries in question format:

> how do i fix leaking outside waste water drain pipes in the uk

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [How AI Is Changing Search Behaviors](https://www.nngroup.com/articles/ai-changing-search-behaviors/)  
> for internal educational use only (non-profit).

# How AI Is Changing Search Behaviors

Generative AI (genAI) is reshaping how people search for information. Anyone watching their content pageviews decline is currently experiencing the impact of this. But what's behind the shift? The speed of the change is impressive, considering how deeply ingrained information-seeking habits can be.

In a recent qualitative study, we asked people to bring their own research tasks into the virtual lab. We explored how users' information-seeking behaviors are shifting in response to AI-powered search tools and chatbots. While AI offers compelling shortcuts around tedious research tasks, it isn't close to completely replacing traditional search. But, even when people are using traditional search, the AI-generated overview that now tops almost all search-results pages steals a significant amount of attention and often shortcuts the need to visit the actual pages.

## In This Article:

- Search Habits Are Hard to Break
- AI Changes Information-Seeking Habits
- AI Doesn't Eliminate the Need for Search
- Gemini and ChatGPT Have a Competitive Advantage: Familiarity
- The Changing Landscape of Information Seeking
- Next in This Series
- About the Study

## Search Habits Are Hard to Break

Information-seeking habits are sticky. Once someone finds a reliable way to easily find information they need, that method becomes nearly instinctive.

This partly explains Google's massive share of the search-engine market. Over the years, many participants have told us they've never even considered Bing or other alternatives, simply because they're familiar with Google and know it works well (enough) for them. In our study, multiple participants commented on their tendency to lean on what they're already comfortable with.

> "I always start with Google. It's familiar. It's what's there. It's what I've been doing for a long time."
> "It's always Google for me. It's where I always start; my search engine of choice."

People tend to reach for whatever information-seeking tool is most convenient. One participant reflected that he got into the habit of using Google because it was built into his Chrome browser.

> "At some point, I used the Google Chrome browser. It has a default setting that redirects me to Google. So, by default, I just start from google.com. I think that's the main reason I've stuck with this for a very long time."

Beyond the choice of where to search for information, these habits also influence how users search. For example, several participants in our study (and many participants in other studies over the years) told us that they always skip over sponsored results on a search-results page. Some participants thought that they're "supposed" to search this way (an example of a technology myth). Others were unable to explain why they skip the ads.

> "I don't know why I always skip the first results. Anything that says sponsored‚Ä¶ I just tend to go to one of the top results that isn't sponsored."

These are information-seeking strategies that people developed over many years. It makes sense that users would fall into these habits over time, because (in most cases) they aren't thinking much about the systems they'll use ‚Äî they're focused on finding answers to their questions.

Users rely on them because they've generally worked well in the past. To change these habits, people need a significant incentive.

## AI Changes Information-Seeking Habits

Generative AI's value in information seeking is powerful enough to change those ingrained habits. Several participants were aware that their own information-seeking behaviors had started to shift since they began experimenting with AI tools.

> "Oh, I always start from Google. I never use different search engines, but these days I also incorporate ChatGPT."

Generative AI offers substantial shortcuts around the often tedious and time-consuming work required to research a topic, including:

- Defining and articulating the information need
- Overcoming information gaps and keyword-foraging problems
- Weighing and selecting credible sources
- Sifting through enormous amounts of information
- Scanning through long pages of text
- Comparing contradicting perspectives from different sources
- Synthesizing and storing information (mentally or in a note)

Even when participants utilized only a handful of genAI's possible information-seeking benefits, they valued the assistance immensely.

### AI Overviews: The First Point of Contact for Novice AI Users

Every single participant in our study had at least heard of AI and had encountered and used AI overviews on Google's search-results pages. But a few of our participants had very limited experience with generative AI beyond those instances.

> AI overviews appear at the top of results pages for many queries on web search engines. They're powered by LLMs and attempt to quickly define keywords or answer questions.

![A Google search results page for "dragonfruit" displaying an AI Overview. The overview describes dragon fruit as a tropical fruit with vibrant pink or yellow skin and white or red flesh dotted with black seeds. It includes details on skin, flesh, varieties, taste, and texture. The right-hand panel displays search results including a Wikipedia snippet, a Healthline article, and a YouTube video thumbnail showing a sliced dragon fruit.](https://media.nngroup.com/media/editor/2025/08/08/ai-overview-example-dragonfruit.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

AI overviews are the modern, upgraded iteration of the featured snippets and answer boxes introduced by Google and its competitors in the 2010s. Even then, many content sites started to notice a dent in their web traffic as people started to find answers without a click. AI overviews are even more likely to satisfy information needs without clicks.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Facilitating AI-Enhanced Workshops: From Ideation to Action](https://www.nngroup.com/articles/facilitating-ai-workshops/)  
> for internal educational use only (non-profit).

# Facilitating AI-Enhanced Workshops: From Ideation to Action

With AI's potential to increase the quality of collaborative outputs, teams should incorporate it into their workshops to maximize success. That starts with thoughtful preparation ‚Äî but once the workshop begins, facilitators need to guide participants in using AI effectively, creatively, and collaboratively. This article discusses how to do just that.

## In This Article:

- 1. Narrow AI-Generated Ideas Before Sharing
- 2. Prepare Evaluation Criteria for AI Outputs
- 3. Plan More Time for Reading and Prompt Iteration than You'd Think
- 4. Encourage People to Cowrite Prompts
- 5. Document Things Digitally

## 1. Narrow AI-Generated Ideas Before Sharing

To understand how to run effective AI workshops, we need to consider how human and AI ideation differ.

For one, most humans tend to fixate on their first few ideas and must be pushed to think outside the box. They also tend to settle on one idea ‚Äî whether it's good or not ‚Äî often because someone charismatic or authoritative has suggested it. AI does not struggle with these problems.

This difference is a bit like that between sheep and rabbits:

- **People (the sheep)** tend to follow a leader and hesitate to stray far from their starting point. Even smart people working together often move as one.
- **AI (the rabbits)** isn't as predictable. People who use AI can generate more divergent ideas than those who don't, because AI isn't swayed by groupthink, moves on quickly, and doesn't get tired

![On the left is a drawing of sheep and the caption "Ideation with people." The right is a drawing of rabbits with the caption "Ideation with AI."](https://media.nngroup.com/media/editor/2025/06/06/facilitating-ai-article-visual.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

This contrast is a great benefit and also a big challenge when involving AI in workshops and ideation. In just 5 minutes, a few people armed with an AI tool can come up with more information than they have time to fully consider or share. In a Procter & Gamble study (discussed in a previous article), both individuals and teams using AI submitted much longer solutions (measured by word count) than those working without AI.

To solve this problem, workshop facilitators must first understand how AI enhancement alters the workshop structure. In human-only sessions, most ideation-based activities have a diverge‚Äìconverge structure:

1. **Diverge** and spend time coming up with as many ideas as possible.
2. **Converge** and work with the group to cluster, discuss, and prioritize everyone's outputs.

When AI is involved, this second step is much harder because there can be hundreds of ideas to consider. You could have people limit the number of ideas the AI is generating, but this undermines its superpower and violates the proper ideation mindset. Personally, I rarely use AI to help ideate without asking it to provide at least 15‚Äì20 different options per prompt.

Instead, use the following structure to benefit from diverging and converging while using AI:

1. **Diverge** and spend time using the AI to come up with as many ideas as possible.
2. **Have the AI narrow down** the ideas to top suggestions, based on provided criteria.
3. **Converge** and work with the group to cluster, discuss, and prioritize everyone's outputs.

## 2. Prepare Evaluation Criteria for AI Outputs

Now, the facilitator must decide what the criteria for narrowing should be.

This question becomes a discussion about prioritization. There are many potential methods for this, but, in my experience, given that LLMs are text-based, a scorecard works best. That way, the AI can provide a rating for each idea based on different criteria.

You can either provide these criteria ahead of time or collaboratively create them as part of the workshop. This is your choice. Here's an example of a sample prompt I might give participants if I had decided the criteria before:

> We can't implement all these ideas. We need to narrow in on around 5 of the best ones. Use the following criteria to select which of these ideas we should move forward with:
> 
> - **Feasibility**: The degree to which the item can be technically built. Does the skillset and expertise exist to create this solution?
> - **Desirability**: How much users want the item. What unique value proposition does it provide? Is the solution fundamentally needed, or are users otherwise able to accomplish their goals?
> - **Viability**: If the item is functionally attainable for the business. Does pursuing the item benefit the business? What are the costs to the business and is the solution sustainable over time?
> 
> Follow these steps:
> 
> 1. Rate each of the ideas you've provided on a scale of 1 ‚Äì 10 for each of the criteria provided. For example, 1 = low feasibility, meaning it would be impossible to build, and 10 = high feasibility, meaning it would be very easy to build. Apply this same framing to each criterion.
> 2. Provide a rationale for each of your ratings.
> 3. Add the three scores from each idea together to create a total score out of a potential of 30.
> 4. Provide all ideas and their respective ratings in a table, ranking them from the highest total scores at the top to the lowest total scores at the bottom.

You can swap out feasibility, desirability, and viability with other criteria relevant to your context. The AI's ratings will also become more meaningful if you provide more context (such as uploaded documents or even a custom AI ‚Äî as discussed in another article in the series).

## 3. Plan More Time for Reading and Prompt Iteration than You'd Think

Because the AI generates ideas so quickly, it's easy to assume that AI-assisted people won't need as much time to work independently as they would when doing all the mental work themselves. This isn't true. People working with AI generally need just as much time, if not more, as people working on their own. Now, they need to write thoughtful prompts, read through the responses, and repeat the cycle a few times.

To individually generate ideas for one problem, I typically give people without AI around 5 minutes; with AI, I give them around 8‚Äì10 minutes. However, this duration varies based on the group's familiarity with AI, the complexity of the problem, and how quickly people seem to be wrapping up.

## 4. Encourage People to Cowrite Prompts

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [4 Tips for Preparing AI-Enhanced Workshops](https://www.nngroup.com/articles/preparing-ai-workshops/)  
> for internal educational use only (non-profit).

# 4 Tips for Preparing AI-Enhanced Workshops

As the research by Fabrizio Dell'Acqua and colleagues proved, generative AI (genAI) can boost team ideation. However, incorporating AI in team ideation can be challenging, especially if different team members have different experience levels with genAI. This article discusses how you can prepare for a successful and productive AI-enhanced workshop. A subsequent article will talk about how to run such a workshop.

## In This Article:

- 1. Prepare an AI WarmUp Activity to Build Familiarity
- 2. Upload Files for Quick and Easy Context
- 3. Create a Custom AI for More Powerful Context and Constraints
- 4. Provide Adaptable Sample Prompts

## 1. Prepare an AI WarmUp Activity to Build Familiarity

When prepping any workshop ‚Äî AI-enhanced or not ‚Äî I always use a warmup activity or icebreaker to get people's minds and mouths moving. Well-crafted warmups help groups become familiar with the session's tools. Whether this is a shared document, a digital whiteboard, or Sharpies and sticky notes, getting the tools into people's hands is meant to remove any hesitations or access obstacles that participants may have. When I teach courses on AI, I always invite people to open their LLMs early in the session. Doing so ensures that everyone has them ready to go. I'd encourage workshop participants to use the same LLM. That way, people can troubleshoot problems and spend less time getting distracted by differences in various tools.

Here are some fun AI-based warmup examples you can use regardless of the group size or AI tool:

- Tell the AI 3 things you know about someone else in the group, and have it generate a picture of them.
- Give the AI the names of everyone in the group and ask it to combine them into a team name.
- Have the team work together to send a short sequence of emojis to the AI and have it generate a story based on them.

Consider asking the AI for more ideas with a prompt like, "Suggest some fun icebreakers I could use for a team who will be using AI in a workshop that have people use the AI in some way." Add more details to the prompt to adapt it to your participants and their interests or to the focus of the workshop.

## 2. Upload Files for Quick and Easy Context

Part of good prompting is providing adequate context. Sample prompts help with this, but it's often much more effective to prepare context-providing documents before the workshop. Participants can then simply upload these documents into their LLMs to provide context for their prompts. Uploads could include things like:

- A description of the company and the goods or services it provides
- Personas or other research artifacts describing data-derived information about users and their needs
- A timeline for an upcoming project
- A list of constraints or requirements pertinent to the task at hand
- Clean and prepared qualitative or quantitative user data (such as survey responses or interview transcripts) ‚Äî however, please avoid using the AI in the workshop to formally analyze this data, as it should serve only for context.

Providing contextual uploads will also allow people to start fresh conversations with the AI without losing the relevant context ‚Äî they could simply reupload the same information.

Keep file formats straightforward and use text-based documents. LLMs are getting better at reading PDFs, spreadsheets, images, and other file types, but I always recommend testing any files you'll use by uploading them yourself ahead of time and asking the LLM to prove that it can read them.

## 3. Create a Custom AI for More Powerful Context and Constraints

Let's say you're running lots of workshops around a certain topic. Or there are many contextual documents that are cumbersome to upload. Or there are very specific constraints or formats you want the AI to apply to its responses. In such situations, I recommend creating a custom AI that already knows everything workshop participants will need. Three prominent examples of custom AIs include GPTs within ChatGPT, Gems within Gemini, and Projects within Claude.

Examples of UX use cases for a custom AI might include:

- **Personas**: Create a separate custom AI to impersonate each persona. Provide it with as much real data as you have about that persona's needs. While the AI's responses will not always align with what real users will say, they'll be just as reliable as asking team members to predict user behavior.
- **Constraints**: Teach the custom AI your team structure and typical work cadence. Also, give it any other major constraints you're working within so that it automatically considers these when suggesting ideas.
- **Stylistic guidelines**: Give the custom AI documentation on style, tone, word choice, and samples of good content that aligns with those guidelines. The LLM will provide outputs that automatically align with that direction, without further instruction from workshop participants.

For example, I recently ran a workshop with the course instructors at NN/g to ideate about valuable future UX trainings. I created a custom "Course Creation" GPT that I had trained to understand learning objectives, good assessment practices, the scope and style of our courses, and many other NN/g-specific details. I had all instructors in the workshop exclusively use this custom GPT. This approach saved significant time and energy by aligning the various outputs from different people.

## 4. Provide Adaptable Sample Prompts

You never want to completely control the prompts people use when working with AI. Writing and iterating prompts together leverage the group's expertise and creativity. However, some people might struggle getting started for a couple of different reasons:

- They aren't comfortable using AI, or aren't very good at writing prompts (yet).
- They don't understand the problem space and aren't sure what direction to take things (initially).
- They're feeling lazy and don't want to do a bunch of writing.
- They aren't comfortable quickly writing out their thoughts for fluency or language reasons.

Your sample prompts are meant to be a starter kit, intended for adaptation and abandonment as people get going. However, they can serve as instructive demonstrations of useful prompting and steer people in a helpful direction for the workshop. Here are some basic examples following our CAREful framework:

### Example 1: Writing Web Copy

> **Context**: I'm a UX writer working on an enterprise productivity app. We're updating the homepage hero section to highlight our new collaborative whiteboard feature better.
> 
> **Ask**: You're a UX copywriter. Write three headline options for the hero banner. Draft ten 12-word subheaders emphasizing seamless teamwork. Suggest ten 4-word calls-to-actions that drive signups.
> 
> **Rules**: Tone: clear, approachable, and action-oriented. Headlines ‚â§ 7 words; subheaders ‚â§ 12 words; CTA ‚â§ 4 words. Include "collaborate" or "together" at least once. Avoid jargon and superlatives.
> 
> **Examples**: Good headline: "Collaborate in Real Time". Bad headline: "Revolutionize Your Team's Synergy"

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [AI Chatbots Discourage Error Checking](https://www.nngroup.com/articles/ai-chatbots-discourage-error-checking/)  
> for internal educational use only (non-profit).

# AI Chatbots Discourage Error Checking

Large language models (LLMs) are being widely introduced into professional workflows through both new standalone software and integrations with existing tools. One of the most important ways that these tools increase users' productivity is through text generation. But generated content is prone to hallucinations, with the AI extrapolating outside of its training data to return outputs that are truth-like, but incorrect.

Designers of generative AI (genAI) products must help users identify and correct these errors. Most genAI tools currently fail in this responsibility.

## In This Article:

- Users Struggle to Verify AI Outputs Successfully
- Reducing Errors Increases Interaction Cost
- LLM Outputs Signal Authoritativeness
- Users Are Not Building Expertise to Spot Errors
- A Finished Product Is Harder to Evaluate
- Designing Checkable AI Tools

## Users Struggle to Verify AI Outputs Successfully

When humans produce text, they alternate between writing and editing. GenAI tools make people more productive by accelerating the pace at which they can create content ‚Äî the AI will near-instantaneously generate some the text based on the prompt, and the user needs to spend time only on editing.

Since LLMs are trained on a corpus of grammatically and syntactically correct content, their outputs may not need much copyediting. But for professional writing, editing also includes fact-checking and ensuring that any argument follows from the facts.

This work is not an afterthought ‚Äî it is a difficult and time-consuming process. Users of genAI tools have been struggling to error-check their outputs successfully. Lawyers have been caught citing cases that don't exist. Scientists refer to hallucinated papers and journals. Doctors end up trusting erroneous AI diagnoses over their own expertise.

It is irresponsible to blame the users for misinformation generated by LLMs. People are efficient (not lazy). Users adopt genAI tools precisely because they come with the promise of greater efficiency. They will not go out of their way to check the work if the effort of doing so does not seem proportional to the AI's likelihood of being wrong.

## Reducing Errors Increases Interaction Cost

It's true that an exceptionally careful user can reduce the number of mistakes the LLM will produce. But doing so requires additional knowledge and user effort.

To verify the answers provided by a chatbot, the user will need to follow the same steps as they would when reviewing any other text, and then some.

- **Review the entire output**: Writers build up a mental model of their work in their heads as they go and maintain a sense of its structure. Editing an LLM output requires the user to understand the structure of the answer and build up that mental model from scratch ‚Äî even between prompts on the same topic, as LLM outputs are nondeterministic.
- **Identify items that need verification**: The text will present and connect different key ideas. It is those ideas ‚Äî facts and claims made on their basis ‚Äî that the editor needs to pay attention to.
- **Validate the claims being made**: In situations where the LLM has provided a citation, the editor needs to track down the text (assuming that it exists) and find the content being cited within it. The editor also needs to determine whether the source is authoritative. If the LLM has not cited a source, the editor must perform research from scratch to confirm the claim as factual (or ask the LLM for supporting evidence).
- **Verify that the arguments follow from the claims**: The editor must separate facts and the conclusions being drawn from those facts. This often requires a significant degree of subject-matter expertise.
- **Articulate corrections**: If the editor is not willing to rewrite the output themselves, they must update the prompt to tell the LLM what needs changing.
- **Verify the response again**: Due to the limited context window and nondeterministic output of LLMs, there is no guarantee that any given error was fixed or new errors were not introduced.

Some of these steps may be shortened with careful prompt engineering. For example, asking the LLM to generate one small and digestible part of a text at a time allows users greater control over what is written and how it is incorporated into the larger whole of the work. But these strategies are workarounds that require user effort and an in-depth understanding of how AIs hallucinate. And working through a document one part at a time is possible only for expert users who already know what those parts ought to be.

## LLM Outputs Signal Authoritativeness

Humans do not check every single novel piece of information for accuracy; the effort to do so would be unjustifiable. The extent to which we take a claim at its value is based on how closely it fits with what we already know and the trustworthiness of the source.

Outputs of genAI tools mimic certain attributes that we associate with authoritative sources. Their tone is unerringly confident, regardless of the accuracy of the response. They are grammatically correct and meticulously formatted. Through a phenomenon called the halo effect, users' positive perception of one attribute of the LLM's responses causes them to have a positive predisposition about all its other attributes ‚Äî including accuracy.

While we ought to consider AI-generated content an early draft, due to these signals we often treat it as finished work. Users frequently have such confidence in an LLM output that they do not even read through it end to end. For example, in our recent round of intern hiring, we were disappointed to see cover letters ending in "Let me know if you need anything else!"

Additionally, nothing about the chat interface (a plain text box) acknowledges that the LLM's outputs might not be entirely accurate outside of a small label: XYZ bot can make mistakes, please check responses. Instead, chatbots typically encourage users to move on from the response by prompting them with a new question.

![Screenshot of the IEA.org chatbot that says "Remember, AI can make mistakes."](https://media.nngroup.com/media/editor/2025/05/06/ieaorg-chatbot-error.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Users Are Not Building Expertise to Spot Errors

Improving writing productivity is not the only application of genAI. These tools are often used as virtual experts, providing advice or performing tasks outside of the user's field of expertise. In practices such as vibe coding, where an AI generates code based on plain-language prompts, the user does not need to know anything about software development to create a usable program.

A side effect of this mode of usage is that users lack a sufficient understanding of the subject matter to meaningfully validate the LLM's outputs. A vibe coder has no way of evaluating the quality of the code for security vulnerabilities, and a guidebook author might find out they gave bad advice only when customers start getting poisoned.

But far from all AI-generated answers are hallucinations. As genAI performance progresses to a point where the majority of LLM responses are correct, even users who started out by verifying every statement may start to feel that their effort is not paying off. As users increasingly take AI outputs at face value, they might lose the expertise necessary to evaluate the accuracy of those outputs. As a Microsoft paper by Hank Lee identifies, operators who come to rely on genAI for routine decisions end up being unprepared for situations that require their intervention.

## A Finished Product Is Harder to Evaluate

While users can choose to collaborate with an LLM section by section to create a document or a working program, they can also just ask it to produce the entire text by itself. While this is a much faster way of getting an output, it creates additional problems when it comes to verifying the output's quality and troubleshooting its constituent parts.

Safety-systems researcher David D. Woods observed that the task of analyzing a whole is made far more difficult without having worked on the parts. The human counterpart has no context for any of the AI's decisions because they have no access to the system's "thought process." To effectively correct any technical issues in the solution, the user has to effectively solve the entire problem on their own in order to reconstruct the mental model necessary for identifying flaws.

## Designing Checkable AI Tools

The particulars of LLM technology may mean that, at least for the time being, hallucinations are here to stay. Responsibly engineered models should structure their outputs in a way that helps users identify these errors, by exposing the reasoning that leads to a conclusion or clearly communicating the degree of confidence for generated assertions. But the designers of genAI tools also have an opportunity to make error checking easier and more salient during interactions between users and LLMs.

### Followup Questions that Encourage Critical Thinking

Today's genAI chatbots often prompt followup questions; this feature could be used to embed fact-checking best practices into the tool. Rather than urging users to move on, these prompts could empower them to investigate the response with the same amount of skepticism that an editor should have for any submission. Prompts that ask for more details about sources or for the degree of certainty about the LLM's conclusion would encourage critical thinking and prevent users from taking the LLM's answer at face value.

![A Copilot screenshot edited to replace the prompts for follow-up questions at the end with How certain are you of this answer? Do other sources provide alterantive explanations? What are some reasons this answer may be incorrect?](https://media.nngroup.com/media/editor/2025/05/06/copilot-alt-prompts.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

### Highlight Referenced Text in the Source

Some existing chatbots can already provide citations for assertions in their responses. These citations are usually links to relevant webpages. However, users still need to do the work of verifying the accuracy of the citation: they have to click on the link, find the passage being referenced, and ensure that the context supports the meaning of the excerpt. Providing a deep link to the referenced passage or showing it in a preview of the source would greatly reduce the interaction cost of checking the LLM's assertions.

## Sources

- **NN Group Research**: [AI Chatbots Discourage Error Checking](https://www.nngroup.com/articles/ai-chatbots-discourage-error-checking/)
- **Prompt Engineering Guide**: [AI UX Patterns](https://www.promptingguide.ai/)
- **Anthropic Research**: [Building Effective AI Agents](https://www.anthropic.com/engineering/building-effective-agents)
- **CrewAI Documentation**: [Agent Design Patterns](https://docs.crewai.com/en/concepts/agents)
- **Claude Documentation**: [Best Practices for Claude](https://docs.anthropic.com/en/docs/best-practices)

## Figures

- Conversation flow diagrams showing good vs bad patterns
- Confidence indicator mockups
- Error handling state diagrams
- User control interface examples
- Progressive disclosure examples
- Accessibility interface mockups

