---
title: "AI UX Behavior & Design Patterns"
slug: "modules-ai-ux-behavior"
updatedAt: "2025-08-18"
tags: [module, ux, design, ai-patterns, claude, anthropic]
---

# AI UX Behavior & Design Patterns

> **Synthesis**: Understanding how users interact with AI systems and designing intuitive, trustworthy, and effective user experiences for AI-powered applications, with special focus on Claude's conversational capabilities.

## Why it's important for designers to know

AI UX behavior is fundamentally different from traditional software UX. Users interact with AI systems through conversation, expect intelligent responses, and need clear feedback about system capabilities and limitations. Designers must understand:

- **Conversational Design**: How to structure AI interactions that feel natural and productive
- **Trust & Transparency**: Building user confidence through clear communication about AI capabilities
- **Error Handling**: Graceful degradation when AI fails or provides unexpected responses
- **Progressive Disclosure**: Revealing AI capabilities gradually to avoid overwhelming users
- **Feedback Loops**: Providing clear signals about what the AI is doing and thinking

<Callout type="info">
  **Claude's UX Advantages**: Claude's constitutional AI principles and natural conversational abilities make it particularly well-suited for creating trustworthy, helpful user experiences.
</Callout>

## Claude's UX Strengths

### Natural Conversation Flow

<Card title="Claude's Conversational Design">
  <p>Claude excels at natural, helpful conversations that feel more human-like than traditional chatbots:</p>
  
  <h4>Conversational Capabilities:</h4>
  <ul>
    <li><strong>Context Awareness:</strong> Maintains conversation context across multiple turns</li>
    <li><strong>Natural Language:</strong> Understands and responds in natural, conversational language</li>
    <li><strong>Helpful Tone:</strong> Consistently helpful and supportive in interactions</li>
    <li><strong>Clarification Skills:</strong> Asks for clarification when needed</li>
    <li><strong>Proactive Assistance:</strong> Offers helpful suggestions and alternatives</li>
  </ul>
  
  <h4>UX Benefits:</h4>
  <ul>
    <li><strong>Reduced Friction:</strong> Users don't need to learn specific commands or syntax</li>
    <li><strong>Increased Trust:</strong> Natural conversation builds user confidence</li>
    <li><strong>Better Engagement:</strong> More engaging than rigid question-answer formats</li>
    <li><strong>Accessibility:</strong> Easier for users with varying technical skills</li>
  </ul>
</Card>

### Trust and Transparency

<Card title="Building Trust with Claude">
  <h4>Trust-Building Features:</h4>
  <ul>
    <li><strong>Honest Communication:</strong> Claude admits when it doesn't know something</li>
    <li><strong>Capability Clarity:</strong> Clear about what it can and cannot do</li>
    <li><strong>Safe Responses:</strong> Built-in safety prevents harmful outputs</li>
    <li><strong>Consistent Behavior:</strong> Predictable, reliable responses</li>
  </ul>
  
  <h4>Transparency Patterns:</h4>
  <ul>
    <li><strong>Confidence Indicators:</strong> Show when Claude is certain vs uncertain</li>
    <li><strong>Source Attribution:</strong> Cite sources when providing information</li>
    <li><strong>Limitation Disclosure:</strong> Clear about capabilities and constraints</li>
    <li><strong>Error Explanation:</strong> Explain why certain requests can't be fulfilled</li>
  </ul>
</Card>

## AI UX Design Patterns

<Tabs>
  <Tab title="Good Design" icon="check-circle">
    ### ‚úÖ Good AI UX Design with Claude
    
    **Progressive Disclosure**: Provides clear options without overwhelming
    - **Graceful Error Handling**: Acknowledges limitations and offers alternatives
    - **Confidence Transparency**: Shows clear confidence levels and reasoning
    - **User Control**: Gives users clear choices and control over complexity
    - **Contextual Awareness**: Maintains conversation context and relevance
    - **Claude Integration**: Leverages Claude's natural conversational abilities
    
    **Example Implementation:**
    ```python
    import anthropic
    
    class GoodClaudeUXSystem:
        def __init__(self):
            self.client = anthropic.Anthropic(api_key="your-api-key")
            self.context_manager = ContextManager()
            self.confidence_detector = ConfidenceDetector()
            self.user_preference_tracker = UserPreferenceTracker()
        
        def generate_response(self, user_input, conversation_history):
            # Progressive disclosure
            if self.is_broad_question(user_input):
                return self.generate_options_response(user_input)
            
            # Graceful error handling
            if not self.can_handle_request(user_input):
                return self.generate_alternative_suggestions(user_input)
            
            # Use Claude's natural conversation abilities
            response = self.client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=1000,
                messages=[
                    {
                        "role": "system",
                        "content": "You are a helpful assistant. Be conversational, honest about your capabilities, and offer alternatives when you can't help."
                    },
                    *conversation_history,
                    {
                        "role": "user",
                        "content": user_input
                    }
                ]
            )
            
            # Add confidence indicators
            confidence = self.confidence_detector.assess(user_input, response.content[0].text)
            
            # User control based on preferences
            if self.user_preference_tracker.prefers_simple(user_input):
                return self.generate_simple_response(response.content[0].text, confidence)
            else:
                return self.generate_detailed_response(response.content[0].text, confidence)
        
        def generate_options_response(self, question):
            return {
                "response": "I'd be happy to help! What specific area are you working on?",
                "options": ["Basic concepts", "Advanced topics", "Practical examples"],
                "confidence": "high",
                "user_control": True,
                "claude_enhanced": True
            }
    ```
  </Tab>
  
  <Tab title="Poor Design" icon="x-circle">
    ### ‚ùå Poor AI UX Design
    
    **Information Overload**: Dumps too much information without guidance
    - **Abrupt Failures**: Provides no recovery path when errors occur
    - **Uncertain Responses**: Shows lack of confidence without explanation
    - **No User Control**: Assumes user wants maximum complexity
    - **Context Loss**: Fails to maintain conversation flow
    - **Ignoring Claude Strengths**: Doesn't leverage Claude's conversational abilities
    
    **Example Implementation:**
    ```python
    class PoorAIUXSystem:
        def __init__(self):
            # No context management
            # No confidence detection
            # No user preference tracking
            # No Claude integration
        
        def generate_response(self, user_input):
            # Information dump without structure
            if self.is_broad_question(user_input):
                return self.dump_all_information(user_input)
            
            # Abrupt failure
            if not self.can_handle_request(user_input):
                return "I cannot help with that."
            
            # No confidence assessment
            # No user control
            # No context consideration
            # No Claude integration
            return self.generate_verbose_response(user_input)
        
        def dump_all_information(self, question):
            return {
                "response": "Here's everything about this topic... [500+ words]",
                "confidence": "unknown",
                "user_control": False,
                "context_aware": False,
                "claude_enhanced": False
            }
    ```
  </Tab>
</Tabs>

## Claude-Specific UX Patterns

### 1. **Conversational Onboarding**

<Card title="Onboarding with Claude">
  <h4>Design Principles:</h4>
  <ul>
    <li><strong>Welcome Message:</strong> Friendly, helpful introduction</li>
    <li><strong>Capability Overview:</strong> Clear explanation of what Claude can do</li>
    <li><strong>Example Interactions:</strong> Show users how to interact effectively</li>
    <li><strong>Progressive Discovery:</strong> Reveal advanced features gradually</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  <pre><code>def create_claude_onboarding():
    welcome_message = """
    Hi! I'm Claude, your AI assistant. I can help you with:
    
    ‚Ä¢ Answering questions and explaining concepts
    ‚Ä¢ Analyzing documents and data
    ‚Ä¢ Writing and editing content
    ‚Ä¢ Solving problems and brainstorming ideas
    
    Just ask me anything! I'm here to help.
    
    üí° Tip: Be specific with your questions for better answers.
    """
    
    return {
        "message": welcome_message,
        "suggestions": [
            "What can you help me with?",
            "Show me an example of your capabilities",
            "How do I get the best results?"
        ],
        "next_steps": "Try asking me a question or uploading a document!"
    }
</code></pre>
</Card>

### 2. **Confidence and Uncertainty**

<Card title="Confidence Indicators with Claude">
  <h4>Confidence Levels:</h4>
  <ul>
    <li><strong>High Confidence:</strong> Clear, direct answers with sources</li>
    <li><strong>Medium Confidence:</strong> Qualified responses with caveats</li>
    <li><strong>Low Confidence:</strong> Honest uncertainty with alternatives</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def generate_confidence_indicator(response_text, confidence_score):
    if confidence_score >= 0.8:
        return {
            "response": response_text,
            "confidence": "high",
            "indicator": "‚úÖ",
            "message": "I'm confident about this information."
        }
    elif confidence_score >= 0.5:
        return {
            "response": response_text,
            "confidence": "medium",
            "indicator": "ü§î",
            "message": "This is my best understanding, but you may want to verify."
        }
    else:
        return {
            "response": response_text,
            "confidence": "low",
            "indicator": "‚ùì",
            "message": "I'm not entirely sure about this. Here are some alternatives..."
        }
</code></pre>
</Card>

### 3. **Error Handling and Recovery**

<Card title="Graceful Error Handling">
  <h4>Error Types and Responses:</h4>
  <ul>
    <li><strong>Unclear Requests:</strong> Ask for clarification</li>
    <li><strong>Out of Scope:</strong> Explain limitations and suggest alternatives</li>
    <li><strong>Technical Issues:</strong> Provide helpful error messages</li>
    <li><strong>Safety Concerns:</strong> Explain why certain requests can't be fulfilled</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def handle_claude_errors(error_type, user_input):
    error_handlers = {
        "unclear": {
            "message": "I'm not sure I understand. Could you rephrase that?",
            "suggestions": ["Can you be more specific?", "What exactly are you looking for?"],
            "help_text": "Try being more specific about what you need help with."
        },
        "out_of_scope": {
            "message": "I can't help with that specific request, but I can help with related topics.",
            "suggestions": ["What's your main goal?", "Is there something else I can help with?"],
            "help_text": "I'm designed to help with information, analysis, and creative tasks."
        },
        "technical": {
            "message": "I'm having trouble processing that. Let's try a different approach.",
            "suggestions": ["Try rephrasing your question", "Break it down into smaller parts"],
            "help_text": "Sometimes simpler, more direct questions work better."
        },
        "safety": {
            "message": "I can't help with that request, but I'm happy to help with other things.",
            "suggestions": ["What's your main goal?", "How else can I assist you?"],
            "help_text": "I'm designed to be helpful while maintaining safety and ethical standards."
        }
    }
    
    handler = error_handlers.get(error_type, error_handlers["unclear"])
    
    return {
        "response": handler["message"],
        "suggestions": handler["suggestions"],
        "help_text": handler["help_text"],
        "error_type": error_type
    }
</code></pre>
</Card>

### 4. **Progressive Disclosure**

<Card title="Progressive Information Disclosure">
  <h4>Disclosure Strategy:</h4>
  <ul>
    <li><strong>Start Simple:</strong> Provide basic information first</li>
    <li><strong>Offer Details:</strong> Give users the option to learn more</li>
    <li><strong>Contextual Help:</strong> Provide help when users need it</li>
    <li><strong>User Control:</strong> Let users choose their level of detail</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def progressive_disclosure_response(user_input, detail_level="basic"):
    if detail_level == "basic":
        return {
            "response": "Here's a simple answer to your question...",
            "show_more": True,
            "more_options": ["Detailed explanation", "Examples", "Related topics"]
        }
    elif detail_level == "detailed":
        return {
            "response": "Here's a comprehensive answer with examples and context...",
            "show_more": False,
            "related_topics": ["Topic 1", "Topic 2", "Topic 3"]
        }
    
    return {
        "response": "I'd be happy to help! What level of detail would you prefer?",
        "options": ["Quick answer", "Detailed explanation", "Step-by-step guide"]
    }
</code></pre>
</Card>

## Advanced UX Patterns

### 1. **Multi-Modal Interactions**

<Card title="Claude's Multi-Modal UX">
  <h4>Interaction Types:</h4>
  <ul>
    <li><strong>Text + Vision:</strong> Analyze images while discussing text</li>
    <li><strong>Document Analysis:</strong> Process and explain documents</li>
    <li><strong>Data Visualization:</strong> Interpret charts and graphs</li>
    <li><strong>Code Review:</strong> Analyze code screenshots</li>
  </ul>
  
  <h4>UX Considerations:</h4>
  <ul>
    <li><strong>Clear Upload Interface:</strong> Easy file upload and drag-and-drop</li>
    <li><strong>Processing Feedback:</strong> Show when Claude is analyzing content</li>
    <li><strong>Context Preservation:</strong> Maintain conversation context with visual content</li>
    <li><strong>Accessibility:</strong> Provide text alternatives for visual content</li>
  </ul>
</Card>

### 2. **Tool Integration UX**

<Card title="Tool Use User Experience">
  <h4>Tool Interaction Patterns:</h4>
  <ul>
    <li><strong>Automatic Tool Selection:</strong> Claude chooses appropriate tools</li>
    <li><strong>Tool Execution Feedback:</strong> Show when tools are being used</li>
    <li><strong>Result Integration:</strong> Seamlessly incorporate tool results</li>
    <li><strong>Error Handling:</strong> Graceful handling of tool failures</li>
  </ul>
  
  <h4>Implementation Example:</h4>
  <pre><code>def tool_integration_ux(tool_name, tool_result, user_query):
    tool_messages = {
        "web_search": "I searched the web for current information about this...",
        "calculator": "I calculated the result for you...",
        "file_reader": "I analyzed the document you provided...",
        "code_executor": "I ran the code and here are the results..."
    }
    
    return {
        "tool_used": tool_name,
        "tool_message": tool_messages.get(tool_name, "I used a tool to help with your request..."),
        "result": tool_result,
        "context": "Based on the " + tool_name + " results, here's what I found...",
        "user_query": user_query
    }
</code></pre>
</Card>

### 3. **Personalization and Adaptation**

<Card title="Personalized UX with Claude">
  <h4>Personalization Features:</h4>
  <ul>
    <li><strong>User Preferences:</strong> Remember user's preferred detail level</li>
    <li><strong>Interaction History:</strong> Adapt based on previous conversations</li>
    <li><strong>Domain Expertise:</strong> Adjust responses based on user's knowledge level</li>
    <li><strong>Communication Style:</strong> Match user's preferred tone and style</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>class PersonalizedClaudeUX:
    def __init__(self):
        self.user_profiles = \{\}
        self.interaction_history = \{\}
    
    def adapt_response(self, user_id, response, user_input):
        profile = self.user_profiles.get(user_id, \{\})
        
        # Adapt detail level
        if profile.get("prefers_simple"):
            response = self.simplify_response(response)
        
        # Adapt tone
        if profile.get("prefers_formal"):
            response = self.make_formal(response)
        elif profile.get("prefers_casual"):
            response = self.make_casual(response)
        
        # Adapt based on expertise level
        expertise_level = profile.get("expertise_level", "intermediate")
        response = self.adjust_for_expertise(response, expertise_level)
        
        return response
    
    def update_user_profile(self, user_id, interaction_data):
        # Update user preferences based on interactions
        if user_id not in self.user_profiles:
            self.user_profiles[user_id] = \{\}
        
        # Analyze interaction patterns
        detail_preference = self.analyze_detail_preference(interaction_data)
        tone_preference = self.analyze_tone_preference(interaction_data)
        expertise_level = self.analyze_expertise_level(interaction_data)
        
        self.user_profiles[user_id].update(\{
            "prefers_simple": detail_preference == "simple",
            "prefers_formal": tone_preference == "formal",
            "expertise_level": expertise_level
        \})
</code></pre>
</Card>

## Performance and Responsiveness

### 1. **Streaming Responses**

<Card title="Real-Time Feedback with Claude">
  <h4>Streaming Benefits:</h4>
  <ul>
    <li><strong>Perceived Performance:</strong> Users see immediate feedback</li>
    <li><strong>Engagement:</strong> Keeps users engaged during processing</li>
    <li><strong>Interruption Support:</strong> Users can interrupt if needed</li>
    <li><strong>Progress Indication:</strong> Shows that work is being done</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>async def stream_claude_response(user_input):
    async with anthropic.Anthropic(api_key="your-api-key") as client:
        stream = await client.messages.create(
            model="claude-3-sonnet-20240229",
            max_tokens=1000,
            messages=[\{"role": "user", "content": user_input\}],
            stream=True
        )
        
        async for chunk in stream:
            if chunk.type == "content_block_delta":
                yield \{
                    "type": "content",
                    "text": chunk.delta.text,
                    "partial": True
                \}
            elif chunk.type == "message_stop":
                yield \{
                    "type": "complete",
                    "text": "",
                    "partial": False
                \}
</code></pre>
</Card>

### 2. **Loading States and Feedback**

<Card title="Loading State Design">
  <h4>Loading Patterns:</h4>
  <ul>
    <li><strong>Typing Indicators:</strong> Show Claude is "thinking"</li>
    <li><strong>Progress Bars:</strong> For longer operations</li>
    <li><strong>Status Messages:</strong> Explain what's happening</li>
    <li><strong>Cancel Options:</strong> Allow users to stop processing</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def create_loading_states():
    return {
        "thinking": {
            "message": "Claude is thinking...",
            "icon": "ü§î",
            "animation": "pulse"
        },
        "analyzing": {
            "message": "Analyzing your request...",
            "icon": "üîç",
            "animation": "spin"
        },
        "searching": {
            "message": "Searching for information...",
            "icon": "üîé",
            "animation": "bounce"
        },
        "processing": {
            "message": "Processing your input...",
            "icon": "‚öôÔ∏è",
            "animation": "rotate"
        }
    }
</code></pre>
</Card>

## Accessibility and Inclusion

### 1. **Universal Design Principles**

<Card title="Accessible AI UX">
  <h4>Accessibility Features:</h4>
  <ul>
    <li><strong>Screen Reader Support:</strong> Proper ARIA labels and descriptions</li>
    <li><strong>Keyboard Navigation:</strong> Full keyboard accessibility</li>
    <li><strong>High Contrast:</strong> Support for high contrast modes</li>
    <li><strong>Font Scaling:</strong> Responsive text sizing</li>
    <li><strong>Alternative Input:</strong> Voice input and other modalities</li>
  </ul>
  
  <h4>Implementation Guidelines:</h4>
  <pre><code>def create_accessible_interface():
    return {
        "aria_labels": {
            "chat_input": "Type your message to Claude",
            "send_button": "Send message to Claude",
            "upload_button": "Upload file for Claude to analyze",
            "settings_button": "Open Claude settings"
        },
        "keyboard_shortcuts": {
            "send_message": "Enter",
            "new_conversation": "Ctrl+N",
            "upload_file": "Ctrl+U",
            "settings": "Ctrl+,"
        },
        "focus_management": {
            "auto_focus": "chat_input",
            "focus_after_send": "chat_input",
            "focus_after_upload": "chat_input"
        }
    }
</code></pre>
</Card>

### 2. **Inclusive Language and Content**

<Card title="Inclusive AI Interactions">
  <h4>Inclusive Design Principles:</h4>
  <ul>
    <li><strong>Diverse Examples:</strong> Use examples from various cultures and backgrounds</li>
    <li><strong>Gender-Neutral Language:</strong> Avoid assumptions about gender</li>
    <li><strong>Cultural Sensitivity:</strong> Respect different cultural perspectives</li>
    <li><strong>Age-Appropriate Content:</strong> Adapt content for different age groups</li>
  </ul>
  
  <h4>Implementation:</h4>
  <pre><code>def create_inclusive_prompts():
    return {
        "system_prompt": """
        You are a helpful, inclusive AI assistant. When providing examples or references:
        - Use diverse examples from various cultures and backgrounds
        - Use gender-neutral language when possible
        - Be culturally sensitive and respectful
        - Consider different perspectives and experiences
        - Avoid stereotypes and assumptions
        """,
        "example_prompts": [
            "Can you provide examples from different cultural perspectives?",
            "How might this apply to people from various backgrounds?",
            "What are some alternative approaches to consider?"
        ]
    }
</code></pre>
</Card>

## Testing and Evaluation

### 1. **UX Testing Methods**

<Card title="AI UX Testing">
  <h4>Testing Approaches:</h4>
  <ul>
    <li><strong>Usability Testing:</strong> Observe real users interacting with Claude</li>
    <li><strong>A/B Testing:</strong> Compare different UX approaches</li>
    <li><strong>Conversation Analysis:</strong> Analyze conversation patterns and flows</li>
    <li><strong>Accessibility Testing:</strong> Test with users who have disabilities</li>
  </ul>
  
  <h4>Testing Metrics:</h4>
  <ul>
    <li><strong>Task Completion Rate:</strong> How often users achieve their goals</li>
    <li><strong>Time to Completion:</strong> How long tasks take</li>
    <li><strong>User Satisfaction:</strong> Subjective ratings and feedback</li>
    <li><strong>Error Rate:</strong> How often users encounter problems</li>
  </ul>
</Card>

### 2. **Continuous Improvement**

<Card title="Iterative UX Improvement">
  <h4>Improvement Process:</h4>
  <ul>
    <li><strong>Data Collection:</strong> Gather usage data and user feedback</li>
    <li><strong>Pattern Analysis:</strong> Identify common issues and opportunities</li>
    <li><strong>Hypothesis Formation:</strong> Develop theories about improvements</li>
    <li><strong>Testing and Validation:</strong> Test improvements with users</li>
    <li><strong>Implementation:</strong> Deploy successful improvements</li>
  </ul>
  
  <h4>Feedback Loops:</h4>
  <pre><code>def create_feedback_system():
    return {
        "user_feedback": {
            "thumbs_up": "Track positive interactions",
            "thumbs_down": "Track negative interactions",
            "text_feedback": "Collect detailed user comments",
            "suggestion_box": "Allow feature requests"
        },
        "analytics": {
            "conversation_length": "Track conversation duration",
            "completion_rate": "Track task completion",
            "error_frequency": "Track common errors",
            "user_satisfaction": "Track satisfaction scores"
        },
        "improvement_cycle": {
            "collect": "Gather data and feedback",
            "analyze": "Identify patterns and issues",
            "design": "Create improvement hypotheses",
            "test": "Validate improvements",
            "implement": "Deploy successful changes"
        }
    }
</code></pre>
</Card>

## Key Design Principles

### 1. **Progressive Disclosure**
- Start simple, add complexity on demand
- Provide clear navigation paths
- Don't overwhelm with options upfront

### 2. **Transparency & Trust**
- Be clear about AI capabilities and limitations
- Show confidence levels when appropriate
- Explain reasoning when possible
- Leverage Claude's built-in honesty

### 3. **Graceful Degradation**
- Always provide a path forward
- Offer alternatives when primary approach fails
- Maintain usefulness even in error states

### 4. **User Control & Freedom**
- Give users clear choices and control
- Allow interruption and redirection
- Provide escape hatches from complex paths

### 5. **Consistent Feedback**
- Show what the AI is doing
- Provide clear success/error states
- Maintain conversation context

### 6. **Natural Conversation**
- Leverage Claude's conversational abilities
- Maintain context across interactions
- Use natural language patterns
- Provide helpful, supportive responses

## Collaboration prompts for engineers

### For Frontend Developers
```
"Can we add a confidence indicator that shows when Claude is certain vs uncertain about its responses? This helps users understand when to trust the AI's advice."
```

### For Backend Developers
```
"We need to implement streaming responses so users see Claude working in real-time. This reduces perceived latency and builds trust."
```

### For UX Researchers
```
"Let's A/B test different error message formats to see which ones help users recover and continue their workflow most effectively."
```

### For Product Managers
```
"We should track user satisfaction after Claude interactions to understand which UX patterns lead to better outcomes and user retention."
```

### For Accessibility Specialists
```
"How can we ensure our Claude interface is fully accessible to users with disabilities? What additional features do we need?"
```

## Related Concepts

<CardGroup cols={2}>
  <Card title="Prompt Engineering" icon="edit" href="../prompting-techniques/chain-of-thought">
    Design effective prompts for Claude
  </Card>
  <Card title="Evaluation & Observability" icon="monitor" href="../evaluation-observability">
    Measure UX effectiveness
  </Card>
  <Card title="Safety & Security" icon="shield" href="../safety-security">
    Build trustworthy AI experiences
  </Card>
  <Card title="Multimodality" icon="image" href="../multimodality">
    Design for multiple interaction types
  </Card>
  <Card title="Tool Use" icon="wrench" href="../prompting-techniques/react">
    Integrate external tools seamlessly
  </Card>
  <Card title="Streaming UX" icon="zap" href="../streaming-ux">
    Real-time interaction patterns
  </Card>
</CardGroup>

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [How To Design Effective Conversational AI Experiences: A Comprehensive Guide](https://www.smashingmagazine.com/2024/07/how-design-effective-conversational-ai-experiences-guide/)  
> for internal educational use only (non-profit).

# How To Design Effective Conversational AI Experiences: A Comprehensive Guide

Conversational AI is revolutionizing information access, offering a personalized, intuitive search experience that delights users and empowers businesses. A well-designed conversational agent acts as a knowledgeable guide, understanding user intent and effortlessly navigating vast data, which leads to happier, more engaged users, fostering loyalty and trust. Meanwhile, businesses benefit from increased efficiency, reduced costs, and a stronger bottom line. On the other hand, a poorly designed system can lead to frustration, confusion, and, ultimately, abandonment.

Achieving success with conversational AI requires more than just deploying a chatbot. To truly harness this technology, we must master the intricate dynamics of human-AI interaction. This involves understanding how users articulate needs, explore results, and refine queries, paving the way for a seamless and effective search experience.

This article will decode the three phases of conversational search, the challenges users face at each stage, and the strategies and best practices AI agents can employ to enhance the experience.

## The Three Phases Of Conversational Search

To analyze these complex interactions, Trippas et al. (2018) (PDF) proposed a framework that outlines three core phases in the conversational search process:

1. **Query formulation**: Users express their information needs, often facing challenges in articulating them clearly.
2. **Search results exploration**: Users navigate through presented results, seeking further information and refining their understanding.
3. **Query re-formulation**: Users refine their search based on new insights, adapting their queries and exploring different avenues.

Building on this framework, Azzopardi et al. (2018) (PDF) identified five key user actions within these phases: reveal, inquire, navigate, interrupt, interrogate, and the corresponding agent actions ‚Äî inquire, reveal, traverse, suggest, and explain.

![Table created by the author based on Azzopardi et al.'s paper](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/how-design-effective-conversational-ai-experiences-guide/1-table-three-phases-conversational-search.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

In the following sections, I'll break down each phase of the conversational search journey, delving into the actions users take and the corresponding strategies AI agents can employ, as identified by Azzopardi et al. (2018) (PDF). I'll also share actionable tactics and real-world examples to guide the implementation of these strategies.

## Phase 1: Query Formulation: The Art Of Articulation

In the initial phase of query formulation, users attempt to translate their needs into prompts. This process involves conscious disclosures ‚Äî sharing details they believe are relevant ‚Äî and unconscious non-disclosure ‚Äî omitting information they may not deem important or struggle to articulate.

This process is fraught with challenges. As Jakob Nielsen aptly pointed out,

> "Articulating ideas in written prose is hard. Most likely, half the population can't do it. This is a usability problem for current prompt-based AI user interfaces."‚Äî Jakob Nielsen

This can manifest as:

- **Vague language**: "I need help with my finances." Budgeting? Investing? Debt management?
- **Missing details**: "I need a new pair of shoes." What type of shoes? For what purpose?
- **Limited vocabulary**: Not knowing the right technical terms. "I think I have a sprain in my ankle." The user might not know the difference between a sprain and a strain or the correct anatomical terms.

These challenges can lead to frustration for users and less relevant results from the AI agent.

### AI Agent Strategies: Nudging Users Towards Better Input

To bridge the articulation gap, AI agents can employ three core strategies:

1. **Elicit**: Proactively guide users to provide more information.
2. **Clarify**: Seek to resolve ambiguities in the user's query.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [When Words Cannot Describe: Designing For AI Beyond Conversational Interfaces](https://www.smashingmagazine.com/2024/02/designing-ai-beyond-conversational-interfaces/)  
> for internal educational use only (non-profit).

# When Words Cannot Describe: Designing For AI Beyond Conversational Interfaces

Few technological innovations can completely change the way we interact with computers. Lucky for us, it seems we've won front-row seats to the unfolding of the next paradigm shift.

These shifts tend to unlock a new abstraction layer to hide the working details of a subsystem. Generalizing details allows our complex systems to appear simpler & more intuitive. This streamlines coding programs for computers as well as designing the interfaces to interact with them.

The Command Line Interface, for instance, created an abstraction layer to enable interaction through a stored program. This hid the subsystem details once exposed in earlier computers that were only programmable by inputting 1s & 0s through switches.

Graphical User Interfaces (GUI) further abstracted this notion by allowing us to manipulate computers through visual metaphors. These abstractions made computers accessible to a mainstream of non-technical users.

Despite these advances, we still haven't found a perfectly intuitive interface ‚Äî the troves of support articles across the web make that evident. Yet recent advances in AI have convinced many technologists that the next evolutionary cycle of computing is upon us.

![An animation by Maximillian Piras depicting a history of interface abstraction: three panes of glass float above each other & each one displays a different interface style. The bottom layer is a command line interface represented by green code on a black screen. The middle layer is a GUI represented by a desktop with icons representing folders. The top layer is an AI-powered conversational interface represented by a chatbot asking, "How can I help you today?".](https://files.smashing.media/articles/designing-ai-beyond-conversational-interfaces/1-smashing-abstraction-intro.gif)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

## The Next Layer Of Interface Abstraction

A branch of machine learning called generative AI drives the bulk of recent innovation. It leverages pattern recognition in datasets to establish probabilistic distributions that enable novel constructions of text, media, & code. Bill Gates believes it's "the most important advance in technology since the graphical user interface" because it can make controlling computers even easier. A newfound ability to interpret unstructured data, such as natural language, unlocks new inputs & outputs to enable novel form factors.

Now our universe of information can be instantly invoked through an interface as intuitive as talking to another human. These are the computers we've dreamed of in science fiction, akin to systems like Data from Star Trek. Perhaps computers up to this point were only prototypes & we're now getting to the actual product launch. Imagine if building the internet was laying down the tracks, AIs could be the trains to transport all of our information at breakneck speed & we're about to see what happens when they barrel into town.

> "Soon the pre-AI period will seem as distant as the days when using a computer meant typing at a C:> prompt rather than tapping on a screen."‚Äî Bill Gates in "The Age of AI Has Begun"

If everything is about to change, so must the mental models of software designers. As Luke Wroblewski once popularized mobile-first design, the next zeitgeist is likely AI-first. Only through understanding AI's constraints & capabilities can we craft delight. Its influence on the discourse of interface evolution has already begun.

Large Language Models (LLMs), for instance, are a type of AI utilized in many new applications & their text-based nature leads many to believe a conversational interface, such as a chatbot, is a fitting form for the future. The notion that AI is something you talk to has been permeating across the industry for years. Robb Wilson, the co-owner of UX Magazine, calls conversation "the infinitely scalable interface" in his book The Age of Invisible Machines (2022). Noah Levin, Figma's VP of Product Design, contends that "it's a very intuitive thing to learn how to talk to something." Even a herald of GUIs such as Bill Gates posits that "our main way of controlling a computer will no longer be pointing and clicking."

![An animation displays a new button in Microsoft PowerPoint that allows users to make edits through a chat window.](https://files.smashing.media/articles/designing-ai-beyond-conversational-interfaces/2-smashing-abstraction-mscopilot-800px.gif)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

The hope is that conversational computers will flatten learning curves. Jesse Lyu, the founder of Rabbit, asserts that a natural language approach will be "so intuitive that you don't even need to learn how to use it."

After all, it's not as if Data from Stark Trek came with an instruction manual or onboarding tutorial. From this perspective, the evolutionary tale of conversational interfaces superseding GUIs seems logical & echoes the earlier shift away from command lines. But others have opposing opinions, some going as far as Maggie Appleton to call conversational interfaces like chatbots "the lazy solution."

This might seem like a schism at first, but it's more so a symptom of a simplistic framing of interface evolution. Command lines are far from extinct; technical users still prefer them for their greater flexibility & efficiency. For use cases like software development or automation scripting, the added abstraction layer in graphical no-code tools can act as a barrier rather than a bridge.

> GUIs were revolutionary but not a panacea. Yet there is ample research to suggest conversational interfaces won't be one, either. For certain interactions, they can decrease usability, increase cost, & introduce security risk relative to GUIs.

So, what is the right interface for artificially intelligent applications? This article aims to inform that design decision by contrasting the capabilities & constraints of conversation as an interface.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Developing A Chatbot Using Microsoft's Bot Framework, LUIS And Node.js (Part 1)](https://www.smashingmagazine.com/2017/05/chatbot-microsoft-bot-framework-luis-nodejs-part1/)  
> for internal educational use only (non-profit).

# Developing A Chatbot Using Microsoft's Bot Framework, LUIS And Node.js (Part 1)

This tutorial gives you hands-on access to my journey of creating a digital assistant capable of connecting with any system via a RESTful API to perform various tasks.
Here, I'll be demonstrating how to save a user's basic information and create a new project on their behalf via natural language processing (NLP).

Note: I am not associated with Microsoft in any way (either directly or indirectly).

## Choosing A Bot Framework

The rising wave of artificial intelligence (AI) in the last couple of years has given a massive push to the idea of conversational interfaces, commonly known as chatbots. Creating a high-performing chatbot that can understand natural language was a tedious and complex task a few years back, but like all other user interfaces, we saw some promising advancements in chatbot-building platforms as well.

The tech community has joined forces to boost the growth of various aspects of AI, including natural language processing and machine learning. Big players such as Facebook, Microsoft, IBM and Google have tirelessly been creating platforms and tools to help developers and entrepreneurs ease the process of integrating such technology into their own products and businesses.

Platforms such as Slack, Facebook Messenger, Pandorabots, Telegram, Wit.ai and Microsoft's Bot Framework have given thousands of technopreneurs like me a strong foundation on which to create useful chatbots that can be integrated with existing platforms and apps in minutes. Some of these frameworks are confined to their own platforms, like Slack and Facebook Messenger, while others integrate with multiple platforms, which saves a lot of effort if you're aiming for a cross-platform launch. Matching user intent and finding entities from their utterances are the foundation on which these frameworks are built. The most extensive suites of cognitive services that can add true intelligence to your bot are offered by both IBM and Microsoft. Both companies have been investing heavily in this domain, and their services can be used as an extension of the "mind" of your bot.

A few months back, I wrote an article on TechCrunch discussing the role of AI and chatbots in the web development industry. This was the same time when I saw that Bot Framework was picking up speed. Microsoft made its Bot Builder available as an open-source SDK, not just on .NET, but also on Node.js and a REST API.

At this time, I was in discussions with Iflexion and Hema Maps about creating chatbots with extensive natural language and AI capabilities for their future projects. During these meetings, I identified the need to eventually add machine learning as well. So, I thought, why not give Microsoft's products a try?

## The Chatbot Journey

### Signing Up And Creating The App

The first part is pretty convenient if you already have a Microsoft account (a work, school or personal account). Just use that to sign in on https://dev.botframework.com/. After you log in, there's a button to "Register a Bot," which takes you to a page asking you to fill in some information about the bot:

- **name**: A friendly name for your bot.
- **bot handle**: Used in the URL for your bot. Cannot be changed once registered.
- **description**: Displayed on your card in the bot directory, if you choose to list it there.
- **messaging end point**: The URL at which your bot will live. You'll get this URL when setting up the server environment.
- **app ID**: Obtained by clicking the "Create Microsoft App ID and password" button above. While creating your app ID, you'll be asked to create a password. Copy and save this password safely: You'll need it in the bot configuration file.
- **owners**: A comma-separated list of email IDs for people who will have editing rights to this bot.
- **instrumentation key**: The Azure App Insights key, if you want to receive analytics about your bot.

Once you have filled in all the information, you can click on "Register."

### Setting Up The Local Environment

We will first set up our local machine to test the bot via the Bot Framework Emulator on Ubuntu (also available for Mac and Windows). When I first downloaded the Bot Framework Emulator for Linux, it was just a version for the command-line interface. Luckily, it soon got replaced with a GUI-based AppImage version, which runs directly without installation.

Because JavaScript is one of my primary coding languages, I chose the Node.js version of the Bot Builder SDK. To run it, you'll obviously need to install Node.js on your system. I used the Linux package manager to do so, but you can select the method that is convenient for you.

```
$ sudo apt install nodejs
```

Once Node.js is installed, create a new folder and a blank JavaScript file in a location of your choice. I'll use /var/www/aplostestbot as my root folder and create a file, aplostestbot.js, inside it. Open a new command-line editor and change the current working directory to the bot's folder. Now, run the following commands to fetch the two dependencies that we need to build our bot:

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [How AI Succeeds (and Fails) to Help People Find Information](https://www.nngroup.com/articles/ai-information-seeking-keyword-foraging/)  
> for internal educational use only (non-profit).

# How AI Succeeds (and Fails) to Help People Find Information

AI chatbots and AI-driven search can help people find what they're looking for ‚Äî even if they don't know what they're looking for. In this way, generative AI tools can greatly alleviate a longstanding major pain point of a search-driven web. However, AI's ability to help with this problem is still limited. In particular, many consumers are simply not aware of how powerful generative AI tools can be in information-seeking, and don't know how to prompt to achieve better outcomes.

## In This Article:

- Keyword Foraging in Traditional Search
- AI Streamlines Simple Keyword Foraging
- Articulation Challenges in Searching and Prompting: An Example from User Testing
- AI Doesn't Yet Eliminate Information-Seeking Challenges

## Keyword Foraging in Traditional Search

As one of our participants in recent qualitative studies on AI told us, "You don't know what you don't know."

That's an eternal challenge when humans seek to answer a question or learn about a topic. With traditional web search, users are severely limited by their ability to articulate their need. They have to know what they're looking for in order to provide keywords to a search engine.

This challenge leads to a user behavior I've named keyword foraging ‚Äî the search before the search.

> When keyword foraging, a user conducts a preliminary search (usually in a web search engine like Google) to determine the right keywords for their information need.

For example, a shopper may want to buy that Y-shaped peeler that bartenders use to remove strips of citrus peel for cocktails but doesn't know it's called a "channel knife." Before she can find the term that will help her get what she wants, the user will need to do some awkward failing through search engines and websites.

## AI Streamlines Simple Keyword Foraging

Because generative AI chats and AI-powered search engines can accept wordy, complex questions, keywords are less of a barrier than they used to be. The ability to express an information need in full sentences, without proper terminology, is extremely powerful.

In the channel-knife example, the shopper can simply explain her goal and describe the thing she wants ‚Äî it doesn't matter if she doesn't know the actual term.

![Google's AI Mode: By articulating the information need in natural language, users can easily work around the keyword-foraging problem.](https://media.nngroup.com/media/editor/2025/08/13/keyword-channel-knife-gemini.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

For simple situations like this one, where the user knows what they need but does not know the correct keyword, generative AI greatly facilitates keyword foraging.

However, keyword foraging still represents an extra initial step before the user can search for channel knives to purchase (her actual goal). You'd expect Google's AI Mode to pull in some options from Google Shopping in this instance, but it failed to.

In contrast, when I gave the same prompt to Amazon's Rufus (its AI-powered shopping assistant), Rufus provided the correct term and some links to purchase options.

![Amazon Rufus provided the term the user was looking for, but it also proactively offered some product options.](https://media.nngroup.com/media/editor/2025/08/13/keyword-channel-knife-rufus.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Articulation Challenges in Searching and Prompting: An Example from User Testing

It can be annoying and tedious to look for the right name of the thing you want to search for. But it's even more difficult when you don't know or can't explain what you need in the first place. This situation goes beyond simple keyword foraging ‚Äî before you can even begin to forage for keywords, you need to explain the problem and identify the potential solution.

For example, one participant in our recent study had noticed that a drainage pipe outside his home was leaking and wanted to fix it by himself. He faced several challenges with this information need: to find a repair video, he needed to search for the right terms, but he had no plumbing vocabulary and no clear diagnosis of his issue.

He started at classic Google (his "search engine of choice") and tried a traditional search query: repair outside drainage pipes uk. He immediately checked the resulting AI overview but was disappointed. He noticed the word "trenchless" and realized that the system was not accurately understanding his situation ‚Äî it was advising on repairing underground wastewater pipes, while he was dealing with an above-ground pipe problem. "It's going off in the wrong direction," he told us.

![Google AI overview: The participant struggled to communicate his plumbing problem. Google's resulting AI overview gave tips for repairing a different type of pipe, which was not helpful for the participant.](https://media.nngroup.com/media/editor/2025/08/13/keyword-drain-uk.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

He reflected that he probably wasn't accurately describing the problem in his query, saying:

> "The terms in my prompt might be wrong. I need to change my terms."

He tried three iterations of his search query but kept getting similar results.

- repair outside drainage pipes uk
- repair outside drainage pipes fixed to exterior wall uk
- repair outside waste water pipes fixed to exterior wall uk

"At this point, I'm getting quite frustrated," he said. He proceeded to visit websites, including Reddit and YouTube, vainly looking for information scent to indicate he was moving in the right direction. After multiple disappointments over nearly 15 minutes, he told the facilitator that he might give up and just call a plumber to fix the problem.

The facilitator asked him to attempt his task with Gemini. This was the user's first time using AI chat for something other than help writing emails and documents. His first prompt simply rephrased his previous queries in question format:

> how do i fix leaking outside waste water drain pipes in the uk

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [How AI Is Changing Search Behaviors](https://www.nngroup.com/articles/ai-changing-search-behaviors/)  
> for internal educational use only (non-profit).

# How AI Is Changing Search Behaviors

Generative AI (genAI) is reshaping how people search for information. Anyone watching their content pageviews decline is currently experiencing the impact of this. But what's behind the shift? The speed of the change is impressive, considering how deeply ingrained information-seeking habits can be.

In a recent qualitative study, we asked people to bring their own research tasks into the virtual lab. We explored how users' information-seeking behaviors are shifting in response to AI-powered search tools and chatbots. While AI offers compelling shortcuts around tedious research tasks, it isn't close to completely replacing traditional search. But, even when people are using traditional search, the AI-generated overview that now tops almost all search-results pages steals a significant amount of attention and often shortcuts the need to visit the actual pages.

## In This Article:

- Search Habits Are Hard to Break
- AI Changes Information-Seeking Habits
- AI Doesn't Eliminate the Need for Search
- Gemini and ChatGPT Have a Competitive Advantage: Familiarity
- The Changing Landscape of Information Seeking
- Next in This Series
- About the Study

## Search Habits Are Hard to Break

Information-seeking habits are sticky. Once someone finds a reliable way to easily find information they need, that method becomes nearly instinctive.

This partly explains Google's massive share of the search-engine market. Over the years, many participants have told us they've never even considered Bing or other alternatives, simply because they're familiar with Google and know it works well (enough) for them. In our study, multiple participants commented on their tendency to lean on what they're already comfortable with.

> "I always start with Google. It's familiar. It's what's there. It's what I've been doing for a long time."
> "It's always Google for me. It's where I always start; my search engine of choice."

People tend to reach for whatever information-seeking tool is most convenient. One participant reflected that he got into the habit of using Google because it was built into his Chrome browser.

> "At some point, I used the Google Chrome browser. It has a default setting that redirects me to Google. So, by default, I just start from google.com. I think that's the main reason I've stuck with this for a very long time."

Beyond the choice of where to search for information, these habits also influence how users search. For example, several participants in our study (and many participants in other studies over the years) told us that they always skip over sponsored results on a search-results page. Some participants thought that they're "supposed" to search this way (an example of a technology myth). Others were unable to explain why they skip the ads.

> "I don't know why I always skip the first results. Anything that says sponsored‚Ä¶ I just tend to go to one of the top results that isn't sponsored."

These are information-seeking strategies that people developed over many years. It makes sense that users would fall into these habits over time, because (in most cases) they aren't thinking much about the systems they'll use ‚Äî they're focused on finding answers to their questions.

Users rely on them because they've generally worked well in the past. To change these habits, people need a significant incentive.

## AI Changes Information-Seeking Habits

Generative AI's value in information seeking is powerful enough to change those ingrained habits. Several participants were aware that their own information-seeking behaviors had started to shift since they began experimenting with AI tools.

> "Oh, I always start from Google. I never use different search engines, but these days I also incorporate ChatGPT."

Generative AI offers substantial shortcuts around the often tedious and time-consuming work required to research a topic, including:

- Defining and articulating the information need
- Overcoming information gaps and keyword-foraging problems
- Weighing and selecting credible sources
- Sifting through enormous amounts of information
- Scanning through long pages of text
- Comparing contradicting perspectives from different sources
- Synthesizing and storing information (mentally or in a note)

Even when participants utilized only a handful of genAI's possible information-seeking benefits, they valued the assistance immensely.

### AI Overviews: The First Point of Contact for Novice AI Users

Every single participant in our study had at least heard of AI and had encountered and used AI overviews on Google's search-results pages. But a few of our participants had very limited experience with generative AI beyond those instances.

> AI overviews appear at the top of results pages for many queries on web search engines. They're powered by LLMs and attempt to quickly define keywords or answer questions.

![A Google search results page for "dragonfruit" displaying an AI Overview. The overview describes dragon fruit as a tropical fruit with vibrant pink or yellow skin and white or red flesh dotted with black seeds. It includes details on skin, flesh, varieties, taste, and texture. The right-hand panel displays search results including a Wikipedia snippet, a Healthline article, and a YouTube video thumbnail showing a sliced dragon fruit.](https://media.nngroup.com/media/editor/2025/08/08/ai-overview-example-dragonfruit.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

AI overviews are the modern, upgraded iteration of the featured snippets and answer boxes introduced by Google and its competitors in the 2010s. Even then, many content sites started to notice a dent in their web traffic as people started to find answers without a click. AI overviews are even more likely to satisfy information needs without clicks.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Facilitating AI-Enhanced Workshops: From Ideation to Action](https://www.nngroup.com/articles/facilitating-ai-workshops/)  
> for internal educational use only (non-profit).

# Facilitating AI-Enhanced Workshops: From Ideation to Action

With AI's potential to increase the quality of collaborative outputs, teams should incorporate it into their workshops to maximize success. That starts with thoughtful preparation ‚Äî but once the workshop begins, facilitators need to guide participants in using AI effectively, creatively, and collaboratively. This article discusses how to do just that.

## In This Article:

- 1. Narrow AI-Generated Ideas Before Sharing
- 2. Prepare Evaluation Criteria for AI Outputs
- 3. Plan More Time for Reading and Prompt Iteration than You'd Think
- 4. Encourage People to Cowrite Prompts
- 5. Document Things Digitally

## 1. Narrow AI-Generated Ideas Before Sharing

To understand how to run effective AI workshops, we need to consider how human and AI ideation differ.

For one, most humans tend to fixate on their first few ideas and must be pushed to think outside the box. They also tend to settle on one idea ‚Äî whether it's good or not ‚Äî often because someone charismatic or authoritative has suggested it. AI does not struggle with these problems.

This difference is a bit like that between sheep and rabbits:

- **People (the sheep)** tend to follow a leader and hesitate to stray far from their starting point. Even smart people working together often move as one.
- **AI (the rabbits)** isn't as predictable. People who use AI can generate more divergent ideas than those who don't, because AI isn't swayed by groupthink, moves on quickly, and doesn't get tired

![On the left is a drawing of sheep and the caption "Ideation with people." The right is a drawing of rabbits with the caption "Ideation with AI."](https://media.nngroup.com/media/editor/2025/06/06/facilitating-ai-article-visual.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

This contrast is a great benefit and also a big challenge when involving AI in workshops and ideation. In just 5 minutes, a few people armed with an AI tool can come up with more information than they have time to fully consider or share. In a Procter & Gamble study (discussed in a previous article), both individuals and teams using AI submitted much longer solutions (measured by word count) than those working without AI.

To solve this problem, workshop facilitators must first understand how AI enhancement alters the workshop structure. In human-only sessions, most ideation-based activities have a diverge‚Äìconverge structure:

1. **Diverge** and spend time coming up with as many ideas as possible.
2. **Converge** and work with the group to cluster, discuss, and prioritize everyone's outputs.

When AI is involved, this second step is much harder because there can be hundreds of ideas to consider. You could have people limit the number of ideas the AI is generating, but this undermines its superpower and violates the proper ideation mindset. Personally, I rarely use AI to help ideate without asking it to provide at least 15‚Äì20 different options per prompt.

Instead, use the following structure to benefit from diverging and converging while using AI:

1. **Diverge** and spend time using the AI to come up with as many ideas as possible.
2. **Have the AI narrow down** the ideas to top suggestions, based on provided criteria.
3. **Converge** and work with the group to cluster, discuss, and prioritize everyone's outputs.

## 2. Prepare Evaluation Criteria for AI Outputs

Now, the facilitator must decide what the criteria for narrowing should be.

This question becomes a discussion about prioritization. There are many potential methods for this, but, in my experience, given that LLMs are text-based, a scorecard works best. That way, the AI can provide a rating for each idea based on different criteria.

You can either provide these criteria ahead of time or collaboratively create them as part of the workshop. This is your choice. Here's an example of a sample prompt I might give participants if I had decided the criteria before:

> We can't implement all these ideas. We need to narrow in on around 5 of the best ones. Use the following criteria to select which of these ideas we should move forward with:
> 
> - **Feasibility**: The degree to which the item can be technically built. Does the skillset and expertise exist to create this solution?
> - **Desirability**: How much users want the item. What unique value proposition does it provide? Is the solution fundamentally needed, or are users otherwise able to accomplish their goals?
> - **Viability**: If the item is functionally attainable for the business. Does pursuing the item benefit the business? What are the costs to the business and is the solution sustainable over time?
> 
> Follow these steps:
> 
> 1. Rate each of the ideas you've provided on a scale of 1 ‚Äì 10 for each of the criteria provided. For example, 1 = low feasibility, meaning it would be impossible to build, and 10 = high feasibility, meaning it would be very easy to build. Apply this same framing to each criterion.
> 2. Provide a rationale for each of your ratings.
> 3. Add the three scores from each idea together to create a total score out of a potential of 30.
> 4. Provide all ideas and their respective ratings in a table, ranking them from the highest total scores at the top to the lowest total scores at the bottom.

You can swap out feasibility, desirability, and viability with other criteria relevant to your context. The AI's ratings will also become more meaningful if you provide more context (such as uploaded documents or even a custom AI ‚Äî as discussed in another article in the series).

## 3. Plan More Time for Reading and Prompt Iteration than You'd Think

Because the AI generates ideas so quickly, it's easy to assume that AI-assisted people won't need as much time to work independently as they would when doing all the mental work themselves. This isn't true. People working with AI generally need just as much time, if not more, as people working on their own. Now, they need to write thoughtful prompts, read through the responses, and repeat the cycle a few times.

To individually generate ideas for one problem, I typically give people without AI around 5 minutes; with AI, I give them around 8‚Äì10 minutes. However, this duration varies based on the group's familiarity with AI, the complexity of the problem, and how quickly people seem to be wrapping up.

## 4. Encourage People to Cowrite Prompts

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [4 Tips for Preparing AI-Enhanced Workshops](https://www.nngroup.com/articles/preparing-ai-workshops/)  
> for internal educational use only (non-profit).

# 4 Tips for Preparing AI-Enhanced Workshops

As the research by Fabrizio Dell'Acqua and colleagues proved, generative AI (genAI) can boost team ideation. However, incorporating AI in team ideation can be challenging, especially if different team members have different experience levels with genAI. This article discusses how you can prepare for a successful and productive AI-enhanced workshop. A subsequent article will talk about how to run such a workshop.

## In This Article:

- 1. Prepare an AI WarmUp Activity to Build Familiarity
- 2. Upload Files for Quick and Easy Context
- 3. Create a Custom AI for More Powerful Context and Constraints
- 4. Provide Adaptable Sample Prompts

## 1. Prepare an AI WarmUp Activity to Build Familiarity

When prepping any workshop ‚Äî AI-enhanced or not ‚Äî I always use a warmup activity or icebreaker to get people's minds and mouths moving. Well-crafted warmups help groups become familiar with the session's tools. Whether this is a shared document, a digital whiteboard, or Sharpies and sticky notes, getting the tools into people's hands is meant to remove any hesitations or access obstacles that participants may have. When I teach courses on AI, I always invite people to open their LLMs early in the session. Doing so ensures that everyone has them ready to go. I'd encourage workshop participants to use the same LLM. That way, people can troubleshoot problems and spend less time getting distracted by differences in various tools.

Here are some fun AI-based warmup examples you can use regardless of the group size or AI tool:

- Tell the AI 3 things you know about someone else in the group, and have it generate a picture of them.
- Give the AI the names of everyone in the group and ask it to combine them into a team name.
- Have the team work together to send a short sequence of emojis to the AI and have it generate a story based on them.

Consider asking the AI for more ideas with a prompt like, "Suggest some fun icebreakers I could use for a team who will be using AI in a workshop that have people use the AI in some way." Add more details to the prompt to adapt it to your participants and their interests or to the focus of the workshop.

## 2. Upload Files for Quick and Easy Context

Part of good prompting is providing adequate context. Sample prompts help with this, but it's often much more effective to prepare context-providing documents before the workshop. Participants can then simply upload these documents into their LLMs to provide context for their prompts. Uploads could include things like:

- A description of the company and the goods or services it provides
- Personas or other research artifacts describing data-derived information about users and their needs
- A timeline for an upcoming project
- A list of constraints or requirements pertinent to the task at hand
- Clean and prepared qualitative or quantitative user data (such as survey responses or interview transcripts) ‚Äî however, please avoid using the AI in the workshop to formally analyze this data, as it should serve only for context.

Providing contextual uploads will also allow people to start fresh conversations with the AI without losing the relevant context ‚Äî they could simply reupload the same information.

Keep file formats straightforward and use text-based documents. LLMs are getting better at reading PDFs, spreadsheets, images, and other file types, but I always recommend testing any files you'll use by uploading them yourself ahead of time and asking the LLM to prove that it can read them.

## 3. Create a Custom AI for More Powerful Context and Constraints

Let's say you're running lots of workshops around a certain topic. Or there are many contextual documents that are cumbersome to upload. Or there are very specific constraints or formats you want the AI to apply to its responses. In such situations, I recommend creating a custom AI that already knows everything workshop participants will need. Three prominent examples of custom AIs include GPTs within ChatGPT, Gems within Gemini, and Projects within Claude.

Examples of UX use cases for a custom AI might include:

- **Personas**: Create a separate custom AI to impersonate each persona. Provide it with as much real data as you have about that persona's needs. While the AI's responses will not always align with what real users will say, they'll be just as reliable as asking team members to predict user behavior.
- **Constraints**: Teach the custom AI your team structure and typical work cadence. Also, give it any other major constraints you're working within so that it automatically considers these when suggesting ideas.
- **Stylistic guidelines**: Give the custom AI documentation on style, tone, word choice, and samples of good content that aligns with those guidelines. The LLM will provide outputs that automatically align with that direction, without further instruction from workshop participants.

For example, I recently ran a workshop with the course instructors at NN/g to ideate about valuable future UX trainings. I created a custom "Course Creation" GPT that I had trained to understand learning objectives, good assessment practices, the scope and style of our courses, and many other NN/g-specific details. I had all instructors in the workshop exclusively use this custom GPT. This approach saved significant time and energy by aligning the various outputs from different people.

## 4. Provide Adaptable Sample Prompts

You never want to completely control the prompts people use when working with AI. Writing and iterating prompts together leverage the group's expertise and creativity. However, some people might struggle getting started for a couple of different reasons:

- They aren't comfortable using AI, or aren't very good at writing prompts (yet).
- They don't understand the problem space and aren't sure what direction to take things (initially).
- They're feeling lazy and don't want to do a bunch of writing.
- They aren't comfortable quickly writing out their thoughts for fluency or language reasons.

Your sample prompts are meant to be a starter kit, intended for adaptation and abandonment as people get going. However, they can serve as instructive demonstrations of useful prompting and steer people in a helpful direction for the workshop. Here are some basic examples following our CAREful framework:

### Example 1: Writing Web Copy

> **Context**: I'm a UX writer working on an enterprise productivity app. We're updating the homepage hero section to highlight our new collaborative whiteboard feature better.
> 
> **Ask**: You're a UX copywriter. Write three headline options for the hero banner. Draft ten 12-word subheaders emphasizing seamless teamwork. Suggest ten 4-word calls-to-actions that drive signups.
> 
> **Rules**: Tone: clear, approachable, and action-oriented. Headlines ‚â§ 7 words; subheaders ‚â§ 12 words; CTA ‚â§ 4 words. Include "collaborate" or "together" at least once. Avoid jargon and superlatives.
> 
> **Examples**: Good headline: "Collaborate in Real Time". Bad headline: "Revolutionize Your Team's Synergy"

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [AI Chatbots Discourage Error Checking](https://www.nngroup.com/articles/ai-chatbots-discourage-error-checking/)  
> for internal educational use only (non-profit).

# AI Chatbots Discourage Error Checking

Large language models (LLMs) are being widely introduced into professional workflows through both new standalone software and integrations with existing tools. One of the most important ways that these tools increase users' productivity is through text generation. But generated content is prone to hallucinations, with the AI extrapolating outside of its training data to return outputs that are truth-like, but incorrect.

Designers of generative AI (genAI) products must help users identify and correct these errors. Most genAI tools currently fail in this responsibility.

## In This Article:

- Users Struggle to Verify AI Outputs Successfully
- Reducing Errors Increases Interaction Cost
- LLM Outputs Signal Authoritativeness
- Users Are Not Building Expertise to Spot Errors
- A Finished Product Is Harder to Evaluate
- Designing Checkable AI Tools

## Users Struggle to Verify AI Outputs Successfully

When humans produce text, they alternate between writing and editing. GenAI tools make people more productive by accelerating the pace at which they can create content ‚Äî the AI will near-instantaneously generate some the text based on the prompt, and the user needs to spend time only on editing.

Since LLMs are trained on a corpus of grammatically and syntactically correct content, their outputs may not need much copyediting. But for professional writing, editing also includes fact-checking and ensuring that any argument follows from the facts.

This work is not an afterthought ‚Äî it is a difficult and time-consuming process. Users of genAI tools have been struggling to error-check their outputs successfully. Lawyers have been caught citing cases that don't exist. Scientists refer to hallucinated papers and journals. Doctors end up trusting erroneous AI diagnoses over their own expertise.

It is irresponsible to blame the users for misinformation generated by LLMs. People are efficient (not lazy). Users adopt genAI tools precisely because they come with the promise of greater efficiency. They will not go out of their way to check the work if the effort of doing so does not seem proportional to the AI's likelihood of being wrong.

## Reducing Errors Increases Interaction Cost

It's true that an exceptionally careful user can reduce the number of mistakes the LLM will produce. But doing so requires additional knowledge and user effort.

To verify the answers provided by a chatbot, the user will need to follow the same steps as they would when reviewing any other text, and then some.

- **Review the entire output**: Writers build up a mental model of their work in their heads as they go and maintain a sense of its structure. Editing an LLM output requires the user to understand the structure of the answer and build up that mental model from scratch ‚Äî even between prompts on the same topic, as LLM outputs are nondeterministic.
- **Identify items that need verification**: The text will present and connect different key ideas. It is those ideas ‚Äî facts and claims made on their basis ‚Äî that the editor needs to pay attention to.
- **Validate the claims being made**: In situations where the LLM has provided a citation, the editor needs to track down the text (assuming that it exists) and find the content being cited within it. The editor also needs to determine whether the source is authoritative. If the LLM has not cited a source, the editor must perform research from scratch to confirm the claim as factual (or ask the LLM for supporting evidence).
- **Verify that the arguments follow from the claims**: The editor must separate facts and the conclusions being drawn from those facts. This often requires a significant degree of subject-matter expertise.
- **Articulate corrections**: If the editor is not willing to rewrite the output themselves, they must update the prompt to tell the LLM what needs changing.
- **Verify the response again**: Due to the limited context window and nondeterministic output of LLMs, there is no guarantee that any given error was fixed or new errors were not introduced.

Some of these steps may be shortened with careful prompt engineering. For example, asking the LLM to generate one small and digestible part of a text at a time allows users greater control over what is written and how it is incorporated into the larger whole of the work. But these strategies are workarounds that require user effort and an in-depth understanding of how AIs hallucinate. And working through a document one part at a time is possible only for expert users who already know what those parts ought to be.

## LLM Outputs Signal Authoritativeness

Humans do not check every single novel piece of information for accuracy; the effort to do so would be unjustifiable. The extent to which we take a claim at its value is based on how closely it fits with what we already know and the trustworthiness of the source.

Outputs of genAI tools mimic certain attributes that we associate with authoritative sources. Their tone is unerringly confident, regardless of the accuracy of the response. They are grammatically correct and meticulously formatted. Through a phenomenon called the halo effect, users' positive perception of one attribute of the LLM's responses causes them to have a positive predisposition about all its other attributes ‚Äî including accuracy.

While we ought to consider AI-generated content an early draft, due to these signals we often treat it as finished work. Users frequently have such confidence in an LLM output that they do not even read through it end to end. For example, in our recent round of intern hiring, we were disappointed to see cover letters ending in "Let me know if you need anything else!"

Additionally, nothing about the chat interface (a plain text box) acknowledges that the LLM's outputs might not be entirely accurate outside of a small label: XYZ bot can make mistakes, please check responses. Instead, chatbots typically encourage users to move on from the response by prompting them with a new question.

![Screenshot of the IEA.org chatbot that says "Remember, AI can make mistakes."](https://media.nngroup.com/media/editor/2025/05/06/ieaorg-chatbot-error.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Users Are Not Building Expertise to Spot Errors

Improving writing productivity is not the only application of genAI. These tools are often used as virtual experts, providing advice or performing tasks outside of the user's field of expertise. In practices such as vibe coding, where an AI generates code based on plain-language prompts, the user does not need to know anything about software development to create a usable program.

A side effect of this mode of usage is that users lack a sufficient understanding of the subject matter to meaningfully validate the LLM's outputs. A vibe coder has no way of evaluating the quality of the code for security vulnerabilities, and a guidebook author might find out they gave bad advice only when customers start getting poisoned.

But far from all AI-generated answers are hallucinations. As genAI performance progresses to a point where the majority of LLM responses are correct, even users who started out by verifying every statement may start to feel that their effort is not paying off. As users increasingly take AI outputs at face value, they might lose the expertise necessary to evaluate the accuracy of those outputs. As a Microsoft paper by Hank Lee identifies, operators who come to rely on genAI for routine decisions end up being unprepared for situations that require their intervention.

## A Finished Product Is Harder to Evaluate

While users can choose to collaborate with an LLM section by section to create a document or a working program, they can also just ask it to produce the entire text by itself. While this is a much faster way of getting an output, it creates additional problems when it comes to verifying the output's quality and troubleshooting its constituent parts.

Safety-systems researcher David D. Woods observed that the task of analyzing a whole is made far more difficult without having worked on the parts. The human counterpart has no context for any of the AI's decisions because they have no access to the system's "thought process." To effectively correct any technical issues in the solution, the user has to effectively solve the entire problem on their own in order to reconstruct the mental model necessary for identifying flaws.

## Designing Checkable AI Tools

The particulars of LLM technology may mean that, at least for the time being, hallucinations are here to stay. Responsibly engineered models should structure their outputs in a way that helps users identify these errors, by exposing the reasoning that leads to a conclusion or clearly communicating the degree of confidence for generated assertions. But the designers of genAI tools also have an opportunity to make error checking easier and more salient during interactions between users and LLMs.

### Followup Questions that Encourage Critical Thinking

Today's genAI chatbots often prompt followup questions; this feature could be used to embed fact-checking best practices into the tool. Rather than urging users to move on, these prompts could empower them to investigate the response with the same amount of skepticism that an editor should have for any submission. Prompts that ask for more details about sources or for the degree of certainty about the LLM's conclusion would encourage critical thinking and prevent users from taking the LLM's answer at face value.

![A Copilot screenshot edited to replace the prompts for follow-up questions at the end with How certain are you of this answer? Do other sources provide alterantive explanations? What are some reasons this answer may be incorrect?](https://media.nngroup.com/media/editor/2025/05/06/copilot-alt-prompts.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

### Highlight Referenced Text in the Source

Some existing chatbots can already provide citations for assertions in their responses. These citations are usually links to relevant webpages. However, users still need to do the work of verifying the accuracy of the citation: they have to click on the link, find the passage being referenced, and ensure that the context supports the meaning of the excerpt. Providing a deep link to the referenced passage or showing it in a preview of the source would greatly reduce the interaction cost of checking the LLM's assertions.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Product-Specific GenAI Needs to Write for the Web](https://www.nngroup.com/articles/genai-write-for-the-web/)  
> for internal educational use only (non-profit).

# Product-Specific GenAI Needs to Write for the Web

Users don't read ‚Äî they scan. To accommodate this behavior, content writers use techniques like chunking, bullet points, and strategic formatting to improve scannability.

Unfortunately, many product teams have forgotten these principles in their rush to include generative AI (genAI) tools. Our recent study showed that AI-generated content often violates fundamental principles of writing for the web.

## In This Article:

- Emulate Broad-Scope AI Tools
- Concision
- Formatting for Scannability
- Inverted Pyramid
- Approachable Language
- Learning About the User

## Emulate Broad-Scope AI Tools

We recently ran a study looking at how people use various product-embedded genAI features like Carvana's Sebastian or Instagram's AI search and how these tools compare with broad-scope, general-purpose AI tools like ChatGPT and Perplexity.

Overall, we found that, although not perfect, broad-scope AI chat tools produce outputs that are more scannable and easier to read than narrow-scope, product-embedded AI tools. While AI teams might not have control over the outputs generated by an LLM, it is worth exploring options for finetuning outputs and following best practices for web writing.

This issue may be eventually addressed as these product-specific AIs integrate broad AI models that already adhere to the principles of writing for the web. However, users may not have the patience to wait until then.

![Screenshot of a ChatGPT output showing bulleted lists and an introduction and conclusion.](https://media.nngroup.com/media/editor/2025/03/27/chatgptscan.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Concision

Here at NN/g, we follow the slogan "Brevity = Brilliance." Shorter text increases user comprehension and engagement. Users read only about 20% of written content on any given web page, so it's doubly important that those words count.

Generative AI outputs often end up being unnecessarily long. Take a look at this output to the question What are pet-friendly rug materials? from Rufus, Amazon's generative AI. Compare it to the much shorter human-revised version.

**üö´ AI-Generated: Too Wordy**

When choosing a pet-friendly rug material, consider the following options:

Wool - Wool is a popular choice for pet owners as it is durable, naturally stain-resistant, and easy to clean. Its dense fibers can withstand heavy traffic and accidental spills.

Synthetic Fibers - Synthetic rugs made of nylon, polyester, or polypropylene are also pet-friendly. They are resistant to stains, moisture, and wear, making them ideal for high-traffic areas.

Sisal And Jute - These natural fibers are durable and pet-friendly, but they are not as stain-resistant as wool or synthetic materials. They are best suited for low-traffic areas or areas where pets are less likely to have accidents.

Leather - Leather rugs are durable and easy to clean, making them a good choice for pet owners. They are resistant to stains and odors, and their smooth surface makes it difficult for pets to scratch or damage them.

**‚úÖ Human-Revised: Concise**

Consider these pet-friendly rug materials:

- Wool: Naturally stain-resistant and easy to clean
- Synthetic Fibers: Resistant to stains and wear
- Leather: Stain-resistant and hard to damage
- Sisal and Jute: Durable but less stain-resistant

When interacting with genAI chatbots, our participants struggled to find the information they needed to complete their task or just got frustrated with the amount of text they needed to sort through. They told us:

> "I think it's a lot of information. I'm someone that likes concise information. The question was [‚Ä¶] a yes-or-no question, so [‚Ä¶] that's why I was like 'That's a lot of info for just yes or no.'"

> "The rest of the information [‚Ä¶] is just a sales pitch for the other products that accompany it; [...] everything else is [‚Ä¶] fluff‚Ä¶"

> "It gives a lot of information. I could have done with 'What is Smart Things? Oh! It's where you can connect all your smart home devices and run them off of one thing.' That's all I need. Which it did say [‚Ä¶], but it also said a lot of things."

> "Wow, very chatty. My question is very short, very concise, and it's giving me a very long response with some options. Which is kinda nice, I suppose. But I don't need all this information."

Participants also repeatedly mentioned needing a shortened version or a summary to help them digest all the information provided.

> "Or maybe just again like a short synopsis and then 'Hey, do you want to know more?' And then, if you click that button, [‚Ä¶] it gives you all the things that I don't care about."

> "I'm sure there are [people] out there who would love all this information, but I would definitely like a ‚Äî What is that called? A TLDR or whatever"

While summaries could increase users' ability to consume the content, they might not be necessary if the information was formatted for scannability.

## Formatting for Scannability

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Discoverability of AI Features: Learn from Amazon's Mistakes](https://www.nngroup.com/articles/discoverability-ai-amazon/)  
> for internal educational use only (non-profit).

# Discoverability of AI Features: Learn from Amazon's Mistakes

The first step to delivering a successful AI feature in your product is to make sure it's valuable to your users and business. (Most AI features fail at step 1.) But even valuable AI features will still fail if users don't see them.

## In This Article:

- People Don't Notice That AI Feature‚Ä¶
- Amazon.com Case Study
- "Rufus" Chat: Valuable but Invisible
- AI in Search: A Mental-Model Mixup
- Product Q&A Chat
- Avoid the AI Discoverability Problem

## People Don't Notice That AI Feature‚Ä¶

‚Ä¶Even if it's right in front of them.

AI features (like all UI elements) are liable to be overlooked by users if they aren't expected. And right now, as many sites and apps are adding their first AI features, their customers (even if fairly AI-savvy) don't expect them.

### Many AI Features Lack Discoverability

A quick reminder:

> When a feature has good findability, users can easily find it if they look for it.

> When a feature has good discoverability, users notice, recognize, and understand it even when they were not previously aware of its existence.

In our qualitative usability testing, we found that the majority of AI features we tested lacked both discoverability and findability. In other words, our participants mostly:

- Didn't expect or think to look for AI features in apps or sites, even when they reported having used similar systems in the past
- Didn't notice, comment on, or use AI features when they happened to encounter them (poor discoverability)
- Couldn't easily find those features when we directed them to look for them as part of a stepped-task study methodology (poor findability)

This was unfortunate because, in several cases, participants found the features useful once researchers led them to these features. For example, one participant told us:

> "Honestly I just didn't even notice [the AI feature]. I'm wondering why it was here. It was definitely really helpful, but I totally missed it."

## Amazon.com Case Study

To explore the discoverability problem, let's look at some of the AI features on Amazon's website and mobile app. The site offered several AI features integrated at various places in its interface. They all were valuable to users, but unfortunately, haphazard design impeded their discoverability and usefulness.

In this article, we'll analyze 3 of Amazon's AI features:

- A global site chat named Rufus, accessed through the global navigation
- AI-prompt suggestions in the search bar
- A product-specific AI-powered Q&A

They were all intended to help people navigate the vast number of product options available on Amazon and make shopping decisions easier, thus ensuring that users will keep coming back and buy more.

At the time of writing, Amazon's AI features appear to be available only to signed-in customers in some regions. (Required authentication is a smart business decision because it likely limits misuse or abuse of those features.)

## "Rufus" Chat: Valuable but Invisible

Amazon's site-wide AI chat offered a lot of user value, especially since it had the useful context of what the shopper was currently (and had previously been) searching and browsing for. It acted as smart shopping assistant, following the user around from page to page and answering questions.

![Amazon desktop site: On larger devices, the site-wide AI chat could be accessed by clicking the Rufus button in the global navigation bar. Once initiated, the AI chat could provide information about product categories (What's the difference between silk and satin pillowcases?) or suggest specific product options.](https://media.nngroup.com/media/editor/2025/03/17/amazon-rufus-chat-annotated-narrow.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Unfortunately, none of our participants noticed it until a researcher pointed it out. The global chat suffered from severe discoverability issues on both desktop and mobile.

On larger devices, the AI chat button was:

- Located in a nonstandard location, in the upper left corner under the logo. People expect to find a chat button in the lower right corner of the screen, where it is usually placed on most sites. (This is an example of how external consistency across experiences sets expectations and builds mental models.)
- Given a label that was unfamiliar to users: Even the frequent Amazon shoppers in our study admitted they'd never noticed or thought to click this button. Why would they? Unless they'd previously encountered it, the name Rufus doesn't mean anything to people.
- Represented by a new version of the sparkles icon: As they encounter the sparkles icon in different products, people are starting to (vaguely) understand that it represents something related to AI. The meaning of the sparkles icon, is, however, nowhere from established, and Amazon's design makes it worse by turning two of the sparkles into chat bubbles.

Additionally, the AI chat's lack of discoverability was aggravated by Amazon's busy design, which displays vast amounts of competing information, options, and promotions. Combined with the fact that users didn't expect an AI chat on Amazon in the first place, these issues made the Rufus button invisible to our participants.

![Amazon mobile app on iOS: On smartphones, the hybrid chat/sparkles icon was more visible, since it was relatively larger in the small screen space. However, it was still ignored by users because the unlabeled and ambiguous icon did not communicate any meaning.](https://media.nngroup.com/media/editor/2025/03/17/amazon-rufus-chat-mobile-annotated.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Once people found and used the chat, it did have several more UX problems (for example, a tendency to get confused about what the user was shopping for or to give unnecessarily wordy and jargon-filled responses.) However, most participants liked Rufus and said they could see how it'd be helpful. A few also admitted that they would've never thought to use it without the facilitator pointing it out.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [UX Leads Adoption of AI Chat](https://www.nngroup.com/articles/ux-ai-adoption/)  
> for internal educational use only (non-profit).

# UX Leads Adoption of AI Chat

An analysis of one million conversations with Claude.ai indicates that, while UX professionals represent less than 0.01% of the US workforce, they generate a remarkable number of AI conversations (7.5% of all Claude conversations). Thus, UX is among the top professions actively engaging with AI, predominantly in areas such as technical writing, web design, and copywriting. However, tasks requiring complex analysis or direct human interaction remain largely untouched by AI.

What does this concentration of AI usage imply for the future of UX work, opportunities, and compensation? This article discusses the UX tasks currently attempted with AI and those where human expertise still prevails.

## In This Article:

- Key Findings
- Anthropic's Data Set and Findings
- AI-Chat Usage per Wage Group
- How AI Is Used in UX
- How Will AI Change the UX Workforce?
- Limitations of This Research

## Key Findings

Two and a half years after the release of ChatGPT, we find ourselves amid speculation regarding AI's impact on the job market. Anthropic's recent analysis of one million Claude.ai conversations assesses the current state of AI use by different professions and provides an important baseline that we can refer to in the future.

This article reports on Anthropic's findings and expands them with my own UX-specific analysis of Anthropic's dataset. The main takeaways include:

- Most American workers are not using AI for their jobs. AI usage for work is highly concentrated (77%) in digitally focused job roles like UX. that make up only about 30% of the workforce. So far, these occupations are in the top 25% of wages. (Anthropic's finding)
- UX professionals use AI for their work more than most others. 7.5% of 1 million AI conversations were related to UX, placing UX among the top 5 occupations in terms of AI-conversation volume. Yet UX professionals make up less than 0.01% of the US workforce. (NN/g's finding)
- People have attempted to use Claude for a little more than half (55%) of UX-related work tasks at least once. This puts UX firmly in the 94% percentile of all occupations ‚Äî meaning that 94% of all occupations attempted to use AI for fewer tasks than UX. (NN/g's finding)
- UX mostly uses AI for writing, design, and development tasks. Not surprisingly, most UX tasks performed with AI are computer-related activities falling under the umbrella of technical writing, web design and development, and copywriting. (NN/g's finding)
- Most UX tasks not yet attempted with AI either involve direct interaction with other humans or are too complex. 42% of the UX tasks that were not attempted with AI involved interfacing with other humans (e.g., users, coworkers); another 20% involved complex analyses (e.g., design reviews). (NN/g's finding)

## Anthropic's Data Set and Findings

Anthropic recently launched a data initiative aimed at understanding the ways in which AI changes how people work. Its first report analyzed one million anonymized conversations with Claude.ai. Researchers used artificial intelligence to cluster these conversations into categories matching various tasks and occupations, as described in a public dataset provided by the US Department of Labor.

### Anthropic's Analysis

Anthropic used the following 2 data sources:

- One million Claude.ai conversations carried out during Dec 2024-Jan 2025
- Descriptions of occupations and corresponding tasks from the O*NET database of occupations provided by the U.S. Department of Labor; this database contains approximately 20,000 task descriptions

Then it used a Claude-based analysis tool to assign each conversation to a single task. Once each conversation was thus classified, the Anthropic team calculated the percentage of Claude conversations that were assigned to each task. This dataset is publicly available.

Anthropic reported:

- The tasks with the largest percentage of AI use
- The occupations that showed the highest penetration of AI

### Digitally Focused Occupations Show the Highest AI Usage

Perhaps unsurprisingly, Anthropic found that the Computer & Mathematical occupational category had the most conversations, containing 37% of the AI conversations in their dataset. The other categories that included more than 5% of the conversations were Arts & Media, Education & Library, Office & Administrative, Life, Physical, & Social Science, and Business & Financial. All these categories have a well-established, strong digital component.

![According to Anthropic's classification, 77% of all AI conversations were related to only 6 broad occupational categories (out of 22 total possible categories). The top category was Computer & Mathematical occupations.](https://media.nngroup.com/media/editor/2025/03/13/uxtasks-ai-barcharts-01.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

More than 3 quarters of all conversations (77%) fell in one of these top 6 categories. Categories with low representation (e.g., Personal Care and Service, Community & Social Service, Sales & Related) were those that traditionally tend to require physical labor or face-to-face interactions.

One may wonder whether these top categories are simply the most common in the US workforce. In fact, even though three quarters of these conversations were in these 6 categories, these occupations make up only 30% of the US workforce.

Thus, so far, the majority of the US workforce makes only negligible use of AI. This finding aligns with a similar one from a recent Pew Research study, which suggested that the majority of American workers are not using AI in their jobs.

(Additionally, the Claude data likely includes conversations initiated by users outside of US, so the numbers reported per occupation will be even smaller if one looked only at US workers.)

### AI-Chat Usage per Occupation

The Anthropic team also examined the extent of AI adoption for each occupation. It did this by looking at the percentage of tasks (out of the tasks for that occupation) that were attempted with AI. It found that only 4% of all occupations have at least 75% of their tasks attempted with AI.

For example, here are the four tasks associated with the occupation Shampooers (picked because it happens to have the least number of tasks):

- Massage, shampoo, and condition patron's hair and scalp to clean them and remove excess oil.
- Advise patrons with chronic or potentially contagious scalp conditions to seek medical treatment.
- Treat scalp conditions and hair loss, using specialized lotions, shampoos, or equipment such as infrared lamps or vibrating equipment.
- Maintain treatment records.

Only the third out of these 4 tasks (treat scalp conditions and hair loss) was attempted with AI, corresponding to 0.004% of all Claude conversations. Thus, the Shampooers occupation has a depth of AI adoption of 1 out of 4, or 25% ‚Äî in other words, only 1 task out of 4 was attempted with AI.

When I analyzed Anthropic's dataset, I found that, on average, occupations had about 27% of their tasks attempted with AI, with half of the occupations having less than 22% of the tasks attempted with AI. That translates to 1 task out of 5 ‚Äî meaning that, for most occupations, users think that Claude could help them only in a very limited way.

At the very high end, a high proportion of teaching professions had most tasks attempted with AI, emphasizing AI's capacity to act as a tool for presenting information in a digestible format.

## AI-Chat Usage per Wage Group

Anthropic found that the use of Claude peaks in occupations with high wages (upper 75% of all wages) ‚Äì particularly Computer Programmers and Software Developers.

However, usage was low for both the highest-paying jobs (e.g., Obstetricians & gynecologists) and the lowest-wage group (e.g., Waiters) ‚Äî supporting the idea that occupations involving human interactions have less AI usage.

## How AI Is Used in UX

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [Beyond the Algorithm: Why GenAI Demands a UX Revolution](https://www.uxmatters.com/mt/archives/2025/03/beyond-the-algorithm-why-genai-demands-a-ux-revolution.php)  
> for internal educational use only (non-profit).

# Beyond the Algorithm: Why GenAI Demands a UX Revolution

> With traditional, direct-manipulation graphic user interfaces (GUIs), users take action, then the system responds in consistent, expected ways. ‚Ä¶ GenAI disrupts this UX design paradigm by introducing an interaction model that is based on the specification of intent-based outcomes.

For decades, principles relating to usability, predictability, and efficiency have guided UX design, ensuring that users can complete their tasks with minimal cognitive effort. With traditional, direct-manipulation graphic user interfaces (GUIs), users take action, then the system responds in consistent, expected ways.

However, generative artificial intelligence (GenAI) disrupts this UX design paradigm by introducing an interaction model that is based on the specification of intent-based outcomes. Instead of following predefined workflows, users describe what they want, and the AI generates variable results‚Äîoften with unpredictable or emergent outcomes.

This UX design paradigm shift that we're currently experiencing challenges long-standing UX design heuristics such as consistency, predictability, and seamlessness. Designing for GenAI requires a new approach‚Äîone that embraces transparency, adaptability, and user control, empowering users to navigate uncertainty, iteration, and co-creation rather than expecting deterministic outputs.

## Why Traditional UX Design Falls Short in the GenAI Era

Two foundational principles have long guided traditional UX design:

- **predictability**‚ÄîUsers expect the same result in response to the same input, ensuring consistency and reliability.
- **seamlessness**‚ÄîUser interfaces should minimize cognitive load, making interactions feel effortless.

However, GenAI disrupts these assumptions, requiring a fundamental shift in how we design user experiences, as follows:

- **generative variability**‚ÄîUnlike deterministic software, GenAI can produce multiple, unpredictable outputs in response to the same input, requiring users to navigate variation rather than expect consistency.
- **active engagement over passive efficiency**‚ÄîInstead of frictionless interactions, GenAI demands deliberate engagement, requiring users to experiment, iterate, and refine outputs to get meaningful results.

## 2 Key UX Design Paradigm Shifts

GenAI requires two key UX design paradigm shifts, as follows:

1. From predictability to generative variability and exploration
2. From seamlessness to active engagement

### 1. From Predictability to Generative Variability and Exploration

> GenAI creates multiple possible outputs in response to the same prompt. ‚Ä¶ UX designers should create user experiences that support exploration, iteration, and refinement.

Traditional software delivers consistent, repeatable results, but GenAI creates multiple possible outputs in response to the same prompt. Instead of designing a single correct response, UX designers should create user experiences that support exploration, iteration, and refinement.

- **Example 1**‚ÄîGoogle Bard presents multiple draft responses to a query, reinforcing the reality that AI-generated text is inherently variable.
- **Example 2**‚ÄîMidJourney and DALL-E generate multiple image options for each prompt, shifting the user's role from receiver to curator.

### 2. From Seamlessness to Active Engagement

> GenAI shifts the focus from efficiency to collaboration. AI is not an infallible expert, but a co-creator that requires user input, verification, and adjustment.

Traditional UX design aims to streamline workflows and automate decision-making, but GenAI shifts the focus from efficiency to collaboration. AI is not an infallible expert, but a co-creator that requires user input, verification, and adjustment.

- **Example 1**‚ÄîAdobe Photoshop's Generative Fill lets designers modify AI-generated images using a familiar user interface, treating AI as an assistive tool rather than a fully autonomous system.
- **Example 2**‚ÄîGitHub Copilot suggests multiple code completions, but developers must actively review, test, and refine its suggestions, highlighting the importance of human oversight.

This shift from automation to collaboration requires rethinking user interactions, embracing uncertainty, iteration, and controlled friction rather than aiming for perfect predictability. Just as users must engage, refine, and interpret AI-generated content, they must also develop new skills to navigate generative variability rather than expecting seamless, deterministic automation.

## Rethinking UX for GenAI: A Framework for Product Teams

> UX designers must prioritize transparency, control, and collaboration.

Because GenAI disrupts traditional UX design principles, UX designers must prioritize transparency, control, and collaboration.

The framework provides a structured approach that is based on six key design principles:

### 1. Design Responsibly

Ensure that AI is ethical, fair, and minimizes risks such as bias, misinformation, and copyright violations. For example, the maker of DALL-E discloses that they train the Generative Fill feature on stock and public-domain data.

### 2. Design for Generative Variability

Help users navigate AI's unpredictability by offering multiple outputs and tracking version history. For example, MidJourney generates multiple image variations that users can refine, then select the best one.

### 3. Design for Mental Models

Guide users in understanding how AI works and adapting their interactions accordingly. For example, GitHub Copilot follows an autocomplete pattern, making AI-powered coding suggestions easier to use.

### 4. Design for Co-Creation

> Give users more control over AI-generated content, making AI a creative partner rather than an automated tool.

Give users more control over AI-generated content, making AI a creative partner rather than an automated tool. For example, Adobe Firefly lets users tweak AI-generated designs, ensuring that they remain active participants in the creative process.

### 5. Design for Appropriate Trust and Reliance

Help users calibrate their trust in AI by making its strengths, limitations, and information sources transparent. For example, ChatGPT warns users that its responses might be inaccurate, reinforcing users' healthy skepticism.

### 6. Design for Imperfection

Prepare users for flawed AI outputs and give them tools to refine, regenerate, or give feedback. For example, Google Bard lets users modify AI-generated text‚Äîmaking it shorter, longer, or simpler‚Äîencouraging iteration.

## Conclusion: The Future of UX in a Generative World

> [UX] research must evolve to meet the challenges and opportunities that GenAI presents.

This is the final part of our six-part series on UX research for GenAI. Throughout our series, we have explored how research must evolve to meet the challenges and opportunities that GenAI presents. From reframing research approaches to understanding how GenAI impacts the user experience, we have examined the roles of trust, transparency, and human-centered design in shaping the future of AI-powered products.

As emphasized, "Calibrating users' trust is crucial for establishing appropriate reliance." The future of UX for GenAI is not just about making AI easier to use, but about ensuring that AI works with users, fostering transparency, exploration, and informed decision-making. As generative technologies continue to evolve, UX professionals can play a critical role in shaping AI-driven experiences that empower rather than overwhelm users.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [Designing AI for Human Expertise: Preventing Cognitive Shortcuts](https://www.uxmatters.com/mt/archives/2025/02/designing-ai-for-human-expertise-preventing-cognitive-shortcuts.php)  
> for internal educational use only (non-profit).

# Designing AI for Human Expertise: Preventing Cognitive Shortcuts

> There is a growing concern that our reliance on AI systems promotes cognitive offloading, diminishes critical thinking, and disrupts the development of human mastery.

Artificial intelligence (AI) is increasingly woven into the fabric of our daily lives, from recommendation engines to large language models (LLMs) that assist with our professional tasks. However, there is a growing concern that our reliance on AI systems promotes cognitive offloading, diminishes critical thinking, and disrupts the development of human mastery. As users delegate reasoning to AI systems, bypassing traditional methods of developing expertise, they reduce their critical engagement with their tasks. All of these factors warrant a deeper exploration to understand the implications of AI on human cognition, creativity, and innovation.

Current research into the influence of AI across different age groups and contexts is beginning to expose the full impact of cognitive offloading. Gerlich's 2025 study, "AI Tools in Society: Impacts on Cognitive Offloading and the Future of Critical Thinking," provides valuable insights into this phenomenon. Through a mixed-methods approach to research, involving 666 participants, Gerlich found that heavy AI use significantly reduced users' critical-thinking skills, mainly because users offloaded cognitive tasks to AI tools rather than engaging deeply with problems themselves. Younger participants, in particular, exhibited a higher dependence on AI tools and lower critical-thinking scores, emphasizing the need for strategies to mitigate these cognitive costs.

> As users delegate reasoning to AI systems, bypassing traditional methods of developing expertise, they reduce their critical engagement with their tasks.

Gerlich's research builds on previous research into other information-on-demand technologies such as Internet searches, mobile-phone directories, and global-positioning systems (GPS), each of which reduces the need for memorization and the internalization of knowledge. Collectively, these tools have reshaped cognitive practices by externalizing memory and diminishing the need to internalize spatial, procedural, domain-specific knowledge and even cognitive skills such as memorizing phone numbers and other behavior patterns. AI now extends this trend into areas requiring higher-order thinking and expertise, further challenging traditional pathways to skills development. As AI integrates with more business processes, we risk eroding the expertise that users develop through repetition, intuitive automation, and unconscious competence. Like AI, humans rely on training data‚Äîthat they gain through real-world experiences, critical engagement, and internalization‚Äîto develop intuition, pattern recognition, and instinctive decision-making. When people bypass these processes, their depth of expertise and the ability to take leaps of creativity and innovate suffer.

## Why This Matters: Innovation, National Security, and Future Generations

AI's integration into workflows poses a significant challenge: the erosion of critical-thinking skills. Without these skills, the human ability to innovate through intuitive leaps‚Äîthose Eureka! moments when insights converge to spark groundbreaking ideas‚Äîdiminishes. This loss affects individual's expertise, as well as the collective capacity for creativity and problem-solving.

Innovation drives business differentiation, giving companies a competitive edge. When work relies solely on variations of pre-existing AI-generated outputs, organizations risk stagnation because of their inability to forge unique paths or disrupt industries.

Moreover, the implications extend beyond economics to national security. A workforce that depends on AI at the expense of critical thinking impairs future generations' ability to lead in thought, technology, and innovation. Within an increasingly competitive global landscape, countries that foster the development of deep cognitive skills retain a strategic advantage‚Äîeconomically and militarily‚Äîensuring their dominance in AI-powered business, manufacturing, healthcare, warfare, cybersecurity, and defense technologies. Although initiatives such as President Trump's push for global AI dominance underscore the urgency of leading in AI technology, we must also prioritize the cultivation of human expertise for long-term strategic advantage. Striking a balance between AI advancements and the preservation of human ingenuity is critical to maintaining a competitive edge globally.

## AI Design Challenges and Opportunities

> The challenge lies in designing tools and processes that maintain AI's velocity and efficiency while preserving critical thinking and mastery‚Äîa balance that fosters innovation rather than dependency, reinforces critical thinking, engages human creativity, and encourages deep cognitive interaction.

The potential cognitive impacts of broad AI use present UX designers and business-process analysts with a new challenge: balancing the immense potential of AI with the foundational principles of human mastery. We must leverage AI to streamline decision-making and avoid redundant focus on skills we've already mastered, but not at the expense of informed decision-making and opportunities for developing and retaining new skills or deepening our expertise. The challenge lies in designing tools and processes that maintain AI's velocity and efficiency while preserving critical thinking and mastery‚Äîa balance that fosters innovation rather than dependency, reinforces critical thinking, engages human creativity, and encourages deep cognitive interaction.

The rise of AI tools in UX and customer experience (CX) design highlights how these technologies can inadvertently foster the illusion of expertise. For example, UX designers might use AI-generated wireframes, personas, journey maps, or design recommendations without fully understanding the data or assumptions driving those outputs. This could result in user experiences that reinforce biases or address surface-level issues while missing deeper usability challenges.

Consider a healthcare app as an example. Without domain expertise, a UX designer might overlook critical nuances such as the emotional and cognitive needs of patients and caregivers. If the designer has never spoken to real patients, he might fail to understand the cognitive impairments patients experience from physical illness, pain, fatigue, and emotional distress. Similarly, without engaging with caregivers, he might not grasp the pressures they face in balancing their lives while caring for family members. Each of these oversights presents an opportunity to innovate‚Äîto meet an unmet need that could provide a market differentiator or inspire a new product line.

As UX designers and process analysts, we are uniquely positioned to influence the design and implementation of AI to enhance its role as a decision-support tool that complements human judgment. AI should develop and build upon human cognition rather than replacing it. To reach this goal we must overcome or mitigate the following design challenges:

- the allure of cognitive offloading
- the perception of AI infallibility
- reinforcement of cognitive bias
- encouragement of passive thinking
- loss of domain-knowledge retention and development
- overestimation of skills and promotion of shallow understanding
- impaired ability to discern true human expertise
- the echo-chamber effect
- psychological effect of AI output speed
- isolation of AI-augmented workforces

## The Allure of Cognitive Offloading

According to cognitive-load theory, humans naturally conserve cognitive resources by simplifying tasks. This evolutionary tendency becomes problematic when AI-generated information creates an illusion of expertise, bypassing deeper cognitive engagement and encouraging reliance on external systems. This occurs particularly with LLMs, which often present information with authoritative certainty despite their limitations. Gerlich's research confirms this tendency, showing that participants frequently trusted AI systems because of their speed and apparent expertise, causing cognitive offloading and diminishing critical engagement. Consider mitigating this challenge using the following user-interface (UI) disclosures:

- **Provide transparency into confidence levels**: AI systems should disclose their confidence levels, the limitations of their training data, and potential sources of error. Transparency could help users understand when to question or verify AI outputs.
- **Define the boundaries of AI capabilities**: AI systems should regularly remind users of the boundaries of their capabilities. Tools should prominently display information about what the AI can and cannot do.

## The Perception of AI Infallibility

> Trust in AI is dangerous in contexts requiring nuanced understanding or iterative problem-solving because it risks eroding human expertise and intuition. ‚Ä¶ While an AI can detect correlations and suggest outcomes, it cannot comprehend meaning or context as human experts do.

AI's ability to process vast amounts of data quickly reinforces the perception of expertise, leading users to assume that AI-generated outputs are infallible. UX studies demonstrate that people equate technical complexity with competence, further deepening their trust in AI. However, such trust in AI is dangerous in contexts requiring nuanced understanding or iterative problem-solving because it risks eroding human expertise and intuition. Despite their sophistication, AI systems are not sentient experts. They are pattern recognizers and their knowledge is limited to the data on which they were trained. While an AI can detect correlations and suggest outcomes, it cannot comprehend meaning or context as human experts do. Consider mitigating this challenge through training and UI notifications, as follows:

- **Institute AI literacy**: Prioritize the teaching of digital-literacy and critical-thinking skills, empowering users to assess the reliability of AI-generated information.
- **Provide prompt-creation support**: AI user interfaces could provide users with more instruction and feedback on developing effective prompts and asking exploratory questions to bridge their knowledge gaps. Tools that offer guided prompts or highlight related knowledge paths can help users refine their inputs and expand their understanding.

## Reinforcement of Cognitive Bias

By presenting results in a deterministic manner, many AI-driven tools and systems risk reinforcing cognitive biases that lead users to over-trust simple solutions. User prompts reflect existing domain knowledge, opinions, and biases, resulting in responses that are constrained by the user's limited perspective. Confirmation bias can lead users to selectively trust AI outputs that align with their expectations, while ignoring the AI's limitations or alternative interpretations. Consider mitigating this challenge by using UX design patterns and conducting systems audits.

- **Challenge cognitive biases**: Embed features that highlight alternative interpretations or edge cases. For example, tools could surface contrasting data points or scenarios to encourage critical thinking.
- **Conduct continuous auditing**: Periodically conduct manual reviews and testing to identify bias, improve accuracy, and ensure that outputs remain aligned with ethical guidelines.
- **Solicit real-time user feedback**: Involve subject-matter experts (SMEs) more consistently in day-to-day oversight, enabling real-time fine-tuning. Embedded feedback functions, performance monitors, and dashboards that provide oversight and track anomalies can alert teams to potential drift in performance and ensure timely intervention.

## Encouragement of Passive Thinking

> Frequent interactions with LLMs fosters passivity rather than active knowledge-building and critical thinking.

Research shows that frequent interactions with LLMs fosters passivity rather than active knowledge-building and critical thinking. In UX design and business process re-engineering (BPR) this could lead to a phenomenon in which users repeatedly defer to an AI, without forming their own opinions or a deeper understanding of workflows. This results in an ever-increasing skills gap over time. Consider mitigating this challenge through UX design, training, or cultural changes, as follows:

- **Reinforce human-AI collaboration**: UX designers must internalize users' real-world problems; leveraging distributed cognition through collaboration with experts, engineers, and users to challenge biases; think critically, and identify opportunities for innovation. An AI can assist by compiling feedback from user-research data and identifying themes and patterns, thereby reinforcing, and prioritizing key insights.
- **Promote active engagement**: Design AI systems that require users to input their reasoning or decisions before showing them AI suggestions. For example, in business-analytics tools, you could ask users to predict outcomes or flag potential patterns before presenting AI-driven insights.
- **Develop critical-thinking exercises**: Regularly include exercises that require employees to challenge AI outputs or provide counterarguments, fostering a culture of questioning and innovation.
- **Reward human contributions**: Acknowledge and celebrate moments in which human creativity or expertise have made a difference, reinforcing the value of critical thinking and hands-on experience.
- **Create diverse input mechanisms**: Design processes in which users interact with an AI an through various modalities such as text, visuals, and interactive decision trees. This approach engages different cognitive pathways and reduces the risk of passive acceptance.

## Loss of Domain-Knowledge Development, Retention, and Recall

Past research shows that, as people rely more on external tools such GPS and search engines, they might become less likely to retain information or develop problem-solving instincts. The Google effect provides an example, where users remember where to find information rather than the content itself. This loss of underlying knowledge retention could erode the iterative learning processes that are essential for building intuition and expertise. A loss of foundational knowledge and real-world context makes it harder for experts to adapt or innovate. Over time, this reduction in underlying domain knowledge inhibits an expert's ability to discern nuance, recognize debunked theories and outdated information, or recall mitigating strategies and edge cases. Consider mitigating this challenge through quizzing, reflection, and cultural changes‚Äîfor example:

- **Create memory aids and testing**: Periodically quiz or prompt users to help them recall previous decisions or key knowledge. This strengthens their memory and keeps them engaged with the material over time.
- **Embed reflection steps**: Incorporate mandatory reflection or review stages in workflows when users summarize what they've learned or explain their rationale for decisions.
- **Establish a human-centered culture**: Maintain messaging that AI is a support, not a replacement for human expertise. Embed messaging within processes that emphasize AI's role as a collaborator or assistant rather than as a decision-maker.

## Overestimation of Skills and Promotion of Shallow Understanding

> Users with only a surface-level understanding could misapply AI-generated insights in complex decision-making scenarios.

The lower understanding of a knowledge domain often correlates with an over-estimation of skill and ability, a psychological condition known as the Krueger Dunning Effect. Thus, users with only a surface-level understanding could misapply AI-generated insights in complex decision-making scenarios. For example, a junior project manager responsible for supply-chain optimization might use an AI-generated report to cut costs by eliminating a vendor without understanding the critical role that the vendor plays in maintaining redundancy during peak seasons.

Similarly, a UX designer who is unfamiliar with accessibility needs might rely on AI-recommended layouts without realizing that they must be paired with the underlying data for screen readers or alternative navigation methods. Such oversights highlight the risks of relying on AI without adequate domain knowledge. AI-generated insights that users misinterpret or blindly trust can exacerbate inequities. For example, an AI-generated design feature that is based on generic user data could unintentionally disadvantage underrepresented groups of users. Consider mitigating this challenge using UX design, business processes, training, and cultural changes, as follows:

- **Implement automation gradually**: Provide controls that let managers introduce automation gradually, allowing more junior team members to develop expertise as they use the tool. For instance, enable manual overrides and require periodic user input to prevent over-reliance on the tool by novice users. Create reports that evaluate the sophistication of user engagement to identify skills gaps and inform career-development plans.
- **Establish feedback loops**: Provide users with detailed feedback, explaining how the AI made certain decisions or why the AI system made specific suggestions. Conversely, encourage expert users to provide feedback to refine and inform AI responses. This both reinforces users' understanding of the underlying data and improves decision-making processes.
- **Create simulated learning environments**: Use AI to create immersive simulations or scenario-based training, encouraging users to engage with problem-solving in realistic contexts without offloading cognitive effort entirely.
- **Foster a knowledge-sharing culture**: Encourage employees to share their expertise by documenting insights or mentoring others, ensuring the distribution of knowledge rather than confining knowledge to individuals or tools.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [The Evolving Role of AI in User Experience](https://www.uxmatters.com/mt/archives/2025/07/the-evolving-role-of-ai-in-user-experience.php)  
> for internal educational use only (non-profit).

# The Evolving Role of AI in User Experience

> Artificial intelligence ‚Ä¶ can now help businesses create smarter, more personalized digital products.

Artificial intelligence (AI) is no longer a futuristic concept. This technology can now help businesses create smarter, more personalized digital products. It delivers a smoother, more meaningful experience to the users of apps, Web sites, and other platforms. In the vast world of user experience, AI is enhancing the way users behave and interact with digital products. AI can not only simplify the user journey but also provide flawless interactions that have a natural feel. Thus, AI is completely shifting the way we design for users.

You might wonder: what is making AI so valuable to businesses? The answer lies in its ability to remove guesswork. AI helps businesses make well-informed, data-driven product and design decisions, which in turn, helps them improve user-interface design and deliver better navigation and more personalized user experiences. With artificial intelligence's recent popularity, this technology has become an important part of digital-product development across industries. Let's take a look at how AI is transforming UX design and users' interactions with digital platforms.

## How AI is Changing the Digital User Experience

> Products are becoming more adaptive, easier to use, and better tailored to individual needs.

No longer a buzzword in the world of technology, AI is now a core technology that is changing users' digital experiences by influencing digital-product design. Rather than promoting static design, AI enables systems to respond quickly to users' behaviors. This helps them create more realistic, fluid, and user-friendly experiences. Thus, products are becoming more adaptive, easier to use, and better tailored to individual needs.

Because AI adapts easily to the behaviors and preferences of users, it delivers experiences that feel truly personal and engaging. Most amazingly, it does more than just personalize content; it also makes Web sites and apps easier to use, more accessible, and faster to respond with smart suggestions. Let's consider a few examples that demonstrate how AI is enhancing modern user experiences.

### Smart Personalization

AI provides more relevant user experiences to people by analyzing their behaviors. It notices how they navigate, what they're searching for, what they're clicking, and how they're using a platform. When people revisit a shopping app or site, they might find exactly what they're looking for based on their past behaviors.

Some new users might receive a warm welcome note conveying special offers or simple instructions on how to get them started. The user interface might also change or adjust, depending on what features people are using or what they're looking for.

While such simple changes might seem small, they can enhance the user experience and help people find what they need. This kind of approach leads to more natural interactions for users, who are more likely to return to a Web site or app.

### Predictive User Experiences That Anticipate Users' Needs

Most people think that AI is primarily about responding to users' behavior and actions. But AI can do more than this. AI is also good at predicting what users might need next. These predictions are possible because AI tracks the behaviors of individual users and analyzes how they use a product by identifying their patterns and offering support at the right time.

Often, users get stuck filling out forms. AI can automatically step in and provide support without the user's asking. On the basis of users' previous activity, AI can suggest features or content that perfectly match their interests. When AI offers suggestions, it can save users a lot of time and effort.

### Real-Time Adaptation

> Adaptations might include adjusting the layouts of pages to show the most relevant content, repositioning controls, or changing the text size for easy reading.

With the growing popularity of AI, modern Web sites and apps are now providing real-time, personalized experiences to their users. By keeping a very close eye on user behaviors, AI can help user interfaces to adapt in real time and respond to them immediately.

These adaptations might include adjusting the layouts of pages to show the most relevant content, repositioning controls, or changing the text size for easy reading. These kinds of innovative adjustments don't require any user actions, they work automatically in the background. They also simplify and support users' efforts and enhance clarity, enabling users to reach their goals more rapidly and efficiently.

### Conversational User Interfaces and Virtual Assistants

Because AI is growing at a rapid pace, digital platforms are starting to rely on conversational AIs to enhance user support. AI virtual assistants and chatbots are the best and most effective ways of doing this. Chatbots provide instantaneous help and can handle everything from a question to a task with ease.

These virtual assistants have the remarkable ability to interpret natural-language context sets, which helps them understand differently from traditional tools. These smart systems are not limited only to automation. In critical situations, they know when to transition users to human support. Combining human support with AI results in a more connected, seamless user experience.

### Voice User Interfaces and Accessibility

> The key factor in the popularity of voice user interfaces ‚Ä¶ is users' desire for hands-free convenience.

Users always prefer greater ease of use; this is why the adoption of voice assistants such as Siri, Alexa, and Google Assistant is spreading. The key factor in the popularity of voice user interfaces (VUIs) is users' desire for hands-free convenience. AI is powering this evolution. Now, systems can easily understand context and speech and give a helping hand to people who have physical limitations.

The best part of voice technology is that it supports users with navigation and search, especially when they are driving or busy with other tasks. By deploying VUI technologies thoughtfully, businesses can enhance their accessibility and stay responsive to how users are engaging.

### Recognizing Emotions with Adaptive User Interfaces

There is no doubt that AI is growing and advancing over time. You can see this in new features that support the recognition of emotions and adaptive user interfaces. Some advanced AI models have the power to recognize users' emotional states.

Now, AI can detect how users are feeling by the way they are speaking, by analyzing their facial expressions, and by assessing their typing speed. Such emotional insights enable AI to provide improved support. For example, when a user becomes annoyed, AI can detect this and offer more supportive guidance. Similarly, AI can guide users with advanced options for fast navigation. This personalized touch creates more natural, interactive experiences.

## Ethical Considerations and User Trust

> To build trust, businesses should clearly explain what information they are collecting and how they intend to use it.

AI has the potential to transform user experiences, but the basis for user trust requires greater responsibility and clarity.

### Data Privacy and Consent

To provide a personalized experience, AI relies on the private data of users, which is often sensitive information. This is the main reason that transparency is so essential. To build trust, businesses should clearly explain what information they are collecting and how they intend to use it.

Most importantly, they should be more open regarding their data practices and offer simple privacy tools to their users. This is necessary not only because legal compliance is crucial, but to gain their users' trust. Empower users with simple, transparent control over their data to earn their trust.

### Ethical Use of AI

There is no doubt that AI can make personalization more innovative and more effective, but the key is to ensure that we deliver personalization with ethics in mind. Businesses must avoid crossing over into manipulation. When businesses use dark patterns or hidden decision logic, this breaks users' trust and frustrates them. UX developers should maintain transparency and user control so users know why certain suggestions appear.

## Future UX Design Trends That AI Is Powering

AI's impact on digital interactions will keep advancing, with notable trends such as the following:

- **emotionally aware UX design**‚ÄîAs AI evolves in the future it will gain the ability to detect emotional signals, more deeply understand users' emotions, and enable user interfaces to provide more empathic responses that automatically enhance user interactions.
- **proactive user support**‚ÄîAI will completely change users' digital experiences by stepping in proactively. Rather than waiting for users to ask for help, AI will assist them by analyzing their behaviors and acting based on users' emotions.
- **instant reconfiguration of user interfaces**‚ÄîDigital user experiences will be more adaptive in the future. AI will enable Web sites and apps to adjust page layouts and features instantly, tailoring them to match users' behaviors and activities.

## Final Thoughts

> The true value of AI in UX design ‚Ä¶ comes from understanding users deeply, honoring their choices, and designing with empathy.

AI is now a cornerstone of UX design because it enables more personal, adaptive, easy-to-use digital environments. It is completely changing the way people interact with technology in their everyday lives. AI can provide faster user experiences, smarter results, and more responsive user interfaces.

But the true value of AI in UX design doesn't lie simply in using the latest technology. It comes from understanding users deeply, honoring their choices, and designing with empathy. When we apply AI thoughtfully, it does more than just streamline interactions, it creates digital experiences that feel natural, supportive, and genuinely enjoyable.

> **Note:** The following article is reproduced verbatim from  
> UXMatters Team, *UXMatters* (2025):  
> [Generative AI in UX Design: Transforming Interactions](https://www.uxmatters.com/mt/archives/2025/05/generative-ai-in-ux-design-transforming-interactions.php)  
> for internal educational use only (non-profit).

# Generative AI in UX Design: Transforming Interactions

> GenAI facilitates personalization, reduces the amount of time designers spend on iterations, increases design efficiency, and creates new creative opportunities.

Generative artificial intelligence (GenAI) is an intriguing aspect of artificial intelligence whose impacts on UX design projects are intensifying. Within the current product-development scenario, UX designers can generate innovative concepts and deliver enhanced digital experiences through GenAI transformations. GenAI is innovative technology that, based on AI algorithms, is potentially capable of creating new content‚Äîranging from text, images, videos, and other elements that are part of creating user interfaces. In UX design, designers can harness GenAI to streamline their workflows, enhance their creativity, and personalize user experiences.

For instance, a UX designer could enter a few criteria into a GenAI tool to produce several variations of a design. GenAI facilitates personalization, reduces the amount of time designers spend on iterations, increases design efficiency, and creates new creative opportunities. As we learn more about the intriguing nexus of AI and UX design, GenAI can create incredibly easy-to-use, user-focused solutions, opening the door to a future in which human creativity and technology coexist.

## What Is GenAI?

> GenAI is driven by vast datasets that analyze underlying patterns and structures to generate relevant materials as a result of a designer's prompt.

GenAI is an integral aspect of AI technology that responds to user prompts and can instantly create new content, whether text, images, music, or any other form of media. GenAI is not limited to predefined rules, but follows smart algorithms that might use deep learning, generative adversarial networks (GANs), or transformer models to generate responses that are similar to human creativity.

GenAI is driven by vast datasets that analyze underlying patterns and structures to generate relevant materials as a result of a designer's prompt. GenAI applications can automate content creation and support artistic endeavors to drive both innovations in product design and personalized marketing strategies.

## Benefits of GenAI in UX Design

GenAI has transformed the way UX designers used to create designs from scratch before GenAI applications existed. With GenAI, a multitude of benefits have elevated the design process, improving user experiences and fostering innovation. These benefits include the following:

- **enhanced creativity**‚ÄîMany UX designers enthusiastically employ the power of GenAI to generate innovative designs and story lines, following fresh creative directions. The analysis and synthesis of vast amounts of data make this possible.
- **personalization**‚ÄîThe user data at the core of GenAI strengthens its performance by customizing user experiences based on users' personal preferences and behaviors. Personalization using GenAI can stabilize user engagement and improve user satisfaction.
- **rapid prototyping**‚ÄîUX designers can create a large number of design prototypes quickly with the help of GenAI tools. This enables faster revisions and quicker cycles between the concept and testing phases.
- **data-driven insights**‚ÄîUX designers can improve their designs, examine existing user interactions, and apply concepts that are in line with a business's creative brief.
- **increased efficiency**‚ÄîUX designers can eliminate redundant tasks and focus more on advanced creative processes that lead to strategic decision-making. The rapid generation of fresh ideas can greatly help UX designers in differentiating their designs in comparison to those of other brands.

## Use Cases of GenAI in UX Design

> AI-driven personalization enables apps to instantly adjust to the preferences of individual users, while also improving the usability of the user interface.

By increasing the creativity and efficiency of the UX design process, GenAI is revolutionizing the field of UX design. AI-driven personalization enables apps to instantly adjust to the preferences of individual users, while also improving the usability of the user interface. The following are some real-world examples of domains that can benefit from the application of GenAI in UX design:

- **technology**‚ÄîIn the technology sector, where software programs and platforms often need customized user experiences to improve usability and engagement, GenAI UX design has broad applications.
- **ecommerce**‚ÄîIn the ecommerce sector, GenAI simplifies the user's shopping experience, increases personalization by using chatbots to recommend products, and answer shoppers' queries based on their individual preferences.
- **healthcare**‚ÄîHealthcare is a sensitive sector that deals with critical cases, saving the lives of patients every day. Applying GenAI to UX design can play a critical role in simplifying patients' connections and interactions with medical service providers, enabling them to access the information they need in a timely manner through the development of user-friendly applications with impressive, easy-to-navigate designs.
- **gaming**‚ÄîThe gaming industry is an attractive one for GenAI, which can develop dynamic gaming concepts and immersive user experiences that are compatible with different types of players.
- **education**‚ÄîThrough the power of GenAI, the education sector has developed learning platforms and practical courses that help learners with different learning styles to understand concepts easily.
- **finance**‚ÄîGenAI in the finance sector has facilitated the development of reliable banking and investment applications, increasing accessibility to financial products and services.
- **entertainment**‚ÄîThe entertainment sector uses GenAI to optimize content suggestions and user-interface components and provide highly tailored, interesting user experiences across platforms such as streaming services.

## Challenges and Considerations of GenAI in UX Design

> Data privacy, bias in AI algorithms, and the possibility of producing inaccurate or inappropriate content are ethical issues that we must closely monitor to ensure moral standards in the UX design process.

A multitude of challenges arise in the synergy between GenAI and UX design, as follows:

- **ethical implications**‚ÄîData privacy, bias in AI algorithms, and the possibility of producing inaccurate or inappropriate content are ethical issues that we must closely monitor to ensure moral standards in the UX design process.
- **integration with existing workflows**‚ÄîIntegrating GenAI into existing UX design workflows can be disruptive, necessitating team adaptations and perhaps intensive retraining to make the most of AI technologies.
- **quality control**‚ÄîAs soon as the user gives a prompt to a GenAI tool, it generates a vast amount of data that could raise quality concerns and even outputs that lack high quality standards. Therefore, the deployment of robust review systems is mandatory to ensure adequate quality assurance for AI outputs.
- **skills development and training**‚ÄîBoth novice and experienced UX designers require skills development and training to understand the latest applications of GenAI technologies in effectively generating high-quality designs for a variety of users.
- **integration with traditional design methods**‚ÄîIntegrating AI tools with traditional design processes can be difficult for UX designers and requires careful planning to ensure that innovations complement rather than interfere with UX design workflows.

## The Future of GenAI in UX Design

> AI ‚Ä¶ can empower UX designers by automating repetitive tasks, generating wireframes, and prototyping user interfaces at unprecedented speeds.

The future of GenAI in UX design can enhance the professional lives of UX designers and the design and innovation process, while considering also users' interests and preferences. As AI technologies evolve, they'll provide upgraded features that can empower UX designers by automating repetitive tasks, generating wireframes, and prototyping user interfaces at unprecedented speeds.

The AI algorithms of GenAI can streamline and accelerate routine UX design tasks such as wireframe generation, layout optimization, and personalized content creation. Plus, AI agent development is paving the way for intelligent UX solutions, whose AI-driven assistants can autonomously refine and optimize user interfaces based on real-time interactions and user behaviors. With greater design automation, UX designers will gain ample time to invest in the more strategic aspects of design.

GenAI can also create a collaborative loop between the AI and human designers, resulting in visually engaging, highly functional designs for different industries. The integration of GenAI into UX design can enhance UX designers' creativity, helping them solve complex problems and create paths to innovations that are dynamic and adapt to the users that products serve.

## Conclusion

> UX designers must be agile enough to develop and adapt their GenAI skills and seamlessly incorporate AI technology into their workflows if they want to remain competitive in a world that is increasingly automated.

In conclusion, the advent of GenAI in UX design is enabling UX designers to take new approaches to innovating design solutions. Because GenAI works with advanced algorithms and machine-learning techniques, UX designers can personalize design solutions to cater to specific users' needs. Automating the creation of different designs lets designers present a range of creative possibilities.

As GenAI advances, AI-driven, user-centric design methods might be able to develop more engaging, user-friendly user interfaces that add value to the business. However, UX designers must be agile enough to develop and adapt their GenAI skills and seamlessly incorporate AI technology into their workflows if they want to remain competitive in a world that is increasingly automated.

## Sources

- **NN Group Research**: [AI Chatbots Discourage Error Checking](https://www.nngroup.com/articles/ai-chatbots-discourage-error-checking/)
- **Prompt Engineering Guide**: [AI UX Patterns](https://www.promptingguide.ai/)
- **Anthropic Research**: [Building Effective AI Agents](https://www.anthropic.com/engineering/building-effective-agents)
- **CrewAI Documentation**: [Agent Design Patterns](https://docs.crewai.com/en/concepts/agents)
- **Claude Documentation**: [Best Practices for Claude](https://docs.anthropic.com/en/docs/best-practices)

## Figures

- Conversation flow diagrams showing good vs bad patterns
- Confidence indicator mockups
- Error handling state diagrams
- User control interface examples
- Progressive disclosure examples
- Accessibility interface mockups

