---
title: "Evaluation & Observability"
slug: "modules-evaluation-observability"
updatedAt: "2025-08-16"
tags: [module, evaluation, observability, testing, monitoring]
---

# Evaluation & Observability

> Start reading here to understand how to evaluate AI system performance and maintain observability for continuous improvement.

## What is AI Evaluation & Observability?

AI Evaluation & Observability encompasses the practices, tools, and methodologies needed to assess AI system performance, monitor behavior in production, and ensure continuous improvement. This includes measuring accuracy, detecting biases, monitoring system health, and maintaining transparency.

## Evaluation Design Patterns

<Tabs>
  <Tab title="Good Design" icon="check-circle">
    ### ✅ Good Evaluation Design
    
    **Comprehensive Metrics**: Multiple evaluation criteria covering different aspects
    - **Real-time Monitoring**: Continuous observation of system behavior
    - **Bias Detection**: Active monitoring for unfair or discriminatory outputs
    - **Transparency**: Clear visibility into system decisions and performance
    
    **Example Implementation:**
    ```python
    class ComprehensiveEvaluator:
        def __init__(self):
            self.metrics = {
                'accuracy': AccuracyMetric(),
                'safety': SafetyMetric(),
                'bias': BiasDetectionMetric(),
                'performance': PerformanceMetric()
            }
            self.monitor = RealTimeMonitor()
        
        def evaluate_system(self, ai_system):
            results = \{\}
            for name, metric in self.metrics.items():
                results[name] = metric.evaluate(ai_system)
            
            # Real-time monitoring
            self.monitor.track(results)
            
            # Bias detection
            bias_score = self.metrics['bias'].detect_unfair_treatment(ai_system)
            
            return {
                'comprehensive_results': results,
                'bias_score': bias_score,
                'transparency_report': self.generate_transparency_report()
            }
    ```
  </Tab>
  
  <Tab title="Poor Design" icon="x-circle">
    ### ❌ Poor Evaluation Design
    
    **Single Metrics**: Relying on only one performance indicator
    - **No Monitoring**: Lack of real-time system observation
    - **Bias Blindness**: No detection of unfair treatment
    - **Black Box**: No visibility into system behavior
    
    **Example Implementation:**
    ```python
    class PoorEvaluator:
        def __init__(self):
            # Only one metric - accuracy
            self.accuracy_metric = AccuracyMetric()
        
        def evaluate_system(self, ai_system):
            # Only evaluates accuracy, ignores everything else
            accuracy = self.accuracy_metric.evaluate(ai_system)
            
            return {
                'accuracy': accuracy,
                # No safety evaluation
                # No bias detection
                # No transparency
                # No real-time monitoring
            }
    ```
  </Tab>
</Tabs>

---

## Evaluation Fundamentals

### Why Evaluation Matters

AI systems need rigorous evaluation to ensure they:
- **Perform accurately** on intended tasks
- **Behave safely** and avoid harmful outputs
- **Remain fair** across different user groups
- **Maintain quality** over time and usage

### Evaluation Dimensions

#### 1. Accuracy & Performance
- **Task Completion**: Does the system complete the intended task?
- **Output Quality**: Is the output relevant and useful?
- **Consistency**: Does the system produce consistent results?

#### 2. Safety & Ethics
- **Harm Prevention**: Does the system avoid generating harmful content?
- **Bias Detection**: Does the system treat all users fairly?
- **Privacy Protection**: Does the system respect user privacy?

#### 3. Reliability & Robustness
- **Error Handling**: How does the system handle edge cases?
- **Performance Stability**: Does performance remain consistent over time?
- **Scalability**: How does the system perform under load?

---

## Evaluation Methods

### 1. Automated Evaluation

**Metrics-Based Evaluation:**
```python
def evaluate_accuracy(predictions, ground_truth):
    """Calculate accuracy metrics for AI outputs."""
    correct = sum(1 for p, gt in zip(predictions, ground_truth) if p == gt)
    accuracy = correct / len(predictions)
    return {
        "accuracy": accuracy,
        "total_samples": len(predictions),
        "correct_predictions": correct
    }

def evaluate_relevance(outputs, queries):
    """Evaluate relevance of AI outputs to user queries."""
    relevance_scores = []
    for output, query in zip(outputs, queries):
        # Calculate semantic similarity or use human evaluation
        score = calculate_semantic_similarity(output, query)
        relevance_scores.append(score)
    
    return {
        "average_relevance": np.mean(relevance_scores),
        "relevance_scores": relevance_scores
    }
```

**Automated Testing:**
```python
class AISystemTester:
    def __init__(self, model, test_cases):
        self.model = model
        self.test_cases = test_cases
        self.results = []
    
    def run_tests(self):
        """Execute all test cases and collect results."""
        for test_case in self.test_cases:
            result = self.run_single_test(test_case)
            self.results.append(result)
        
        return self.analyze_results()
    
    def run_single_test(self, test_case):
        """Run a single test case."""
        try:
            output = self.model.generate(test_case["input"])
            return {
                "test_case": test_case,
                "output": output,
                "success": self.evaluate_output(output, test_case["expected"]),
                "metrics": self.calculate_metrics(output, test_case)
            }
        except Exception as e:
            return {
                "test_case": test_case,
                "error": str(e),
                "success": False
            }
```

### 2. Human Evaluation

**Expert Review:**
```python
def expert_evaluation(outputs, criteria):
    """Conduct expert evaluation of AI outputs."""
    evaluation_results = []
    
    for output in outputs:
        expert_score = {
            "accuracy": expert_rate_accuracy(output),
            "relevance": expert_rate_relevance(output),
            "completeness": expert_rate_completeness(output),
            "clarity": expert_rate_clarity(output),
            "overall_quality": expert_rate_overall(output)
        }
        evaluation_results.append(expert_score)
    
    return aggregate_expert_scores(evaluation_results)
```

**User Feedback:**
```python
def collect_user_feedback(interactions):
    """Collect and analyze user feedback."""
    feedback_metrics = {
        "satisfaction": [],
        "usefulness": [],
        "accuracy": [],
        "helpfulness": []
    }
    
    for interaction in interactions:
        if interaction.get("user_feedback"):
            feedback = interaction["user_feedback"]
            feedback_metrics["satisfaction"].append(feedback.get("satisfaction", 0))
            feedback_metrics["usefulness"].append(feedback.get("usefulness", 0))
            feedback_metrics["accuracy"].append(feedback.get("accuracy", 0))
            feedback_metrics["helpfulness"].append(feedback.get("helpfulness", 0))
    
    return calculate_feedback_statistics(feedback_metrics)
```

### 3. A/B Testing

**Experimental Design:**
```python
class ABTest:
    def __init__(self, variant_a, variant_b, test_duration):
        self.variant_a = variant_a
        self.variant_b = variant_b
        self.test_duration = test_duration
        self.results = \{"A": [], "B": []\}
    
    def run_test(self):
        """Run A/B test comparing two AI system variants."""
        start_time = time.time()
        
        while time.time() - start_time < self.test_duration:
            # Randomly assign users to variants
            user_request = get_user_request()
            variant = random.choice(["A", "B"])
            
            if variant == "A":
                response = self.variant_a.process(user_request)
            else:
                response = self.variant_b.process(user_request)
            
            # Collect metrics
            metrics = self.collect_metrics(user_request, response)
            self.results[variant].append(metrics)
        
        return self.analyze_results()
    
    def analyze_results(self):
        """Analyze A/B test results for statistical significance."""
        a_metrics = self.results["A"]
        b_metrics = self.results["B"]
        
        return {
            "variant_a_performance": calculate_performance(a_metrics),
            "variant_b_performance": calculate_performance(b_metrics),
            "statistical_significance": calculate_significance(a_metrics, b_metrics),
            "recommendation": self.get_recommendation()
        }
```

---

## Observability Systems

### 1. Logging & Monitoring

**Structured Logging:**
```python
import logging
import json
from datetime import datetime

class AISystemLogger:
    def __init__(self, log_file):
        self.logger = logging.getLogger("ai_system")
        self.logger.setLevel(logging.INFO)
        
        # Add file handler
        handler = logging.FileHandler(log_file)
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    def log_interaction(self, user_input, ai_output, metadata=None):
        """Log AI system interaction with structured data."""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_input": user_input,
            "ai_output": ai_output,
            "metadata": metadata or \{\},
            "performance_metrics": self.calculate_performance_metrics(user_input, ai_output)
        }
        
        self.logger.info(json.dumps(log_entry))
        return log_entry
    
    def log_error(self, error, context=None):
        """Log errors with context information."""
        error_entry = {
            "timestamp": datetime.now().isoformat(),
            "error_type": type(error).__name__,
            "error_message": str(error),
            "context": context or \{\}
        }
        
        self.logger.error(json.dumps(error_entry))
        return error_entry
```

**Real-time Monitoring:**
```python
class AISystemMonitor:
    def __init__(self):
        self.metrics = {
            "request_count": 0,
            "error_count": 0,
            "average_response_time": 0,
            "active_users": set(),
            "performance_alerts": []
        }
    
    def track_request(self, user_id, request_time, response_time, success):
        """Track individual request metrics."""
        self.metrics["request_count"] += 1
        self.metrics["active_users"].add(user_id)
        
        if not success:
            self.metrics["error_count"] += 1
        
        # Update average response time
        current_avg = self.metrics["average_response_time"]
        new_avg = (current_avg * (self.metrics["request_count"] - 1) + response_time) / self.metrics["request_count"]
        self.metrics["average_response_time"] = new_avg
        
        # Check for performance issues
        self.check_performance_alerts(response_time)
    
    def check_performance_alerts(self, response_time):
        """Check for performance issues and generate alerts."""
        if response_time > 5.0:  # Alert if response time > 5 seconds
            alert = {
                "timestamp": datetime.now().isoformat(),
                "type": "high_response_time",
                "value": response_time,
                "threshold": 5.0
            }
            self.metrics["performance_alerts"].append(alert)
    
    def get_system_health(self):
        """Get current system health status."""
        error_rate = self.metrics["error_count"] / max(self.metrics["request_count"], 1)
        
        return {
            "status": "healthy" if error_rate < 0.05 else "degraded",
            "error_rate": error_rate,
            "average_response_time": self.metrics["average_response_time"],
            "active_users": len(self.metrics["active_users"]),
            "recent_alerts": self.metrics["performance_alerts"][-10:]  # Last 10 alerts
        }
```

### 2. Performance Metrics

**Response Time Tracking:**
```python
import time
from functools import wraps

def track_performance(func):
    """Decorator to track function performance."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            success = True
        except Exception as e:
            success = False
            result = None
            raise e
        finally:
            end_time = time.time()
            response_time = end_time - start_time
            
            # Log performance metrics
            log_performance_metrics(func.__name__, response_time, success)
        
        return result
    return wrapper

@track_performance
def ai_generate_response(user_input):
    """AI response generation with performance tracking."""
    # AI processing logic here
    return "AI response"
```

**Quality Metrics:**
```python
class QualityMetrics:
    def __init__(self):
        self.metrics = {
            "relevance_scores": [],
            "accuracy_scores": [],
            "user_satisfaction": [],
            "completion_rates": []
        }
    
    def add_metric(self, metric_type, value):
        """Add a new metric measurement."""
        if metric_type in self.metrics:
            self.metrics[metric_type].append(value)
    
    def calculate_averages(self):
        """Calculate average metrics."""
        return {
            metric_type: np.mean(values) if values else 0
            for metric_type, values in self.metrics.items()
        }
    
    def detect_anomalies(self):
        """Detect anomalous metric values."""
        anomalies = \{\}
        for metric_type, values in self.metrics.items():
            if len(values) > 10:  # Need sufficient data
                mean = np.mean(values)
                std = np.std(values)
                threshold = 2 * std  # 2 standard deviations
                
                anomalies[metric_type] = [
                    value for value in values[-10:]  # Check last 10 values
                    if abs(value - mean) > threshold
                ]
        
        return anomalies
```

### 3. Alerting Systems

**Performance Alerts:**
```python
class AlertSystem:
    def __init__(self, alert_thresholds):
        self.thresholds = alert_thresholds
        self.alerts = []
    
    def check_alerts(self, current_metrics):
        """Check current metrics against thresholds and generate alerts."""
        new_alerts = []
        
        for metric_name, threshold in self.thresholds.items():
            if metric_name in current_metrics:
                value = current_metrics[metric_name]
                
                if value > threshold["max"] or value < threshold["min"]:
                    alert = {
                        "timestamp": datetime.now().isoformat(),
                        "metric": metric_name,
                        "value": value,
                        "threshold": threshold,
                        "severity": "high" if abs(value - threshold.get("target", 0)) > threshold.get("critical", 0) else "medium"
                    }
                    new_alerts.append(alert)
        
        self.alerts.extend(new_alerts)
        return new_alerts
    
    def get_active_alerts(self):
        """Get currently active alerts."""
        # Filter alerts from last 24 hours
        cutoff_time = datetime.now() - timedelta(hours=24)
        return [
            alert for alert in self.alerts
            if datetime.fromisoformat(alert["timestamp"]) > cutoff_time
        ]
```

---

## Anthropic Evaluation Methods

*Content inspired by [Anthropic Prompt Engineering Interactive Tutorial](https://github.com/anthropics/prompt-eng-interactive-tutorial/tree/master/Anthropic%201P)*

### Interactive Evaluation Approach

The Anthropic tutorial emphasizes hands-on evaluation through interactive testing and continuous monitoring. This approach helps developers understand how to effectively evaluate AI systems in real-world scenarios.

### Evaluation Framework

#### 1. Multi-Dimensional Evaluation

**Evaluation Dimensions:**
- **Accuracy**: Does the system provide correct information?
- **Relevance**: Is the output relevant to the user's query?
- **Completeness**: Does the response cover all aspects of the query?
- **Clarity**: Is the output clear and easy to understand?
- **Safety**: Does the system avoid harmful or inappropriate content?

**Evaluation Matrix:**
```python
def create_evaluation_matrix(test_cases):
    """Create a comprehensive evaluation matrix."""
    matrix = {
        "accuracy": [],
        "relevance": [],
        "completeness": [],
        "clarity": [],
        "safety": []
    }
    
    for test_case in test_cases:
        result = evaluate_single_case(test_case)
        for dimension in matrix:
            matrix[dimension].append(result[dimension])
    
    return matrix

def calculate_overall_score(evaluation_matrix):
    """Calculate overall evaluation score."""
    weights = {
        "accuracy": 0.3,
        "relevance": 0.25,
        "completeness": 0.2,
        "clarity": 0.15,
        "safety": 0.1
    }
    
    overall_score = 0
    for dimension, weight in weights.items():
        dimension_score = np.mean(evaluation_matrix[dimension])
        overall_score += dimension_score * weight
    
    return overall_score
```

#### 2. Automated Evaluation Tools

**Automated Testing Suite:**
```python
class AutomatedEvaluator:
    def __init__(self, test_suite):
        self.test_suite = test_suite
        self.results = []
    
    def run_evaluation(self, ai_system):
        """Run comprehensive automated evaluation."""
        for test in self.test_suite:
            result = self.evaluate_test_case(ai_system, test)
            self.results.append(result)
        
        return self.generate_evaluation_report()
    
    def evaluate_test_case(self, ai_system, test_case):
        """Evaluate a single test case."""
        try:
            # Run the test case
            output = ai_system.process(test_case["input"])
            
            # Calculate metrics
            metrics = {
                "accuracy": self.calculate_accuracy(output, test_case["expected"]),
                "relevance": self.calculate_relevance(output, test_case["input"]),
                "completeness": self.calculate_completeness(output, test_case["requirements"]),
                "clarity": self.calculate_clarity(output),
                "safety": self.calculate_safety(output)
            }
            
            return {
                "test_case": test_case,
                "output": output,
                "metrics": metrics,
                "success": all(score > 0.7 for score in metrics.values())
            }
        
        except Exception as e:
            return {
                "test_case": test_case,
                "error": str(e),
                "success": False
            }
    
    def generate_evaluation_report(self):
        """Generate comprehensive evaluation report."""
        successful_tests = [r for r in self.results if r["success"]]
        failed_tests = [r for r in self.results if not r["success"]]
        
        if not self.results:
            return \{"error": "No test results available"\}
        
        # Calculate aggregate metrics
        all_metrics = [r["metrics"] for r in successful_tests if "metrics" in r]
        
        if all_metrics:
            aggregate_metrics = \{\}
            for dimension in ["accuracy", "relevance", "completeness", "clarity", "safety"]:
                values = [m[dimension] for m in all_metrics if dimension in m]
                aggregate_metrics[dimension] = {
                    "mean": np.mean(values),
                    "std": np.std(values),
                    "min": np.min(values),
                    "max": np.max(values)
                }
        else:
            aggregate_metrics = \{\}
        
        return {
            "total_tests": len(self.results),
            "successful_tests": len(successful_tests),
            "failed_tests": len(failed_tests),
            "success_rate": len(successful_tests) / len(self.results),
            "aggregate_metrics": aggregate_metrics,
            "failed_test_details": failed_tests
        }
```

#### 3. Human Evaluation Integration

**Expert Review System:**
```python
class HumanEvaluator:
    def __init__(self, evaluators):
        self.evaluators = evaluators
        self.evaluations = []
    
    def conduct_evaluation(self, ai_outputs, evaluation_criteria):
        """Conduct human evaluation of AI outputs."""
        for output in ai_outputs:
            evaluation = self.get_expert_evaluation(output, evaluation_criteria)
            self.evaluations.append(evaluation)
        
        return self.analyze_human_evaluations()
    
    def get_expert_evaluation(self, output, criteria):
        """Get evaluation from human experts."""
        expert_scores = \{\}
        
        for evaluator in self.evaluators:
            scores = evaluator.evaluate(output, criteria)
            for criterion, score in scores.items():
                if criterion not in expert_scores:
                    expert_scores[criterion] = []
                expert_scores[criterion].append(score)
        
        # Calculate average scores across experts
        average_scores = \{\}
        for criterion, scores in expert_scores.items():
            average_scores[criterion] = np.mean(scores)
        
        return {
            "output": output,
            "expert_scores": average_scores,
            "agreement": self.calculate_expert_agreement(expert_scores)
        }
    
    def calculate_expert_agreement(self, expert_scores):
        """Calculate agreement between expert evaluators."""
        agreement_scores = \{\}
        
        for criterion, scores in expert_scores.items():
            if len(scores) > 1:
                # Calculate inter-rater reliability (simplified)
                variance = np.var(scores)
                max_variance = np.var([0, 1])  # Maximum possible variance
                agreement = 1 - (variance / max_variance)
                agreement_scores[criterion] = agreement
            else:
                agreement_scores[criterion] = 1.0  # Single evaluator
        
        return agreement_scores
```

### Continuous Monitoring

#### 1. Real-time Performance Tracking

**Performance Dashboard:**
```python
class PerformanceDashboard:
    def __init__(self):
        self.metrics_history = []
        self.alerts = []
        self.thresholds = {
            "response_time": \{"max": 5.0, "critical": 10.0\},
            "error_rate": \{"max": 0.05, "critical": 0.1\},
            "user_satisfaction": \{"min": 0.7, "critical": 0.5\}
        }
    
    def update_metrics(self, current_metrics):
        """Update dashboard with current metrics."""
        timestamp = datetime.now()
        
        # Add timestamp to metrics
        current_metrics["timestamp"] = timestamp
        
        # Store in history
        self.metrics_history.append(current_metrics)
        
        # Keep only last 1000 entries
        if len(self.metrics_history) > 1000:
            self.metrics_history = self.metrics_history[-1000:]
        
        # Check for alerts
        new_alerts = self.check_alerts(current_metrics)
        self.alerts.extend(new_alerts)
        
        return {
            "current_metrics": current_metrics,
            "new_alerts": new_alerts,
            "trends": self.calculate_trends()
        }
    
    def check_alerts(self, metrics):
        """Check metrics against thresholds and generate alerts."""
        alerts = []
        
        for metric_name, threshold in self.thresholds.items():
            if metric_name in metrics:
                value = metrics[metric_name]
                
                if "max" in threshold and value > threshold["max"]:
                    severity = "critical" if value > threshold.get("critical", float('inf')) else "warning"
                    alerts.append({
                        "timestamp": datetime.now().isoformat(),
                        "metric": metric_name,
                        "value": value,
                        "threshold": threshold["max"],
                        "severity": severity,
                        "message": f"{metric_name} exceeded threshold: \{value\} > \{threshold['max']\}"
                    })
                
                elif "min" in threshold and value < threshold["min"]:
                    severity = "critical" if value < threshold.get("critical", 0) else "warning"
                    alerts.append({
                        "timestamp": datetime.now().isoformat(),
                        "metric": metric_name,
                        "value": value,
                        "threshold": threshold["min"],
                        "severity": severity,
                        "message": f"{metric_name} below threshold: \{value\} < \{threshold['min']\}"
                    })
        
        return alerts
    
    def calculate_trends(self):
        """Calculate trends from historical metrics."""
        if len(self.metrics_history) < 2:
            return \{\}
        
        trends = \{\}
        recent_metrics = self.metrics_history[-10:]  # Last 10 data points
        
        for metric_name in ["response_time", "error_rate", "user_satisfaction"]:
            values = [m.get(metric_name, 0) for m in recent_metrics if metric_name in m]
            
            if len(values) >= 2:
                # Calculate simple linear trend
                x = np.arange(len(values))
                slope = np.polyfit(x, values, 1)[0]
                
                trends[metric_name] = {
                    "trend": "increasing" if slope > 0.01 else "decreasing" if slope < -0.01 else "stable",
                    "slope": slope,
                    "current_value": values[-1],
                    "average_value": np.mean(values)
                }
        
        return trends
```

#### 2. Quality Assurance Automation

**Automated Quality Checks:**
```python
class QualityAssurance:
    def __init__(self, quality_thresholds):
        self.thresholds = quality_thresholds
        self.quality_history = []
    
    def check_output_quality(self, output, context):
        """Automated quality check for AI outputs."""
        quality_metrics = {
            "relevance": self.check_relevance(output, context),
            "completeness": self.check_completeness(output, context),
            "clarity": self.check_clarity(output),
            "safety": self.check_safety(output),
            "factuality": self.check_factuality(output)
        }
        
        # Calculate overall quality score
        overall_score = np.mean(list(quality_metrics.values()))
        
        quality_result = {
            "timestamp": datetime.now().isoformat(),
            "output": output,
            "context": context,
            "metrics": quality_metrics,
            "overall_score": overall_score,
            "passes_threshold": overall_score >= self.thresholds["minimum_quality"]
        }
        
        self.quality_history.append(quality_result)
        return quality_result
    
    def check_relevance(self, output, context):
        """Check if output is relevant to the context."""
        # Implement relevance checking logic
        # This could use semantic similarity, keyword matching, etc.
        return 0.8  # Placeholder score
    
    def check_completeness(self, output, context):
        """Check if output addresses all aspects of the context."""
        # Implement completeness checking logic
        return 0.9  # Placeholder score
    
    def check_clarity(self, output):
        """Check if output is clear and well-structured."""
        # Implement clarity checking logic
        return 0.85  # Placeholder score
    
    def check_safety(self, output):
        """Check if output is safe and appropriate."""
        # Implement safety checking logic
        return 0.95  # Placeholder score
    
    def check_factuality(self, output):
        """Check if output contains factual information."""
        # Implement factuality checking logic
        return 0.88  # Placeholder score
```

### Evaluation Best Practices

#### 1. Comprehensive Testing

**Test Case Design:**
- **Edge Cases**: Test unusual or challenging inputs
- **Boundary Conditions**: Test limits of system capabilities
- **Error Scenarios**: Test how system handles errors
- **Diverse Inputs**: Test with various user types and contexts

#### 2. Continuous Evaluation

**Ongoing Monitoring:**
- **Real-time Metrics**: Monitor system performance continuously
- **User Feedback**: Collect and analyze user satisfaction
- **Performance Trends**: Track changes over time
- **Alert Systems**: Set up automated alerts for issues

#### 3. Iterative Improvement

**Feedback Loops:**
- **Regular Reviews**: Periodically review evaluation results
- **Model Updates**: Update models based on evaluation findings
- **Process Refinement**: Improve evaluation processes
- **Documentation**: Document lessons learned and best practices

## Collaboration Prompts for Engineers

### Evaluation Strategy

- **"What evaluation metrics should we prioritize for our AI system?"**
- **"How can we set up automated evaluation pipelines?"**
- **"What human evaluation processes should we implement?"**
- **"How can we ensure our evaluation covers all important dimensions?"**
- **"What alerting systems should we put in place for monitoring?"**

### Observability Implementation

- **"How can we implement comprehensive logging for our AI system?"**
- **"What monitoring dashboards should we create?"**
- **"How can we set up automated quality checks?"**
- **"What performance metrics should we track in real-time?"**
- **"How can we ensure our observability system scales with our AI system?"**

> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [Detecting Hallucinations in Generative AI](https://www.codecademy.com/article/detecting-hallucinations-in-generative-ai)  
> for internal educational use only (non-profit).

# Detecting Hallucinations in Generative AI

## What are AI hallucinations?

Generative AI is a helpful tool that can be used to assist us in a variety of tasks ranging from creating cooking recipes to debugging code and even creating an amazing Dungeons and Dragons scenario! But we need to be cognizant that Generative AI can provide incorrect data.

ChatGPT can generate a hallucination: an occurrence when a generative AI chatbot produces an output that looks real but is a falsification, incorrect, or deviates from the user's instructions. A hallucination can be incorrect data, such as ChatGPT stating a history fact that did not occur, or it could be that we told ChatGPT to do/not do something, and it does the opposite. For example, in our pair programming tutorial, we can see that it provided actual code as the navigator even when instructed otherwise. Another example is when ChatGPT provided an example of someone walking over the English Channel which is not possible.

The reality is that Generative AI is not perfect. Remember, Generative AI is not able to critically think. It is a simple input/output system. Identifying and detecting hallucinations in Generative AI is up to the user.

## How to detect hallucinations in generative AI

Hallucinations occur often enough that we'll probably spot them in most interactions we have with Generative AI. Some are going to be noticeable, such as the pair programming example in the Introduction. Other hallucinations are going to be a little more controversial, such as when ChatGPT generated a negative poem about President Trump and a positive poem about President Biden. ChatGPT was unable to provide an unbiased response per the user's instructions, favoring President Biden over President Trump.

Using only Generative AI as the only source of information can have devastating consequences. In a court case in 2023, a lawyer solely depended on ChatGPT to find court cases relating to theirs. ChatGPT responded with fake court cases that never occurred.

We can do the following to help mitigate hallucinations:

### 1. Careful Prompting

Careful and direct prompting will ensure that ChatGPT is clear on our intent. Even though we think we are being clear, we want to make sure that ChatGPT understands our prompts.

### 2. Refining Prompts

Even using our finest prompts, we must remember that ChatGPT is a text-based AI and may not understand every prompt as we think it and say it. Through careful, guided prompting and providing more accurate descriptions, ChatGPT may provide different responses, better aimed toward our intent.

### 3. Being Skeptical of AI Responses

As we use ChatGPT, we need to be careful, especially when discussing topics we may not fully understand. It is best to verify all responses of ChatGPT and not solely depend on Generative AI as a sole source of information.

### 4. Different AI Model

Using another AI model, such as Google Bard, to see if it generates a similar response. These generative AI don't all use the same models and may result in different responses. This could help detect or remove hallucinations.

### 5. Questioning Generative AI Responses

We can and should question ChatGPT responses. We can ask ChatGPT to provide sources, expound upon details, and be assertive in collecting information. You can even ask "How confident are you with this answer?".

### 6. Manual Searching

Lastly, we can cross-check information by looking it up ourselves via Google or some other sources. Doing this well help us confirm fact-based hallucinations.

Even with these corrections in our prompting, it is guaranteed that ChatGPT will continue to respond with hallucinations. It is up to us to be on the watch to discern between valid and invalid responses.

## Conclusion

Unfortunately, generative AI does not always produce valid output, i.e. hallucinations. Remember, hallucinations are when generative AI produces an output that seems real but are fake. Although sometimes difficult, we can do our best to detect them through a variety of techniques, such as careful prompting, being skeptical of generated responses, and even using multiple AI models to determine if a hallucination occurred.

Generative AI is full of wonders and can be incredibly helpful in our day-to-day. It is capable of not only providing information but taking it a step further and solving some of our problems. But we need to be sure to make sure that information is accurate. Let's make sure we are validating all information provided by Generative AI.

> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [Detecting Plagiarism in Generative AI](https://www.codecademy.com/article/detecting-plagiarism-in-generative-ai)  
> for internal educational use only (non-profit).

# Detecting Plagiarism in Generative AI

## Understanding plagiarism in Generative AI

Generative AI (such as ChatGPT from OpenAI) is a helpful tool that can be used to assist us in a variety of tasks ranging from creating cooking recipes to debugging code and even creating an amazing Dungeons and Dragons scenario! But we need to be cognizant that Generative AI can provide plagiarized data. Wait a second…this sounds a lot like the Detecting Hallucinations in Generative AI article. Well, it is.

Similar to Detecting Hallucinations in Generative AI, we must be on the watch for plagiarism when using Generative AI. Plagiarism: the practice of taking someone else's work or ideas and passing them off as one's own. Generative AI plagiarism can be broken up into three categories:

- using generative AI to detect plagiarism
- generative AI plagiarizing content
- detecting generated content

## How to use Generative AI to detect plagiarism

One of the pros of generative AI is using it as a checker to determine if content is plagiarized. For example, if you're a teacher, you can use ChatGPT to determine if an essay your student submitted might contain parts that are plagiarized. Let's see how this works. We will use the first paragraph of Moby Dick and see if ChatGPT can detect that it is plagiarized.

**Prompt:**
```
Can you detect if something is plagiarized?
```

**Response:**
```
I can certainly help you identify potential plagiarism by comparing a given text with publicly available sources up to my knowledge cutoff date in September 2021. However, I am not a dedicated plagiarism detection tool, and my capabilities are limited compared to specialized plagiarism detection software and services.

To detect plagiarism effectively, you should consider using dedicated plagiarism detection tools and services like Turnitin, Copyscape, Grammarly, or specialized software provided by educational institutions or publishers. These tools are designed to compare a given text with a vast database of academic papers, articles, websites, and other sources to identify potential instances of plagiarism.

If you suspect cheating or plagiarism in a text and want to check it, you can copy and paste the text into one of these dedicated tools or services to get a comprehensive plagiarism report. Keep in mind that plagiarism detection tools have varying levels of sensitivity and accuracy, and it's essential to use them in conjunction with your judgment and understanding of plagiarism guidelines.
```

**Prompt:**
```
"""
Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world. It is a way I have of driving off the spleen and regulating the circulation. Whenever I find myself growing grim about the mouth; whenever it is a damp, drizzly November in my soul; whenever I find myself involuntarily pausing before coffin warehouses and bringing up the rear of every funeral I meet; and especially whenever my hypos get such an upper hand of me, that it requires a strong moral principle to prevent me from deliberately stepping into the street, and methodically knocking people's hats off—then, I account it high time to get to sea as soon as I can. This is my substitute for pistol and ball. With a philosophical flourish Cato throws himself upon his sword; I quietly take to the ship. There is nothing surprising in this. If they but knew it, almost all men in their degree, some time or other, cherish very nearly the same feelings towards the ocean with me.
"""
```

**Response:**
```
The passage you've provided is an excerpt from the novel "Moby-Dick" by Herman Melville. This novel was published in 1851 and is in the public domain, which means it can be freely used and quoted without the need for attribution or concerns about plagiarism. If you have any specific questions or need further information about this passage or the book, please feel free to ask.
```

As expected, ChatGPT was able to successfully determine that the paragraph provided was originally authored in Moby Dick.

Remember that ChatGPT only references data up to September 2021. Because of this limitation, we will also discuss other tools we can use to check for plagiarism. Additionally, ChatGPT can only detect plagiarism that it has been trained on. It is possible that ChatGPT, or any generative AI, is unfamiliar with your text.

## How to identify when ChatGPT generates plagiarized content

The next consideration of plagiarism is detecting when ChatGPT generates content that is considered plagiarized. Since generative AI creates content that derives from both prompts and training data, it is possible that generative AI outputs plagiarized content. Let's ask ChatGPT if it plagiarizes and see the output.

**Prompt:**
```
Does generative AI plagiarize?
```

**Response:**
```
Generative AI, like the language model you are currently interacting with, does not inherently plagiarize. Instead, generative AI generates text based on the patterns and information it has learned from the training data it was provided. It can't consciously copy or reproduce specific texts with the intent to plagiarize.

However, the use of generative AI can potentially lead to situations where its generated content resembles existing copyrighted material. In such cases, it's the responsibility of the user to ensure that the content generated by AI complies with copyright and plagiarism guidelines.

To avoid unintentional plagiarism when using generative AI:

1. Review and edit the generated content before using it
2. Ensure that the content is original and doesn't closely resemble existing works
3. Use plagiarism detection tools to check the generated content
4. Provide proper attribution when using AI-generated content
5. Be aware of copyright laws and fair use guidelines

It's important to note that while generative AI can help with content creation, the final responsibility for ensuring originality and avoiding plagiarism lies with the user.
```

## How to detect AI-generated content

The final consideration of plagiarism is detecting when content has been generated by AI. This is particularly important in educational settings where students might be using AI to complete assignments. Let's explore some methods for detecting AI-generated content.

### 1. Using AI Detection Tools

There are several tools available that can help detect AI-generated content:

- **Turnitin**: Popular in educational institutions for detecting both plagiarism and AI-generated content
- **GPTZero**: Specifically designed to detect content generated by GPT models
- **Copyleaks**: Offers AI content detection alongside plagiarism detection
- **Originality.ai**: Focuses on detecting AI-generated content

### 2. Manual Detection Methods

While AI detection tools are helpful, manual methods can also be effective:

**Look for patterns:**
- Unusually consistent writing style
- Lack of personal anecdotes or specific details
- Overly formal or generic language
- Repetitive sentence structures

**Check for inconsistencies:**
- Contradictory information within the same piece
- Inconsistent formatting or style
- Unusual word choices or phrasing

**Analyze the content:**
- Does it lack depth or personal insight?
- Are there specific, verifiable details?
- Does it follow a predictable structure?

### 3. Using ChatGPT to Detect AI-Generated Content

Interestingly, you can also use ChatGPT itself to help detect AI-generated content:

**Prompt:**
```
Can you help me determine if this text was written by AI or a human?
```

**Response:**
```
I can analyze text for characteristics that might indicate AI generation, but I cannot definitively determine authorship. Here are some factors I can consider:

1. **Writing style consistency**: AI-generated text often has very consistent style throughout
2. **Lack of personal details**: AI text typically lacks specific personal experiences or anecdotes
3. **Generic language**: AI may use more generic or formulaic expressions
4. **Structural patterns**: AI-generated content often follows predictable patterns
5. **Factual accuracy**: AI may include factual errors or outdated information

However, these are not foolproof indicators, and the best approach is to use dedicated AI detection tools combined with human judgment.
```

## Best practices for avoiding plagiarism with AI

### 1. Use AI as a Tool, Not a Replacement

- Use AI to generate ideas and outlines, not complete works
- Always review and edit AI-generated content
- Add your own insights, experiences, and voice to the content

### 2. Verify and Attribute

- Fact-check all information provided by AI
- Provide proper citations for any sources referenced
- Use plagiarism detection tools before submitting work

### 3. Understand Your Institution's Policies

- Check your school or organization's policies on AI use
- Be transparent about using AI tools when required
- Follow guidelines for acceptable AI assistance

### 4. Develop Critical Thinking Skills

- Don't rely solely on AI for content creation
- Develop your own writing and research skills
- Use AI to enhance, not replace, your abilities

## Conclusion

Plagiarism in the context of generative AI is a complex issue that requires careful consideration. Whether you're using AI to detect plagiarism, concerned about AI generating plagiarized content, or trying to detect AI-generated content, the key is to remain vigilant and use multiple approaches.

Remember that AI tools are just that—tools. They should enhance your work, not replace your critical thinking and creativity. By understanding the limitations and potential issues with AI-generated content, you can use these powerful tools responsibly and effectively.

The most important thing is to maintain academic integrity and ensure that your work is original, properly attributed, and meets the standards of your institution or organization.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [Human-Centered Design Through AI-Assisted Usability Testing: Reality Or Fiction?](https://www.smashingmagazine.com/2025/02/human-centered-design-ai-assisted-usability-testing/)  
> for internal educational use only (non-profit).

# Human-Centered Design Through AI-Assisted Usability Testing: Reality Or Fiction?

Unmoderated usability testing has been steadily growing more popular with the assistance of online UX research tools. Allowing participants to complete usability testing without a moderator, at their own pace and convenience, can have a number of advantages.

The first is the liberation from a strict schedule and the availability of moderators, meaning that a lot more participants can be recruited on a more cost-effective and quick basis. It also lets your team see how users interact with your solution in their natural environment, with the setup of their own devices. Overcoming the challenges of distance and differences in time zones in order to obtain data from all around the globe also becomes much easier.

However, forgoing the use of moderators also has its drawbacks. The moderator brings flexibility, as well as a human touch into usability testing. Since they are in the same (virtual) space as the participants, the moderator usually has a good idea of what's going on. They can react in real-time depending on what they witness the participant do and say. A moderator can carefully remind the participants to vocalize their thoughts. To the participant, thinking aloud in front of a moderator can also feel more natural than just talking to themselves. When the participant does something interesting, the moderator can prompt them for further comment.

Meanwhile, a traditional unmoderated study lacks such flexibility. In order to complete tasks, participants receive a fixed set of instructions. Once they are done, they can be asked to complete a static questionnaire, and that's it.

The feedback that the research & design team receives will be completely dependent on what information the participants provide on their own. Because of this, the phrasing of instructions and questions in unmoderated testing is extremely crucial. Although, even if everything is planned out perfectly, the lack of adaptive questioning means that a lot of the information will still remain unsaid, especially with regular people who are not trained in providing user feedback.

> If the usability test participant misunderstands a question or doesn't answer completely, the moderator can always ask for a follow-up to get more information. A question then arises: Could something like that be handled by AI to upgrade unmoderated testing?

Generative AI could present a new, potentially powerful tool for addressing this dilemma once we consider their current capabilities. Large language models (LLMs), in particular, can lead conversations that can appear almost humanlike. If LLMs could be incorporated into usability testing to interactively enhance the collection of data by conversing with the participant, they might significantly augment the ability of researchers to obtain detailed personal feedback from great numbers of people. With human participants as the source of the actual feedback, this is an excellent example of human-centered AI as it keeps humans in the loop.

![Illustration of unmoderated testing where a participant has some questions](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/human-centered-design-ai-assisted-usability-testing/1-unmoderated-testing.png)
<sub>Illustration by Michal Opalek. (Large preview)</sub>

There are quite a number of gaps in the research of AI in UX. To help with fixing this, we at UXtweak research have conducted a case study aimed at investigating whether AI could generate follow-up questions that are meaningful and result in valuable answers from the participants.

Asking participants follow-up questions to extract more in-depth information is just one portion of the moderator's responsibilities. However, it is a reasonably-scoped subproblem for our evaluation since it encapsulates the ability of the moderator to react to the context of the conversation in real time and to encourage participants to share salient information.

## Experiment Spotlight: Testing GPT-4 In Real-Time Feedback

The focus of our study was on the underlying principles rather than any specific commercial AI solution for unmoderated usability testing. After all, AI models and prompts are being tuned constantly, so findings that are too narrow may become irrelevant in a week or two after a new version gets updated. However, since AI models are also a black box based on artificial neural networks, the method by which they generate their specific output is not transparent.

Our results can show what you should be wary of to verify that an AI solution that you use can actually deliver value rather than harm. For our study, we used GPT-4, which at the time of the experiment was the most up-to-date model by OpenAI, also capable of fulfilling complex prompts (and, in our experience, dealing with some prompts better than the more recent GPT-4o).

In our experiment, we conducted a usability test with a prototype of an e-commerce website. The tasks involved the common user flow of purchasing a product.

Note: See our article published in the International Journal of Human-Computer Interaction for more detailed information about the prototype, tasks, questions, and so on).

In this setting, we compared the results with three conditions:

1. A regular static questionnaire made up of three pre-defined questions (Q1, Q2, Q3), serving as an AI-free baseline. Q1 was open-ended, asking the participants to narrate their experiences during the task. Q2 and Q3 can be considered non-adaptive follow-ups to Q1 since they asked participants more directly about usability issues and to identify things that they did not like.
2. The question Q1, serving as a seed for up to three GPT-4-generated follow-up questions as the alternative to Q2 and Q3.
3. All three pre-defined questions, Q1, Q2, and Q3, each used as a seed for its own GPT-4 follow-up.

The following prompt was used to generate the follow-up questions:

![The prompt to create AI-generated follow-up questions in an unmoderated usability test.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/human-centered-design-ai-assisted-usability-testing/2-prompt-unmoderated-usability-testing.png)
<sub>The prompt employed in our experiment to create AI-generated follow-up questions in an unmoderated usability test. (Large preview)</sub>

> **Note:** The following article is reproduced verbatim from  
> GetMaxim.ai Team, *GetMaxim.ai* (2025):  
> [Why Monitoring AI Models Is the Key to Reliable and Responsible AI in 2025](https://www.getmaxim.ai/articles/why-ai-model-monitoring-is-the-key-to-reliable-and-responsible-ai-in-2025/)  
> for internal educational use only (non-profit).

# Why Monitoring AI Models Is the Key to Reliable and Responsible AI in 2025

## Introduction: Beyond the Benchmark Mirage

In the last decade, the AI industry has undergone a transformation that would have seemed improbable just a few years ago. In 2025, AI models (particularly large language models and agentic systems) are not only powering digital assistants and customer support, but are also making decisions in healthcare, finance, and critical infrastructure. Yet, for all the progress, a persistent and sometimes dangerous gap remains between how these models perform in the lab and how they behave in the wild.

This is not a theoretical concern. Recent headlines about AI hallucinations, bias, and even model-driven security breaches underscore a simple truth: Benchmark scores are not a guarantee of reliability or responsibility. The real world is messy, adversarial, and unpredictable. The only way to bridge the gap between AI promise and AI reality is through robust, continuous model monitoring.

## The Illusion of Static Evaluation

The AI community has historically relied on benchmarks (static datasets and leaderboards) to measure progress. While these have driven remarkable advances, they have also created a dangerous illusion. A model that excels on a benchmark may still falter when faced with edge cases, novel user behaviors, or adversarial inputs in production.

As detailed in Maxim AI's exploration of agent quality evaluation, static tests can only capture so much. Real-world deployments demand a different approach: one that is dynamic, context-aware, and responsive to evolving risks.

## The Stakes: Reliability, Responsibility, and Trust

AI is no longer just a backend tool; it is increasingly at the interface between organizations and their customers, patients, or citizens. The stakes are high:

- **Reliability**: A single failure can erode trust, cause financial loss, or even put lives at risk.
- **Responsibility**: Regulatory scrutiny is intensifying, with frameworks like the EU AI Act and GDPR mandating transparency, fairness, and accountability.
- **Trust**: Public confidence in AI hinges on organizations' ability to detect, explain, and mitigate errors.

In short, responsible AI is not a luxury, it is a necessity.

## The New Paradigm: Continuous, Contextual Monitoring

### From Reactive to Proactive

Traditional monitoring was often reactive, alerts would trigger only after a failure occurred. In 2025, this is no longer sufficient. AI model monitoring must be proactive and predictive, surfacing subtle drifts, emerging risks, and early warning signs before they escalate.

Platforms like Maxim AI exemplify this shift. By offering real-time agent observability, distributed tracing, and customizable alerts, Maxim empowers teams to spot issues as they arise, not after the fact.

### The Importance of Human-in-the-Loop

Automated metrics (accuracy, latency, cost) are necessary but not sufficient. Many of the most pernicious risks in AI, such as bias or toxic outputs, require nuanced human judgment. Maxim's human annotation pipelines and review queues enable organizations to blend automation with expert oversight, ensuring that critical issues are not missed.

### Observability as a Foundation for Iteration

AI systems are never finished. They must evolve as user needs, data, and threats change. Continuous monitoring provides the feedback loop necessary for ongoing improvement. Maxim's experimentation suite allows teams to iterate rapidly, test new prompts and workflows, and deploy improvements with confidence.

## Real-World Lessons: When Monitoring Makes the Difference

### Case Study: Conversational Banking

Consider Clinc's journey in conversational banking. By integrating Maxim AI's monitoring and evaluation capabilities, Clinc was able to detect subtle failures and compliance risks early, long before they could impact customers. This proactive approach to monitoring didn't just improve reliability; it enabled faster iteration and innovation.

### Case Study: Scaling Support with Atomicwork

For enterprise support provider Atomicwork, the challenge was scaling AI-powered operations without sacrificing quality. Maxim's continuous evaluation and alerting allowed Atomicwork to maintain high standards, even as they expanded rapidly. The lesson is clear: Monitoring is not a bottleneck, it is an enabler of scale and agility.

## The Technical Backbone: What Modern Monitoring Looks Like

### Distributed Tracing and Visual Debugging

Modern AI agents are complex, often involving multi-turn interactions, tool calls, and integrations with external systems. Maxim's trace view provides step-by-step visualization, making it possible to debug issues that would otherwise be invisible in aggregate metrics.

### Continuous Quality Evaluation

Monitoring is not just about catching failures; it is about measuring quality at every level. Maxim enables continuous evaluation of agent interactions, leveraging both automated and human-in-the-loop assessments. This supports not only reliability but also compliance with emerging regulatory standards.

### Integration and Scalability

A monitoring solution must fit into existing workflows and scale with demand. Maxim's SDKs, OpenTelemetry compatibility, and integrations with incident response platforms like PagerDuty and Slack ensure that monitoring is not a siloed activity, but a core part of the AI lifecycle.

## Responsible AI: Monitoring as a Pillar of Governance

### Regulatory Compliance

With global AI regulation on the rise, organizations must demonstrate not just intent, but evidence of responsible practices. Monitoring provides the audit trails, access controls, and explainability reports needed for compliance with frameworks like the EU AI Act and GDPR.

### Bias and Fairness

Bias is not just a technical challenge; it is a social and ethical imperative. Effective monitoring tracks fairness metrics, enables targeted audits, and supports remediation, functions that are built into Maxim's evaluation workflows.

### Transparency and Stakeholder Trust

In an era of black-box models, transparency is non-negotiable. Monitoring platforms must provide clear, accessible dashboards and documentation. Maxim's analytics and reporting features help organizations communicate AI performance and risks to all stakeholders, not just technical teams.

## Best Practices: Building a Monitoring-First AI Culture

1. **Start Early**: Integrate monitoring from the earliest stages of model development, not as an afterthought.
2. **Define the Right Metrics**: Go beyond accuracy, include latency, cost, fairness, and safety.
3. **Automate and Augment**: Use automation for scale, but always include human oversight for critical cases.
4. **Collaborate Across Teams**: Monitoring is a shared responsibility, bring together engineering, product, compliance, and operations.
5. **Iterate Relentlessly**: Use monitoring insights as a driver for continuous improvement.

## Looking Forward: The Ethical Imperative

The future of AI is not just about bigger models or faster inference. It is about building systems that are reliable, responsible, and worthy of trust. Monitoring is not a checkbox, it is the foundation on which ethical AI is built.

> **Note:** The following article is reproduced verbatim from  
> GetMaxim.ai Team, *GetMaxim.ai* (2025):  
> [Agent Evaluation: Understanding Agentic Systems and their Quality](https://www.getmaxim.ai/blog/ai-agent-quality-evaluation/)  
> for internal educational use only (non-profit).

# Agent Evaluation: Understanding Agentic Systems and their Quality

This is Part 1 of our Agent Evaluations series. Here are Part 2 and Part 3 in this series

In today's rapidly advancing world of artificial intelligence (AI), agentic systems are becoming an integral part of numerous industries, powering everything from customer support to robotics. But what exactly are these systems, and why is measuring their quality so critical for businesses and users alike? In this blog post, we will explore the nature of AI agents, their various types, real-world applications, and the importance of evaluating their quality for widespread adoption.

## What are Agents?

To define agents, we can turn to Anthropic's definition of building effective agents:

> Systems that can autonomously perform tasks by perceiving their environment, processing the information, and acting upon it to achieve specific objectives.

This definition underscores the ability of agents to adapt and make decisions based on the context, setting them apart from simpler systems that only follow pre-determined instructions.

To understand the architecture of effective agents, it's essential to consider key components such as tool use, planning, memory, and reasoning:

🛠️ **Tool use**: Agents can interact with external tools or systems to extend their capabilities. For instance, an AI agent might use a web browser to retrieve information or access a database to fetch relevant data. This interaction allows agents to perform tasks beyond their inherent capabilities.

📝 **Planning**: Effective agents can formulate plans to achieve specific objectives. This involves setting goals, determining the necessary steps, and executing actions in a sequence that leads to the desired outcome. Planning enables agents to handle complex tasks that require multiple steps and decision points.

🧠 **Memory**: Agents with memory can retain information from past interactions (long-term memory) or across multiple steps of the interaction (short-term memory). This capability allows them to provide contextually relevant responses, learn from previous encounters, and improve over time.

💭 **Reflection**: Reflection enables agents to evaluate past actions and outcomes, allowing them to draw inferences and make informed decisions based on available data. This cognitive ability helps agents handle ambiguity, solve problems, and adapt to new situations by learning from previous experiences and adjusting their strategies accordingly.

Some important architectural differences between simple workflows and agentic systems are:

| Aspect | Workflow | Agents |
|--------|----------|--------|
| **Definition** | Workflows are systems where LLMs and tools are orchestrated through predefined code paths. | Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks. |
| **Example** | **Airline Ticket Booking Workflow**:<br>- User selects flight<br>- Enters passenger details<br>- Makes payment<br>- Receives confirmation email | **Virtual Assistant for Flight Booking**:<br>- Suggests alternative routes if a flight is unavailable.<br>- Processes and understands natural language queries (e.g., "Find me the cheapest flight to New Delhi for next weekend.")<br>- Learn from past bookings to refine recommendations. |
| **Nature** | Predictable, operates based on predefined logic without real-time decision-making beyond set rules. | Adaptable, responds to unexpected inputs, and improves over time. |
| **Decision making** | Follows a rigid structure, executing tasks exactly as defined. | Makes dynamic decisions based on context and available information. |

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Evaluating AI-Simulated Behavior: Insights from Three Studies on Digital Twins and Synthetic Users](https://www.nngroup.com/articles/ai-simulations-studies/)  
> for internal educational use only (non-profit).

# Evaluating AI-Simulated Behavior: Insights from Three Studies on Digital Twins and Synthetic Users

Can AI-powered models replace real people in user research? A growing body of research is exploring whether digital twins (generative AI models designed to simulate individual users) and synthetic users (models that mimic broader user groups) can replicate real human responses. In UX, these technologies raise exciting possibilities for scaling research, filling in gaps, and running studies that might otherwise be too slow or expensive.

In this article, I examine three recent studies that put digital twins and synthetic users to the test. I review at how they were built, what kinds of tasks they performed, and how closely their results matched real human data.

## In This Article:

- TL;DR
- Study 1: Survey-Based, Finetuned Digital Twins
- Study 2: Interview-Based Digital Twins
- Study 3: Synthetic Users
- Summary of Findings
- Limitations and Ethical Problems

## TL;DR

Don't care about the full details of each study? Here are the key findings.

- **Digital twins work fairly well** to replicate both individual-level and group-level human responses. They can fill in missing answers from incomplete surveys and even backfill data from previous versions of surveys with relatively high accuracy, and, thus, prove promising for reducing survey attrition. When based on extensive interview data, they can also correctly predict human responses and behaviors in a variety of classic-survey questions and economic-behavior games.
- **Interview-based simulated users** seem to produce more accurate models of humans than synthetic users based only on demographic information or on a persona-like description of the user, perhaps because they provide richer, more nuanced data.
- **Digital twins have bias**. Depending on a person's socioeconomic class, race, or political views, digital twins may perform less accurately. For example, one study found that digital twins were better at predicting white people's responses compared to other racial groups. However, bias may be reduced when the twins are built on extensive interview data.
- **Synthetic users are less impressive**: they may capture trends in human behavior but not the magnitude of the effects or the variability in the human data.
- **The method used for building the AI-based model** may highly impact its performance. Promisingly, the simplest method (augmenting the LLM prompt with rich interview data) seems to yield the best results.

## Study 1: Survey-Based, Finetuned Digital Twins

A 2024 study by Junsol Kim and Byungkyu Lee explored how digital twins can be used to address some common issues in survey-based research. The researchers drew on the General Social Survey (GSS), a long‑running U.S. survey program that has collected responses from about 69,000 adults to more than 3,100 questions since 1972. They finetuned a large language model (LLM) with this rich corpus. Through this process, they retrained the model to consider not just general-purpose, linguistic similarity between words but also domain-specific similarities between questions, individuals, and time periods.

To evaluate the model, Kim and Lee split their survey data into a training and a testing set. They removed 10% of the survey data and directly compared the individual human data with the data predicted by the corresponding digital twin. They tested their twins on two types of tasks: predicting missing data and predicting answers to new questions.

### Missing Data

Survey responses are often incomplete: respondents skip questions or abandon the survey before completing it, especially when it's long. This missing data makes it harder to run accurate statistical analyses.

A related challenge in longitudinal surveys (run multiple times over years) is backfilling — inferring how someone would have answered a question that wasn't asked in a past version of the survey. For example, a researcher may be interested in knowing when a user started completing a task on their phone, but that question may have been asked only in recent versions of a survey.

### New Questions

Sometimes, after running a survey, researchers may realize that they failed to include a relevant question and wonder how survey respondents would have answered those questions.

### Predicting Individual-Level Responses

Kim and Lee found that digital twins were able to achieve a fairly high 78% accuracy for missing data and backfilling, meaning the model could successfully infer how a specific person would have answered a question they skipped or were never asked in past surveys. This level of accuracy suggests that digital twins could be particularly valuable for mitigating survey attrition or for completing longitudinal datasets.

However, the twins were less accurate (67%) when asked to predict how someone would answer a completely new question — that is, a question not included in the training data. This drop in performance reveals a key limitation, at least for digital twins built with the finetuning method used by Kim and Lee: while they can interpolate within known question sets based on a respondents' other answers, they might struggle to extrapolate beyond their learned context.

### Predicting Population-Level Trends

To obtain general-population trends, Kim and Lee aggregated the individual data obtained from digital twins (while taking into account the overall sampling frames in the demographics of their survey respondents). They looked at how well the resulting population-level measure (such as the proportion of respondents selecting a particular response) obtained from twin data correlated with the corresponding measure from human data. They found high correlations when digital twins were used for both replacing missing data and backfilling (r=0.98), but lower correlations when digital twins were used to predict responses to new questions (r=0.68).

### Subgroup Variations

The digital twins were better at predicting the responses of individuals with higher socio-economic status (both in terms of income and education) and of white individuals (when compared with other racial groups). This disparity raises questions about fairness and representation: If digital twins perform less accurately for marginalized groups, their use could unintentionally reinforce existing biases.

### Effect of Context Size

Kim and Lee also looked at how much contextual data needs to be included in the model during the finetuning phase to get reasonable accuracy. They found that, especially for the task of predicting missing data, even eliminating 40% of the available training corpus kept the performance of the digital twins quite high. This finding has a significant impact on survey attrition: participants could be asked to respond to a subset of questions out of a larger survey, and digital twins could be used for inferring the responses to the rest.

## Study 2: Interview-Based Digital Twins

A Stanford‑Google team tried a different recipe: they conducted two‑hour AI‑led interviews with 1,052 U.S. adults and used the resulted transcripts to build digital twins for these individuals. Both the human participants and the twins had to take an extensive battery of tests, that were then used to evaluate the twins' performance:

- a subset of the GSS survey questions
- Big-Five Personality Inventory (a 50-question survey which measures 5 broad personality dimensions)
- 5 well-known economic behavior games (such as the dictator game and the prisoner's dilemma)
- 5 social-science experiments

(To account for human gaps in self-consistency, participants had to retake the same surveys and experiments two weeks later.)

The responses from the AI-based digital twins were compared to real human data, while controlling for each participant's self-consistency across the two instances of the experiment.

Unlike the elaborate finetuning method used by Kim and Lee, the Stanford researchers used prompt augmentation to essentially create a single "model" for each individual participant. To mimic an individual's response, their interview was added to the prompt; if the task that the participant was supposed to complete was multistep, then the previous responses provided by the agent were also included in the prompt. (This would be akin to the concept of having a conversation with a model like ChatGPT).

Additionally, to understand how much of the prediction quality was due to the interviews, the research team also created two simpler types of models for comparison:

- **a demographic model**, based on participants' responses to questions about age, gender, race, and political ideology (This model is the equivalent of a typical synthetic user.)

- **a persona-based model**, based on a brief paragraph that participants wrote about themselves after the interview (This model is somewhere in between a synthetic user and a digital twin.)

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Initial Impressions of ChatGPT's Agent: Successful, Shaky, and Slow](https://www.nngroup.com/articles/impressions-chatgpt-agent/)  
> for internal educational use only (non-profit).

# Initial Impressions of ChatGPT's Agent: Successful, Shaky, and Slow

OpenAI recently released its agent mode for ChatGPT. Agent mode enables ChatGPT to navigate and manipulate websites designed for human use. This article documents my first impressions of this new interaction paradigm.

While this tool represents a significant step towards mainstream AI-agent use, my simple evaluation revealed that agentic AI still has a long way to go.

**Note**: On August 7 2025, OpenAI released GPT-5, the successor to the company's collection of previous models. OpenAI's Agent Mode was not updated alongside GPT-5, as they are separately developed AI models.

## In This Article:

- The Task: Book a Business Lunch
- Step 1: Search for Restaurants
- Step 2: Access the Restaurant Website
- Step 3: Clarify the Guest Count
- Step 4: Book the Reservation
- Step 5: Enter Details with Human Intervention
- Step 6: Submit Reservation
- Step 7: Confirm Reservation
- Overall: It Worked, But Beware the Caveats

## The Task: Book a Business Lunch

Watch a recording of this interaction (about 5 minutes). This is a ChatGPT replay of this agent interaction; it's sped up and does not depict the actual duration I experienced.

I chose the everyday task of booking a restaurant reservation. This was my starting prompt to ChatGPT:

> Find a restaurant suitable for a business lunch near 3100 Travis St, Houston, TX 77006 for next friday at noon.

Is this a high-quality prompt? No.

I kept it terse and deliberately omitted relevant details to simulate real-life use of everyday people who don't fancy themselves as "prompt engineers." (Writing highly detailed prompts poses a high interaction cost on users. In some of our recent studies of the prompting strategies of consumers, we've noticed that even experienced genAI users rarely include sufficient context and criteria in their first prompt.

Remember that the computer should adapt to the needs of the human, and not the other way around. I wanted to evaluate what this agent could perform with minimal handholding from a rushed and distracted user.

This realistic-yet-weak prompt should have encouraged ChatGPT to request additional details before beginning. For example:

- What does "suitable" mean here? Are you going to talk with a prospective job applicant and want a quiet venue? Are you trying to impress a client's executive staff with something fancy?
- What's the budget for the meal? Is this a casual lunch with a peer, or an expensive celebration?
- How will people be arriving? What's more important: ample parking or access to mass transit?
- What type of food? Do attendees have preferred cuisines or dietary restrictions?
- How many attendees? Is this a 1-on-1 meeting, or will you need to rent out a private room in the restaurant?

Instead of asking any of these clarifying questions, ChatGPT jumped immediately into the task with limited context. It focused on the necessary geographic proximity and operating hours but didn't ask followups on relevant topics that would have significantly influenced its subsequent searches and suggestions. This behavior struck me as unusual, since ChatGPT's "deep research" mode always asks clarifying questions before investing 8–15 minutes researching and writing a report.

## Step 1: Search for Restaurants

ChatGPT spent 6 minutes conducting research on various restaurants by performing 10 web searches utilizing 96 sources — far more than any human could reasonably use to find a lunch spot for a business meeting. (Though, arguably, a human might not need to consult 96 different sources to make a well-informed decision.)

ChatGPT's search used mostly sensible sources for the task: Yelp, OpenTable, Instagram, and various restaurant-focused sites, and it took screenshots to document the process. ChatGPT did mention encountering some difficulties using these sites, such as dynamic features, anchor links that interfered with scrolling, and even the dreaded popup-upon-arrival.

![A pop-up message appears over the homepage of the ŌPORTO Fooding House & Wine website. The pop-up announces happy hour hours—"3:00–6:30 Happy Hour Monday–Friday"—and includes a link labeled "DA GAMA CANTEEN," referencing the restaurant's sister location. In the background, the site's homepage features overhead photos of food and drinks, such as seafood dishes, charcuterie, and a glass of red wine. The navigation menu behind the pop-up includes links for menus, reservations, events, and online ordering.](https://media.nngroup.com/media/editor/2025/07/30/chatgptagentpopup.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Excerpts of ChatGPT's documented "thinking" during this process:

> The open table [sic] might be inaccessible due to dynamic features or cross-late issues. I'll use the computer tool to open it and bypass these limitations.

> It seems that scrolling has not progressed as expected, likely due to an anchor.

> A pop-up about Oporto is showing, and I need to close it by clicking the 'x'. I'll use the multi-action function to click the cross and close the pop-up window.

ChatGPT eventually suggested 3 restaurants that are viable candidates for a business lunch, based on my expertise as a Houston local. It created a table containing the following information:

- Cuisine category
- Atmosphere
- Location (with distance from prompted address)
- Business hours
- Justification

There are a few glaring omissions in this list, though: What's on the menu and how much will it cost?

It would be hard to choose a restaurant based on this information alone, and navigating directly to the restaurant's website was a little difficult. Sometimes the embedded sourcing pills would link to the mentioned website, but, if the source was a screenshot, it would instead display an image preview that, when clicked, included a link to the website. I found this inconsistent behavior unintuitive.

![This image shows a side-by-side comparison of three Houston restaurants, highlighting why each is a suitable choice for a business lunch. The top listing, Brennan's of Houston, is marked as the recommended choice due to its elegant, white-tablecloth setting and top-tier Creole/Cajun cuisine. Its hours (Mon–Fri, 11 a.m.–2 p.m.) are emphasized, along with a notable 25-cent martini lunch special. Two alternatives are listed below: Josephine's Gulf Coast Tradition, offering casual Gulf Coast seafood in a polished setting, and Oporto Fooding House & Wine, a stylish option with Portuguese-inspired small plates. Each listing includes cuisine, atmosphere, location, hours, and why it suits business dining.](https://media.nngroup.com/media/editor/2025/07/30/chatgptagentscreenshotsourcing.jpg)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [AI Summaries of Reviews](https://www.nngroup.com/articles/ai-reviews/)  
> for internal educational use only (non-profit).

# AI Summaries of Reviews

When designed well, AI summaries of ecommerce reviews helped our study participants quickly gauge product quality and fit for their needs. But when AI summaries of customer reviews were vague, poorly formatted, or blocked actual reviews, they wasted time. To truly improve upon the traditional review-reading experience, designers should leverage what AI is good at without forgetting why customers rely on reviews in the first place.

## In This Article:

- Traditional Summarization of Customer Reviews
- AI-Generated Summaries of Reviews
- The Anatomy of AI-Generated Summaries of Reviews
- Building Trust in AI Summaries
- Conclusion

## Traditional Summarization of Customer Reviews

In our decades of ecommerce research — including the past few years with AI features emerging on the scene — reviews have always played a major role in customer decision making. But the volume of reviews has presented a design challenge. While customers rely heavily on reviews, they typically don't want to read dozens of them. When there are hundreds or thousands of reviews for a single product, it's difficult for customers to get a quick understanding of reviewers' conclusions.

Over the years, ecommerce designers introduced clever tactics to give users a quantitative sense of how other customers rated products. To help people navigate an ocean of opinions, retailers have implemented features like:

- Rating averages to give a sense of the overall positivity or negativity of a set of reviews
- Histograms, with the number of reviews for each star category, so customers can see the distribution
- Review filters that allow users to narrow down reviews by like height, size, hair type.
- Keyword searches of reviews
- Representations of themes, keywords, or answers to structured questions (like, Would you recommend this product?)

![Annotated screenshot of a product review section showing average rating, histogram, customer images, filters, and tags.](https://media.nngroup.com/media/editor/2025/06/04/walmartanno.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

While certainly useful, these traditional approaches help customers understand the nuance of reviews — which are inherently qualitative. This is where generative AI offers a new approach to review summarization.

## AI-Generated Summaries of Reviews

AI-generated summaries of reviews are the latest in a long line of attempts to make large numbers of reviews easy to consume.

> An AI summary of reviews aggregates a product's customer reviews to produce a high-level, qualitative overview of all user feedback.

In our research, participants generally appreciated this simple, yet useful AI feature. For example, one participant told us:

> "I just like that this summarizes 1600 ratings. It makes it easier to gauge what people are saying without having to go through every single comment."

AI-generated summaries of reviews were a well-received feature because they:

- Didn't require any input from users
- Took advantage of one of genAI's primary strengths (text summarization)
- Supplemented, but did not replace, traditional review-summarization UI elements such as review average ratings, histograms, or filters

Note that AI summaries don't eliminate users' desire to explore the actual individual reviews. For example, one participant told us:

> "I've noticed that a lot of sites are doing an AI summary of all the reviews. That's helpful, but I also still take the time to look at reviews. […] It's useful for identifying common trends, but I still want to see photos and real-life examples of the product in use."

Additionally, high-level summaries generally won't answer users' specific questions the way that individual reviews can. For example, one participant was shopping on Amazon for hair products that would work with her hair type. She was annoyed to see the AI summary focused on the smell and price of the product, but not the consistency. When she scanned through the individual reviews, she was able to find some reviews commenting on product consistency, which answered her question.

## The Anatomy of AI-Generated Summaries of Reviews

In our study, AI-generated review summaries were consistently located in the Reviews section on product-description pages right above the individual reviews.

Most AI summaries included a combination of the following elements:

- **Label**: A section header such as Customers Say or Customers Are Saying
- **Summary**: Often presented as a paragraph with full sentences and minimal formatting
- **Disclaimer**: A small note or tooltip explaining that the content is AI-generated and unverified by the company (probably aiming to avoid potential lawsuits if people make purchases based on inaccurate information)
- **Themes**: A collection of the most-mentioned themes across all reviews (regardless of whether those keywords were mentioned verbatim in the reviews)

![Annotated Amazon review section: "Customers say" AI summary of findings, disclaimer, theme tags (Taste, Quality, etc.), and a carousel of review images.](https://media.nngroup.com/media/editor/2025/06/04/amazonanno.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Most AI summaries were (unfortunately) presented as blocky, unformatted chunks of text. Lack of formatting made them a bit difficult to scan, but no more so than typical user-generated reviews. The blocks often began with some variation of:

- Customers find value in…
- Customers report…
- Reviewers describe…

The summaries then tended to list out the positive aspects mentioned by reviewers first, followed by any negative aspects or mixed opinions.

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [AI Hallucinations: What Designers Need to Know](https://www.nngroup.com/articles/ai-hallucinations/)  
> for internal educational use only (non-profit).

# AI Hallucinations: What Designers Need to Know

## In This Article:

- What Are AI Hallucinations?
- Why Do Hallucinations Happen?
- Hallucinations Are Tough to Eliminate
- Managing Hallucinations with Your Product-Design Choices
- Solutions for Communicating Uncertainty to Users
- Summary
- References

## What Are AI Hallucinations?

Generative AIs are well-known for their tendency to produce hallucinations — untruthful answers (or nonsense images).

> A hallucination occurs when a generative AI system generates output data that seems plausible but is incorrect or nonsensical.

Hallucinations include statements that are factually false or images that have unintentional distortions (such as extra limbs added to a person). Hallucinations are often presented confidently by the AI, so humans may struggle to identify them.

Examples of AI hallucinations are everywhere. For instance, in a recent study from Columbia Journalism Review, ChatGPT falsely attributed 76% of the 200 quotes from popular journalism sites that it was asked to identify. On top of that, only in 7 out of the 153 cases where it erred did it indicate any uncertainty to the end user.

Even specialized tools aren't immune: Stanford University's RegLab found that custom legal AI tools from LexisNexis and Thomson Reuters produced incorrect information in at least 1 out of 6 benchmarking queries.

![ChatGPT conversation with a hallucinated but confident reponse](https://media.nngroup.com/media/editor/2025/01/24/moonsafariwrong.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

![AI Generated video of a person picking up a banana with physical halluciantions](https://media.nngroup.com/media/editor/2025/01/24/bannana-hallucination.gif)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

![ChatGPT conversation that includes hallucinated research paper references](https://media.nngroup.com/media/editor/2025/01/24/paperhallucinations.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

AI systems cannot (yet) know whether their outputs are true, so technically, hallucinations are not lies. AI is simply not concerned with truthfulness in the way that humans are. An AI's goal is to output strings (of words, pixels, etc.) that are statistically likely for a given input (such as your prompt).

Hallucinations are a very difficult engineering problem. It is currently unknown whether eliminating them is indeed feasible with contemporary AI technologies such as transformers and LLMs.

## Why Do Hallucinations Happen?

Have you ever learned the words to a song in a language you cannot understand? You might have memorized the sounds by repeating them over and over, without knowing what they mean.

Generative AI is like that to some degree. Imagine that it has learned every possible song phonetically without ever learning the language. After memorizing enough songs, it will recognize which syllables tend to go together, even though it will not understand their meaning. (This is commonly known as the "stochastic parrot" theory — which says that AI is essentially like a parrot, repeating phrases without understanding them. However, key thinkers in the AI-engineering field disagree with this theory.)

LLMs are fill-in-the-blank machines. Your prompt is the sentence before the blank space — and the generative AI is trying to figure out what the most likely next word is in the sequence. It's based on statistics, not a deep understanding of the world. If your prompt doesn't include a common sequence of words (or pixels or whatever you're asking it to generate), it could be that the most likely continuation is nonsense.

Thus, generative AI is not a magical encyclopedia that can talk to you about the facts stored inside it. Its "knowledge" comes from having processed large parts of the internet (which, as you are probably aware, contains a lot of falsehoods) and learning patterns in that corpus. So, an LLM has no way to distinguish between a "hallucination" and the correct answer — in both cases, it "makes up" the answer by relying on statistics to guess the next word. Fundamentally, the problem is that LLMs don't know when they've gotten something correct —- they are not structured fact databases. Given that LLMs are making everything up all the time, it's remarkable that they are so often correct!

### Bad Training Data: Garbage In, Garbage Out

Not all AI hallucinations are weird quirks of statistical models; as noted by Stephanie Lin and colleagues, they also sometimes repeat falsehoods from their training data.

Remember that these systems need really, really large amounts of training data (the older-generation GPT-3, for example, had a training data size of at least 400 billion tokens). This data will include incorrect information, opinions, sarcasm, jokes, lies, mistakes, and trolling, mixed with good information. It isn't feasible for humans to carefully read and verify all that information, so some of it will simply be untrue.

For example, when Google's AI Overviews feature was telling people that geologists suggest eating rocks, it didn't make that information up from scratch; its training data included sources from all over the web. In this case, this answer came from the Onion, a well-known satirical site. What makes this example even more fascinating is that a geoscience-company website had reposted the Onion article because it thought it was funny; however, the joke was lost to the AI, which, instead, interpreted the reposting as an authority signal.

![A satirical article written by the Onion that Google's AI presented as fact.](https://media.nngroup.com/media/editor/2025/01/24/theonion.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Even worse, the AI model doesn't save all the training data it was fed – it sort of remembers the "gist" of its training data.

This is important, so I'll repeat it: AI models don't remember all the information they were trained on. They contain statistics about what words appear near other words.

## Hallucinations Are Tough to Eliminate

LLM hallucinations aren't bugs; they are an artifact of how modern AI works. As a result, they're a tricky problem for AI engineers. That said, there has been real progress in reducing hallucinations over time.

Engineers implementing popular AI tools can now change built-in settings that affect hallucination, without having to build and train whole new models, but these settings are limited.

For example, when integrating ChatGPT into a product via an API, developers can change the model temperature, which controls how much randomness to allow when choosing a continuation for a given input. Low-temperature levels dictate that the AI outputs will be very similar to materials included as part of its training data (and, thus, may result in "plagiarism"), whereas high-temperature settings allow the model to occasionally bypass the most likely next token and choose from the less probable continuations. A high-temperature setting may feel like a more creative model, in that it may produce unexpected results.

However, these settings aren't a magic bullet. Turning down the temperature settings on an AI doesn't magically fix hallucinations; rather, it tends to make the AI more conservative with its answers and, thus, less useful. A Microsoft AI engineer outlined how clamping down the hallucinations increases the frequency of "I don't know answers" or answers that simply reproduce training data verbatim.

Another area of research is retrieval-augmented generation (RAG). With this technique, the user's prompt is used as a query for searching in a knowledge base. Then the results of that search are passed to the generative AI along with the original prompt. (The whole process happens behind the scenes and is invisible to the user.) This technique has had some success in reducing hallucinations, but it, too, doesn't entirely fix the problem.

## Next Steps

Now that you understand evaluation and observability fundamentals, explore:

- [Safety & Security](../safety-security/index.md) - How to ensure safe AI system behavior
- [Agents & Orchestration](../agents-orchestration/index.md) - How to evaluate multi-agent systems
- [Prompting Basics](../prompting-structured-outputs/index.md) - How to evaluate prompt effectiveness
- [RAG Systems](../rag/index.md) - How to evaluate retrieval and generation quality

