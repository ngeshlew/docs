---
title: "AI Foundations"
description: "Master the fundamental concepts of AI, machine learning, and large language models that form the basis of modern AI applications"
slug: "modules-foundations"
updatedAt: "2025-08-19"
tags: [module, foundations, ai, machine-learning, llms, claude, anthropic]
---

# AI Foundations

<Callout type="info">
  **Learning Objective**: Understand the fundamental concepts of AI, machine learning, and large language models that form the basis of modern AI applications and inform design decisions.
</Callout>

## Overview

AI foundations provide the theoretical and practical understanding needed to build effective AI-powered products. This module covers the core concepts, architectures, and principles that drive modern AI systems, with a focus on how these fundamentals influence design and user experience.

<CardGroup cols={2}>
  <Card title="Core Concepts" icon="brain">
    Understanding the fundamental principles of AI and machine learning enables better design decisions and more effective collaboration with engineering teams.
  </Card>
  <Card title="Practical Applications" icon="zap">
    These foundations directly inform how we design AI-powered interfaces, interactions, and user experiences.
  </Card>
</CardGroup>

## Anthropic Claude: Enterprise AI Foundation

<Callout type="info">
  **Claude Overview**: Claude is a family of highly performant and intelligent AI models built by Anthropic, designed to be the most trustworthy and reliable AI available for enterprise applications.
</Callout>

### Claude's Core Capabilities

<CardGroup cols={3}>
  <Card title="Text and Code Generation" icon="file-text">
    <ul>
      <li>Brand voice adherence for customer experiences</li>
      <li>Production-level code generation and debugging</li>
      <li>Automatic translation between languages</li>
      <li>Complex financial forecasting</li>
      <li>High-quality technical analysis</li>
    </ul>
  </Card>
  <Card title="Vision Processing" icon="eye">
    <ul>
      <li>Chart and graph analysis</li>
      <li>Code generation from images</li>
      <li>Image description for accessibility</li>
      <li>Visual data extraction</li>
      <li>Document processing</li>
    </ul>
  </Card>
  <Card title="Tool Use" icon="wrench">
    <ul>
      <li>External tool integration</li>
      <li>Structured output generation</li>
      <li>API interaction capabilities</li>
      <li>Function calling support</li>
      <li>Workflow automation</li>
    </ul>
  </Card>
</CardGroup>

### Enterprise Considerations

<Table>
  <TableHead>
    <TableRow>
      <TableHeader>Feature</TableHeader>
      <TableHeader>Description</TableHeader>
      <TableHeader>Enterprise Benefit</TableHeader>
    </TableRow>
  </TableHead>
  <TableBody>
    <TableRow>
      <TableCell><strong>Security</strong></TableCell>
      <TableCell>Enterprise-grade security, SOC II Type 2 certified, HIPAA compliance options</TableCell>
      <TableCell>Trust and compliance for sensitive data</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Trustworthiness</strong></TableCell>
      <TableCell>Resistant to jailbreaks, copyright indemnity protections</TableCell>
      <TableCell>Safe deployment in high-trust industries</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Capability</strong></TableCell>
      <TableCell>200K token context window, tool use, multimodal input</TableCell>
      <TableCell>Handles complex, real-world applications</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Reliability</strong></TableCell>
      <TableCell>Very low hallucination rates, accurate over long documents</TableCell>
      <TableCell>Consistent, dependable performance</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Global</strong></TableCell>
      <TableCell>Fluency in English and non-English languages</TableCell>
      <TableCell>International deployment capabilities</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Cost Conscious</strong></TableCell>
      <TableCell>Family of models balancing cost, performance, and intelligence</TableCell>
      <TableCell>Optimized resource utilization</TableCell>
    </TableRow>
  </TableBody>
</Table>

### Claude Model Family

<Card title="Model Selection Guide">
  <Table>
    <TableHead>
      <TableRow>
        <TableHeader>Model</TableHeader>
        <TableHeader>Best For</TableHeader>
        <TableHeader>Context Window</TableHeader>
        <TableHeader>Use Case</TableHeader>
      </TableRow>
    </TableHead>
    <TableBody>
      <TableRow>
        <TableCell><strong>Claude 3 Opus</strong></TableCell>
        <TableCell>Most intelligent, complex reasoning</TableCell>
        <TableCell>200K tokens</TableCell>
        <TableCell>Research, analysis, creative tasks</TableCell>
      </TableRow>
      <TableRow>
        <TableCell><strong>Claude 3 Sonnet</strong></TableCell>
        <TableCell>Balanced performance and speed</TableCell>
        <TableCell>200K tokens</TableCell>
        <TableCell>General business applications</TableCell>
      </TableRow>
      <TableRow>
        <TableCell><strong>Claude 3 Haiku</strong></TableCell>
        <TableCell>Fastest, most cost-effective</TableCell>
        <TableCell>200K tokens</TableCell>
        <TableCell>Simple tasks, high-volume processing</TableCell>
      </TableRow>
    </TableBody>
  </Table>
</Card>

## Why It's Important for Designers to Know

### 1. **Understanding Model Capabilities and Limitations**

<Card title="Model Understanding Framework">
  <ul>
    <li><strong>Context Windows:</strong> How much information a model can process at once</li>
    <li><strong>Token Limits:</strong> Understanding input/output constraints</li>
    <li><strong>Latency Characteristics:</strong> How long models take to respond</li>
    <li><strong>Accuracy Patterns:</strong> When models are reliable vs. uncertain</li>
    <li><strong>Bias and Safety:</strong> Understanding model limitations and risks</li>
  </ul>
</Card>

### 2. **Informing Interaction Design**

<Callout type="warning">
  **Critical Insight**: AI model limitations directly constrain what's possible in user interface design and interaction patterns.
</Callout>

<Table>
  <TableHead>
    <TableRow>
      <TableHeader>Model Limitation</TableHeader>
      <TableHeader>Design Implication</TableHeader>
      <TableHeader>Design Solution</TableHeader>
    </TableRow>
  </TableHead>
  <TableBody>
    <TableRow>
      <TableCell><strong>Context Window Limits</strong></TableCell>
      <TableCell>Can't process unlimited text</TableCell>
      <TableCell>Chunking, summarization, progressive disclosure</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Latency</strong></TableCell>
      <TableCell>Response delays affect UX</TableCell>
      <TableCell>Loading states, streaming, optimistic updates</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Uncertainty</strong></TableCell>
      <TableCell>Models can be wrong or uncertain</TableCell>
      <TableCell>Confidence indicators, fallbacks, user controls</TableCell>
    </TableRow>
    <TableRow>
      <TableCell><strong>Token Costs</strong></TableCell>
      <TableCell>Expensive to process large inputs</TableCell>
      <TableCell>Efficient prompting, caching, batching</TableCell>
    </TableRow>
  </TableBody>
</Table>

### 3. **Enabling Effective Collaboration**

<Card title="Shared Language with Engineers">
  <ul>
    <li><strong>Technical Constraints:</strong> Understand what's technically feasible</li>
    <li><strong>Performance Trade-offs:</strong> Balance quality vs. speed vs. cost</li>
    <li><strong>Architecture Decisions:</strong> Participate in system design discussions</li>
    <li><strong>User Experience Optimization:</strong> Design within technical boundaries</li>
  </ul>
</Card>

## Core AI Concepts

### 1. **Machine Learning Fundamentals**

<Accordion type="single" collapsible>
  <AccordionItem value="supervised-learning">
    <AccordionTrigger>Supervised Learning</AccordionTrigger>
    <AccordionContent>
      <Card title="Learning from Labeled Data">
        <p>Supervised learning involves training models on data where the correct answers are known.</p>
        
        <h4>Key Concepts:</h4>
        <ul>
          <li><strong>Training Data:</strong> Labeled examples used to teach the model</li>
          <li><strong>Validation:</strong> Testing on unseen data to assess performance</li>
          <li><strong>Overfitting:</strong> When a model memorizes training data instead of learning patterns</li>
          <li><strong>Generalization:</strong> Ability to perform well on new, unseen data</li>
        </ul>
        
        <h4>Design Implications:</h4>
        <ul>
          <li>Models need sufficient, high-quality training data</li>
          <li>Performance varies based on data quality and quantity</li>
          <li>Models may struggle with edge cases not in training data</li>
        </ul>
      </Card>
    </AccordionContent>
  </AccordionItem>
  
  <AccordionItem value="unsupervised-learning">
    <AccordionTrigger>Unsupervised Learning</AccordionTrigger>
    <AccordionContent>
      <Card title="Finding Patterns Without Labels">
        <p>Unsupervised learning discovers hidden patterns in data without predefined answers.</p>
        
        <h4>Key Concepts:</h4>
        <ul>
          <li><strong>Clustering:</strong> Grouping similar data points together</li>
          <li><strong>Dimensionality Reduction:</strong> Simplifying complex data representations</li>
          <li><strong>Anomaly Detection:</strong> Identifying unusual patterns or outliers</li>
        </ul>
        
        <h4>Design Implications:</h4>
        <ul>
          <li>Useful for exploratory data analysis</li>
          <li>Can reveal unexpected insights</li>
          <li>Results may be less predictable than supervised learning</li>
        </ul>
      </Card>
    </AccordionContent>
  </AccordionItem>
  
  <AccordionItem value="reinforcement-learning">
    <AccordionTrigger>Reinforcement Learning</AccordionTrigger>
    <AccordionContent>
      <Card title="Learning Through Trial and Error">
        <p>Reinforcement learning agents learn by taking actions and receiving rewards or penalties.</p>
        
        <h4>Key Concepts:</h4>
        <ul>
          <li><strong>Agent:</strong> The learning system that takes actions</li>
          <li><strong>Environment:</strong> The world in which the agent operates</li>
          <li><strong>Reward Function:</strong> Defines what constitutes success</li>
          <li><strong>Policy:</strong> The strategy for choosing actions</li>
        </ul>
        
        <h4>Design Implications:</h4>
        <ul>
          <li>Requires careful reward function design</li>
          <li>Can learn complex behaviors through interaction</li>
          <li>May take time to converge to optimal behavior</li>
        </ul>
      </Card>
    </AccordionContent>
  </AccordionItem>
</Accordion>

### 2. **Neural Networks and Deep Learning**

<Card title="Neural Network Fundamentals">
  <p>Neural networks are computational models inspired by biological brains, consisting of interconnected nodes (neurons) that process information.</p>
  
  <h4>Key Components:</h4>
  <ul>
    <li><strong>Input Layer:</strong> Receives the initial data</li>
    <li><strong>Hidden Layers:</strong> Process and transform the data</li>
    <li><strong>Output Layer:</strong> Produces the final result</li>
    <li><strong>Weights and Biases:</strong> Parameters that determine how information flows</li>
  </ul>
  
  <h4>Design Implications:</h4>
  <ul>
    <li>Models can learn complex, non-linear relationships</li>
    <li>Require significant computational resources</li>
    <li>Training can be time-consuming and expensive</li>
    <li>Results may be difficult to interpret (black box problem)</li>
  </ul>
</Card>

### 3. **Large Language Models (LLMs)**

<Card title="LLM Architecture and Capabilities">
  <p>Large Language Models are neural networks trained on vast amounts of text data to understand and generate human language.</p>
  
  <h4>Key Characteristics:</h4>
  <ul>
    <li><strong>Scale:</strong> Billions of parameters trained on massive datasets</li>
    <li><strong>Context Windows:</strong> Amount of text they can process at once</li>
    <li><strong>Emergent Abilities:</strong> Capabilities that appear at scale</li>
    <li><strong>Few-shot Learning:</strong> Ability to learn from examples</li>
  </ul>
  
  <h4>Claude-Specific Capabilities:</h4>
  <ul>
    <li><strong>200K Token Context:</strong> Can process very long documents</li>
    <li><strong>Tool Use:</strong> Can interact with external systems</li>
    <li><strong>Vision:</strong> Can analyze images and visual content</li>
    <li><strong>Safety:</strong> Built with constitutional AI principles</li>
  </ul>
</Card>

## Claude Implementation Framework

### 1. **Scoping Your Use Case**

<Card title="Use Case Definition">
  <h4>Key Questions:</h4>
  <ul>
    <li>What specific problem are you trying to solve?</li>
    <li>What are the success criteria and performance requirements?</li>
    <li>What are the cost and latency constraints?</li>
    <li>What level of accuracy and reliability is needed?</li>
  </ul>
  
  <h4>Claude-Specific Considerations:</h4>
  <ul>
    <li>Choose appropriate model (Opus, Sonnet, Haiku) based on complexity</li>
    <li>Consider context window requirements for your data</li>
    <li>Evaluate need for tool use or vision capabilities</li>
    <li>Assess security and compliance requirements</li>
  </ul>
</Card>

### 2. **Designing Your Integration**

<Card title="Integration Architecture">
  <h4>Deployment Options:</h4>
  <ul>
    <li><strong>Anthropic API:</strong> Direct integration with Claude models</li>
    <li><strong>AWS Bedrock:</strong> Managed Claude deployment on AWS</li>
    <li><strong>Google Vertex AI:</strong> Claude integration on Google Cloud</li>
    <li><strong>Custom Deployment:</strong> Self-hosted solutions</li>
  </ul>
  
  <h4>Capability Selection:</h4>
  <ul>
    <li><strong>Text Generation:</strong> Content creation, summarization, translation</li>
    <li><strong>Code Generation:</strong> Programming assistance, debugging</li>
    <li><strong>Vision Processing:</strong> Image analysis, document understanding</li>
    <li><strong>Tool Use:</strong> External API integration, function calling</li>
  </ul>
</Card>

### 3. **Preparing Your Data**

<Card title="Data Preparation">
  <h4>Context Management:</h4>
  <ul>
    <li><strong>Token Counting:</strong> Monitor input/output token usage</li>
    <li><strong>Context Optimization:</strong> Efficient use of 200K token window</li>
    <li><strong>Data Cleaning:</strong> Ensure high-quality inputs</li>
    <li><strong>Structured Data:</strong> Format data for optimal processing</li>
  </ul>
  
  <h4>Claude-Specific Data Considerations:</h4>
  <ul>
    <li>Leverage Claude's ability to handle long documents</li>
    <li>Use structured prompts for consistent outputs</li>
    <li>Consider multimodal inputs (text + images)</li>
    <li>Plan for tool integration if needed</li>
  </ul>
</Card>

### 4. **Developing Your Prompts**

<Card title="Prompt Engineering with Claude">
  <h4>Claude Best Practices:</h4>
  <ul>
    <li><strong>Be Clear and Direct:</strong> Claude responds well to explicit instructions</li>
    <li><strong>Use Examples:</strong> Few-shot prompting improves performance</li>
    <li><strong>Let Claude Think:</strong> Encourage step-by-step reasoning</li>
    <li><strong>Use XML Tags:</strong> Structure complex prompts effectively</li>
    <li><strong>Give Claude a Role:</strong> System prompts define behavior</li>
  </ul>
  
  <h4>Advanced Techniques:</h4>
  <ul>
    <li><strong>Chain of Thought:</strong> Encourage step-by-step reasoning</li>
    <li><strong>Few-shot Learning:</strong> Provide examples for complex tasks</li>
    <li><strong>Role Definition:</strong> Give Claude a specific persona</li>
    <li><strong>Iterative Refinement:</strong> Build complex prompts gradually</li>
  </ul>
</Card>

### 5. **Testing and Evaluation**

<Card title="Claude Evaluation Framework">
  <h4>Success Criteria:</h4>
  <ul>
    <li><strong>Accuracy:</strong> Measure correctness of outputs</li>
    <li><strong>Relevance:</strong> Assess alignment with user intent</li>
    <li><strong>Safety:</strong> Evaluate adherence to safety guidelines</li>
    <li><strong>Performance:</strong> Monitor latency and throughput</li>
  </ul>
  
  <h4>Testing Strategies:</h4>
  <ul>
    <li><strong>Red Teaming:</strong> Test for potential misuse</li>
    <li><strong>A/B Testing:</strong> Compare different approaches</li>
    <li><strong>User Testing:</strong> Gather real-world feedback</li>
    <li><strong>Automated Evaluation:</strong> Scale testing processes</li>
  </ul>
</Card>

### 6. **Safety and Guardrails**

<Card title="Claude Safety Features">
  <h4>Built-in Protections:</h4>
  <ul>
    <li><strong>Constitutional AI:</strong> Safety principles built into training</li>
    <li><strong>Jailbreak Resistance:</strong> Resistant to prompt injection attacks</li>
    <li><strong>Hallucination Reduction:</strong> Lower rates of false information</li>
    <li><strong>Content Filtering:</strong> Automatic detection of harmful content</li>
  </ul>
  
  <h4>Additional Guardrails:</h4>
  <ul>
    <li><strong>Output Validation:</strong> Verify Claude's responses</li>
    <li><strong>Input Sanitization:</strong> Clean user inputs</li>
    <li><strong>Rate Limiting:</strong> Prevent abuse</li>
    <li><strong>Monitoring:</strong> Track usage and behavior</li>
  </ul>
</Card>

## Real-World Applications

### 1. **Customer Support Systems**

<Card title="Claude in Customer Support">
  <h4>Use Cases:</h4>
  <ul>
    <li><strong>Ticket Routing:</strong> Automatically categorize and route support tickets</li>
    <li><strong>Response Generation:</strong> Create personalized, accurate responses</li>
    <li><strong>Knowledge Base Search:</strong> Find relevant information quickly</li>
    <li><strong>Escalation Detection:</strong> Identify cases needing human intervention</li>
  </ul>
  
  <h4>Claude Advantages:</h4>
  <ul>
    <li>200K context window handles long conversation histories</li>
    <li>Tool use enables integration with support systems</li>
    <li>Vision capabilities for analyzing screenshots and documents</li>
    <li>High accuracy reduces need for human oversight</li>
  </ul>
</Card>

### 2. **Content Creation and Management**

<Card title="Content Generation with Claude">
  <h4>Applications:</h4>
  <ul>
    <li><strong>Marketing Copy:</strong> Generate brand-consistent content</li>
    <li><strong>Technical Documentation:</strong> Create clear, accurate docs</li>
    <li><strong>Code Documentation:</strong> Explain complex codebases</li>
    <li><strong>Multilingual Content:</strong> Translate and localize materials</li>
  </ul>
  
  <h4>Claude Strengths:</h4>
  <ul>
    <li>Maintains brand voice and style consistency</li>
    <li>Handles technical topics with high accuracy</li>
    <li>Can process and reference source materials</li>
    <li>Supports multiple languages effectively</li>
  </ul>
</Card>

### 3. **Data Analysis and Insights**

<Card title="Analytics with Claude">
  <h4>Capabilities:</h4>
  <ul>
    <li><strong>Data Interpretation:</strong> Analyze complex datasets</li>
    <li><strong>Chart Analysis:</strong> Extract insights from visualizations</li>
    <li><strong>Report Generation:</strong> Create comprehensive summaries</li>
    <li><strong>Trend Identification:</strong> Spot patterns and anomalies</li>
  </ul>
  
  <h4>Claude Features:</h4>
  <ul>
    <li>Vision capabilities for chart and graph analysis</li>
    <li>Long context window for comprehensive reports</li>
    <li>Tool use for database queries and calculations</li>
    <li>Structured output for consistent formatting</li>
  </ul>
</Card>

## Best Practices for Claude Integration

### 1. **Model Selection**

<Card title="Choosing the Right Claude Model">
  <Table>
    <TableHead>
      <TableRow>
        <TableHeader>Use Case</TableHeader>
        <TableHeader>Recommended Model</TableHeader>
        <TableHeader>Reasoning</TableHeader>
      </TableRow>
    </TableHead>
    <TableBody>
      <TableRow>
        <TableCell>Simple Q&A, high volume</TableCell>
        <TableCell>Claude 3 Haiku</TableCell>
        <TableCell>Fast, cost-effective for straightforward tasks</TableCell>
      </TableRow>
      <TableRow>
        <TableCell>General business applications</TableCell>
        <TableCell>Claude 3 Sonnet</TableCell>
        <TableCell>Balanced performance and cost for most use cases</TableCell>
      </TableRow>
      <TableRow>
        <TableCell>Complex reasoning, research</TableCell>
        <TableCell>Claude 3 Opus</TableCell>
        <TableCell>Highest intelligence for challenging problems</TableCell>
      </TableRow>
    </TableBody>
  </Table>
</Card>

### 2. **Prompt Engineering**

<Card title="Effective Claude Prompting">
  <h4>Structure Your Prompts:</h4>
  <ul>
    <li><strong>Clear Instructions:</strong> Be specific about what you want</li>
    <li><strong>Context Provision:</strong> Provide relevant background information</li>
    <li><strong>Example Outputs:</strong> Show Claude the desired format</li>
    <li><strong>Constraints:</strong> Specify limitations and requirements</li>
  </ul>
  
  <h4>Advanced Techniques:</h4>
  <ul>
    <li><strong>Chain of Thought:</strong> Encourage step-by-step reasoning</li>
    <li><strong>Few-shot Learning:</strong> Provide examples for complex tasks</li>
    <li><strong>Role Definition:</strong> Give Claude a specific persona</li>
    <li><strong>Iterative Refinement:</strong> Build complex prompts gradually</li>
  </ul>
</Card>

### 3. **Performance Optimization**

<Card title="Optimizing Claude Performance">
  <h4>Context Management:</h4>
  <ul>
    <li><strong>Token Efficiency:</strong> Minimize unnecessary context</li>
    <li><strong>Structured Inputs:</strong> Organize information clearly</li>
    <li><strong>Caching:</strong> Store and reuse common responses</li>
    <li><strong>Batching:</strong> Process multiple requests together</li>
  </ul>
  
  <h4>Cost Optimization:</h4>
  <ul>
    <li><strong>Model Selection:</strong> Choose appropriate model for task complexity</li>
    <li><strong>Prompt Efficiency:</strong> Minimize input tokens</li>
    <li><strong>Response Limits:</strong> Set appropriate output constraints</li>
    <li><strong>Usage Monitoring:</strong> Track token consumption and costs</li>
  </ul>
</Card>

## Related Concepts

<CardGroup cols={3}>
  <Card title="Prompt Engineering" icon="edit" href="../prompting-techniques/chain-of-thought">
    Master the art of crafting effective prompts
  </Card>
  <Card title="Tool Use" icon="wrench" href="../prompting-techniques/react">
    Integrate external tools and APIs
  </Card>
  <Card title="Evaluation" icon="check-circle" href="../evaluation-observability">
    Measure and improve AI system performance
  </Card>
  <Card title="Safety" icon="shield" href="../safety-security">
    Build safe and trustworthy AI systems
  </Card>
  <Card title="Multimodality" icon="image" href="../multimodality">
    Work with text, images, and other data types
  </Card>
  <Card title="Enterprise AI" icon="building" href="../productization-mlops">
    Deploy AI systems at scale
  </Card>
</CardGroup>

> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [Getting Started with OpenAI Models](https://www.codecademy.com/article/getting-started-with-open-ai-models)  
> for internal educational use only (non-profit).

## Getting Started with OpenAI Models

Learn how to set up, use, and fine-tune OpenAI models for various applications. Get step-by-step guidance and tips on utilizing these powerful AI tools.

### Introduction

Artificial intelligence offers unprecedented opportunities, and OpenAI models are at the forefront of this revolution. This guide will discuss how to set up our environment, use an OpenAI model, and in the end how to finetune it to better fit tasks or topics.

### What are OpenAI Models used for?

OpenAI models lead the way in artificial intelligence development, particularly generative AI. These models, trained on vast amounts of data, can understand, and generate human-like text. OpenAI models like GPT-3.5 turbo are designed to perform a wide range of natural language processing tasks, including translation, question answering, and text generation. OpenAI models are used in various applications such as customer service automation, content creation, data analysis, etc.

### Examples of Popular OpenAI Models

OpenAI has released many generative AI models. One prominent example is GPT-3 (Generative Pre-trained Transformer 3). It can generate text, write code, create stories, and provide educational content. GPT-3.5-turbo, another popular model, excels in conversational AI applications. These models are widely used across industries to automate tasks, enhance customer interactions, and support data analysis, among other applications. Understanding how to utilize and customize these models can significantly enhance your projects. Let us explore the steps to get started with OpenAI models.

### Setting up OpenAI Models

Setting up OpenAI models involves creating an OpenAI account and configuring your development environment.

#### Signing Up with OpenAI

Begin by  registering for an account on the OpenAI website. After signing up, you'll gain access to API keys and comprehensive documentation essential for developing applications with OpenAI models.

#### Setting Up Your Development Environment

The development environment setup varies based on your preferred programming language or platform. Python is commonly used due to its simplicity and extensive library support. Install the necessary packages, such as openai, using pip:

```
pip install openai
```

Now navigate to your OpenAI account settings to obtain your API key. Keep this key secure, as it provides access to OpenAI models and safeguards your usage.

#### Sending API Requests to OpenAI Models

With the API key, you can send requests to OpenAI's API. Below is a simple example using Python:

```python
import openai
# Replace 'your_api_key' with your actual OpenAI API key
openai.api_key = 'your_api_key'
response = openai.completions.create(
    model="gpt-3.5-turbo-0125",
    prompt="Translate the following English text to French: 'Hello, how are you?'",
    max_tokens=60
)
print(response.choices[0].text.strip())
```

The above code sends a prompt to the gpt-3.5-turbo-0125 model for translation.

The parameter response function uses:

- **model**: Specifies the model to be used for generating the completion.
- **prompt**: The text you provide as input for the model to generate a response.
- **max_tokens**: Limits the length of the generated response.

### How to Use OpenAI Models for Various Tasks

OpenAI offers a range of models tailored to different tasks. For instance, gpt-3.5-turbo-0125 excels at text generation and analysis, GPT-3.5-turbo is ideal for conversational tasks, DALL·E is used for creating and editing images based on prompts, and Whisper is designed to convert audio into text. Select a model that fits your specific use case. Refer to OpenAI's model documentation for more details.

#### Using OpenAI Models for Text Generation and More

Once you have selected a model, you can send structured prompts and handle responses. Here's another example using the gpt-3.5-turbo-0125 model for summarization:

```python
import openai
# Replace 'your_api_key' with your actual OpenAI API key
openai.api_key = 'your_api_key'
response = openai.Completion.create(
    engine="gpt-3.5-turbo-0125",
    prompt="Summarize the following article: 'OpenAI has announced ...'",
    max_tokens=150
)
print(response.choices[0].text.strip())
```

By using different prompts, we can perform tasks like text summarization, translation, and question answering.

### How to Fine-Tune an OpenAI Model

Fine-tuning OpenAI models involve training a pre-existing model further using specific data, tailoring it to better fit tasks or topics. This process enhances the model's performance, making it more accurate for your needs.

#### Fine Tuning OpenAI Model

To fine-tune an OpenAI model, follow these steps:

1. **Prepare Your Dataset**: Gather and clean a text dataset relevant to your application.
2. **Upload the Dataset**: Use OpenAI's data management tools to upload your dataset.
3. **Initiate Fine-Tuning**: Start the fine-tuning process using the OpenAI API.

Here's an example:

```python
import openai
# Replace 'your_api_key' with your OpenAI API key
openai.api_key = 'your_api_key'
response = openai.FineTune.create(
    training_file='path/to/your/training_dataset.json',
    model="gpt-3.5-turbo-0125"
)
print("Fine tune job initiated:", response)
```

4. **Monitor and Evaluate**: Track the fine-tuning progress and evaluate the results using validation datasets to ensure effectiveness.

#### Conclusion

We've explored how to get started with OpenAI models, from setting up and making API calls to choosing and fine-tuning models. This knowledge enables the integration of advanced AI into your projects, driving productivity and innovation. By mastering the use and customization of these models, we can tailor them to meet specific needs, ensuring your applications are as effective and efficient as possible.

> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [What is Reinforcement Learning? With Examples](https://www.codecademy.com/article/what-is-reinforcement-learning-with-examples)  
> for internal educational use only (non-profit).

## What is Reinforcement Learning? With Examples

### What is reinforcement learning (RL)?

Reinforcement learning (RL) is a machine learning approach where an AI agent learns to make optimal decisions through trial and error, receiving rewards for good actions and penalties for bad ones. Imagine teaching a dog to sit. You reward it with a treat when it obeys and penalize it when it doesn't. Over time, the dog learns to repeat actions that earn rewards and avoid those that lead to penalties. This same principle guides how RL trains AI agents.

A typical reinforcement learning setup looks as follows:

![Diagram showing reinforcement learning process with agent, environment, actions, states and rewards in a continuous feedback loop](https://static-assets.codecademy.com/reinforcement-learning/reinforcement_learning.jpg)
<sub>Source: *Codecademy*, Codecademy Team (2025).</sub>

Here, we have an agent being trained using reinforcement learning.

- The agent takes an action in the given environment.
- The state of the environment changes due to the action, and the agent receives a reward or penalty.
- Based on the current action, the current state of the environment, the new state of the environment after the action, and the reward received, the agent chooses the next action carefully to maximize its rewards.

The reinforcement learning process happens iteratively in three steps. Before discussing how RL works, let's discuss different components in an RL setup.

### Components of a reinforcement learning system

A reinforcement learning setup includes an agent, environment, states, actions, policy, reward, and value function.

- **Agent**: An agent is the learner or decision maker. It interacts with the environment, learns from its experience, and performs actions based on what it has learned so far during training.
- **Environment**: It is the world the agent lives in. The environment consists of everything the agent interacts with and responds to the agent's actions by giving new states and rewards.
- **States**: States represent all the possible scenarios for a given environment. A state provides a snapshot of the current situation and helps the agent decide what to do next.
- **Actions**: Actions include the number of choices an agent can make. Every action performed by an agent changes the environment in some way.
- **Reward**: Reward is a feedback signal that tells the agent how good or bad its action was.
- **Policy**: Policy is the strategy the agent follows to choose an action in a given state. The policy maps states to actions and helps the agent decide what action to take in a given state. It can be deterministic or non-deterministic.
- **Value function**: The value function measures how good a state or action is regarding future rewards. It measures how good it is to be in a state or how good an action is in a particular state.

Now that we know the different components of a reinforcement learning setup, let's discuss how reinforcement learning works.

### How does reinforcement learning work?

Reinforcement learning involves three steps: exploration, feedback, and adjustment. Let's discuss each process separately.

#### Exploration

When an agent is put into a given state in the environment, it won't know the action that offers the best rewards. Hence, it explores different actions using trial and error to see what happens. Exploration is useful as it helps the agent try different actions in a given state, which results in the agent finding the best action for the current state.

For example, if a robot learning to move from source to destination in a maze takes the right at every cell it reaches, it might never find the fastest path to the goal. However, the robot can explore going left, right, and forward, which can result in finding the shortest path.

#### Feedback

Every action taken by the agent changes the state of the environment. The reward model gives the agent feedback based on how good the action was. The feedback helps the agent decide whether an action was good or bad in the long run.

For example, if a step in the maze gets the robot closer to the goal, it might get +10 points. If the robot rams into a wall, it might get -5 points. Hence, the robot will always try to move in a direction that gets it closer to the goal.

#### Adjustment

The agent updates its policy in response to feedback. This helps the agent make better decisions by choosing actions that led to higher rewards in the past.

- If the agent is rewarded after an action in a given state, it adjusts its policy to repeat the same action in the future if it encounters the same state.
- If the agent is penalized after an action, it adjusts its policy to avoid the action in the future.

By iteratively exploring different actions and making adjustments, the agent updates its policy to achieve the highest rewards in a given state. Eventually, the agent learns to complete the task optimally.

Let's discuss some reinforcement learning examples to understand how the exploration, feedback, and adjustment steps work.

### Reinforcement learning examples

We will discuss three examples to understand how reinforcement learning works.

#### Example 1: Game playing

AI agents are trained to play board games like Chess or Atari. In such games, reinforcement learning works as follows:

- **Exploration**: During exploration, the agent tries random moves to see how they affect the game.
- **Feedback**: The agent receives feedback in terms of points. Losing points or pieces means a negative reward, while getting points means a positive reward. The rewards are also decided based on whether the agent moves closer to a win or a loss after an action.
- **Adjustment**: The agent updates its policy to prefer moves that lead to wins or higher scores.

Over time, the agent learns strategies that win more games.

#### Example 2: Online advertisement placement

An agent trying to learn how to give the best advertisements to the audience is trained using reinforcement learning as follows:

- **Exploration**: The agent shows different types of ads to the audience.
- **Feedback**: If the user clicks on an ad, it means a positive reward. For conversions, the agent receives higher rewards. The agent gets a neutral or negative reward if the user doesn't interact with the ad.
- **Adjustment**: The agent learns to select ad types, content, and platforms that lead to more clicks and conversions.

After being trained for a long time using the feedback data from ads, the agent learns user preferences and targets ads more effectively, resulting in an increase in return on ad spend.

#### Example 3: Algorithmic trading

Algorithmic trading uses AI agents to buy and sell stocks in a fraction of a second. An agent for algorithmic trading is trained as follows:

- **Exploration**: The trading agent experiments with various buying/selling strategies based on a stock's given state.
- **Feedback**: Profits and losses from trades are rewards and penalties. If an action results in a loss, the agent tries to avoid it in the future. If an action leads to profits, the agent is more likely to repeat the action.
- **Adjustment**: The agent refines its policy to maximize the overall profit and avoid risky trades.

Over time, agents become sophisticated and execute automated trades that generate significant profits for companies like Tower Research Capital, Jane Street, and Hudson River Trading.

Having discussed how reinforcement learning works, let's discuss the different types of reinforcement learning, which will help you understand how the feedback and adjustments work in an RL setup.

### Types of reinforcement learning

We can categorize reinforcement learning based on learning approach and policy-optimization methods.

#### Reinforcement learning types based on learning approach

We can categorize reinforcement learning into two types based on the learning approach, i.e., model-free RL and model-based RL.

- **Model-free RL**: In model-free reinforcement learning, the agent learns without building a model of the environment. Instead, it focuses on learning a policy or value function directly through trial and error. Examples of model-free RL include Q-learning and the State-action-reward-state-action (SARSA) algorithm.
- **Model-based RL**: In model-based reinforcement learning, the agent models the environment to predict future states and rewards. This helps the agent plan actions by simulating future steps without interacting with the environment. Examples of model-based RL include the Dyna-Q and Monte Carlo tree search algorithms.

#### Reinforcement learning types based on policy optimization

Model-free reinforcement learning can be categorized into three types, i.e., policy-based RL, value-based RL, and actor-critic methods.

- **Policy-based reinforcement learning**: In policy-based RL, the agent directly estimates the optimal policy. For this, the agent represents the policy as a combination of learnable parameters and converts the training process into an optimization problem. Then, the agent samples the different trajectories and rewards and uses the information to improve the policy by optimizing the average value function across all the states in the environment. Policy-based RL is useful for high-dimensional or continuous spaces. Examples of policy-based RL include Proximal Policy Optimization (PPO), Monte Carlo Policy Gradient (REINFORCE), and Deterministic Policy Gradient (DPG).
- **Value-based reinforcement learning**: In value-based RL, the agent assumes that the optimal policy can be derived by accurately estimating the value function of every state. Using Bellman equation, the agent tries to find all the paths in the environment and the associated rewards. In this process, the agent finds the optimal value function that provides the value of states or state-action pairs. Then, the agent acts greedily at every step to find the optimal path. SARSA and Q-learning are value-based reinforcement learning methods. Note that value-based RL doesn't directly optimize the policy.
- **Actor-critic methods**: Actor-critic methods combine policy-based and value-based RL. In this approach, the agent has two components, i.e., the actor and the critic. The actor updates the policy function, whereas the critic evaluates the action using the value function. In other words, the actor is responsible for what action will lead to long-term rewards, and the critic evaluates how good an action is. Examples of actor-critic methods include Asynchronous Advantage Actor-Critic (A3C), Advantage Actor-Critic (A2C), and Deep Deterministic Policy Gradient (DDPG).

We have discussed the working and types of reinforcement learning. Now, let's discuss its advantages and disadvantages.

### Advantages and disadvantages of reinforcement learning

Due to its training approach, reinforcement learning has many advantages:

- **Solving complex problems**: In many situations, we cannot model every state of the environment. In such cases, reinforcement learning (RL) helps us explore different paths to reach the optimal solution. RL works even if the states and rewards are non-deterministic and may change over time, which makes it useful for many industry applications.
- **Continuous improvement**: The RL agent updates its strategy according to each action and the reward it receives. Hence, the model continuously learns from its past and uses its learnings to take actions that maximize the rewards in the future.
- **RL Works with delayed feedback**: For use cases like online advertisement placement or product recommendation systems, feedback might get delayed for several reasons. For such cases, RL works well by optimizing for long-term rewards. Traditional supervised learning algorithms like linear regression or convolutional neural networks don't work with delayed feedback.
- **Avoids explicit modeling**: Traditional supervised learning methods require labeled data and a defined algorithm for model training. On the contrary, model-free RL methods do not require us to prepare datasets or build complex models of the environment. We can skip modeling the environment if it is infeasible or too complex. Even then, reinforcement learning works well.

Despite all the advantages of the non-deterministic and adaptive learning approach in reinforcement learning, we also encounter some challenges.

- **Sample insufficiency**: Reinforcement learning requires a huge number of interactions with the environment for the agent to learn how to behave. This can be infeasible for real-world applications where collecting feedback is slow. Even simulating the environment in a controlled system can incur huge costs, which makes RL infeasible in many cases.
- **Training instability**: The training process can become unstable if the state transitions in a reinforcement learning system are very complex and there is a high variance in updates. Hence, we need to design reward models and parameter updates to avoid divergence.
- **Reward design**: Designing an appropriate reward function that leads to desired behavior is challenging. Poorly designed reward models can cause unintended or suboptimal agent behavior. Hence, the reward model should be designed to ensure that the agent behaves harmlessly and safely. In real-world applications like robots or self-driving cars, we also need to ensure extensive safeguards to prevent the agent from becoming unsafe.
- **Resource requirement**: Reinforcement learning needs significant computational resources. Hence, training RL models can be expensive and time-consuming.

The advantages of reinforcement learning often outweigh its challenges, and many companies use RL to build AI systems. Let's discuss some applications of RL across various industries.

### Use cases and applications for reinforcement learning

Reinforcement learning is used in various domains, including finance, automobile, robotics, retail, and e-commerce. Let's discuss some use cases of reinforcement learning in each sector.

#### Finance

In finance, reinforcement learning is used in portfolio management, algorithmic trading, and risk management. The RL agents are trained for different use cases, such as dynamic asset allocation based on market conditions, implementing optimal trading strategies by interacting with market data, and adaptive hedging strategies for minimizing risks while maintaining good returns.

#### Automobile

The automobile sector uses reinforcement learning for self-driving cars, fleet management, and traffic control. Self-driving vehicles use reinforcement learning to learn driving policies for navigation, lane changing, and obstacle avoidance. Similarly, fleet management applications use RL for dynamic route planning for ride-sharing or delivery vehicles. In traffic control, RL optimizes traffic light timings to reduce congestion.

#### Robotics

Robots are used across industries to perform repetitive tasks efficiently and accurately. Companies use reinforcement learning to teach robots to walk, balance, and navigate uneven terrain. They are also trained to handle objects and plan paths in dynamic environments.

#### Retail and e-commerce

In retail and e-commerce, reinforcement learning is used to train models for creating dynamic pricing, recommendations, and retargeting applications. Using the customer journey data, RL applications are trained to show dynamic prices to users to maximize revenue. Similarly, product recommendation and retargeting systems use the user's click and conversion data as feedback to derive sales.

Reinforcement learning also plays a huge role in training generative AI applications like ChatGPT and Gemini to produce correct outputs without any discrimination or bias. For this, we use reinforcement learning with human feedback (RLHF). Let's discuss what RLHF is.

### Reinforcement learning with human feedback (RLHF)

Reinforcement learning with human feedback (RLHF) combines traditional reinforcement learning with human guidance to fine-tune LLMs. It allows an agent to learn desirable behavior from human-provided feedback such as preferences, corrections, or demonstrations.

For example, ChatGPT sometimes provides two outputs for a query and asks users to select the better response. Using your preference, it shapes its future output. During training, the LLM generates multiple outputs for a given query. Then, the outputs are ranked by humans and provided as feedback to the LLM. Based on the rankings, the LLM learns to generate answers that fetch better rankings. In this way, RLHF combines RL with human guidance.

RLHF helps us generate polite, accurate, and safe responses from LLMs by aligning AI responses with human values. It also finds its applications in robotics, where we use it to train robot behaviors by observing and ranking their actions.

### Conclusion

Reinforcement learning is an evolving area of machine learning. From game-playing AI agents to robotics and personalized recommendations, RL is already significantly impacting industries. While it offers promising advantages, it also comes with challenges like high computational cost and complexity. With advancements like RLHF, the field continues to grow in capability and accessibility.

This article discussed the basics of reinforcement learning with its components and working. We also discussed the examples, advantages, disadvantages, and applications of RL along with RLHF.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [A High-Level Overview Of Large Language Model Concepts, Use Cases, And Tools](https://www.smashingmagazine.com/2023/10/overview-large-language-model-concepts-use-cases-tools/)  
> for internal educational use only (non-profit).

# A High-Level Overview Of Large Language Model Concepts, Use Cases, And Tools

Even though a simple online search turns up countless tutorials on using Artificial Intelligence (AI) for everything from generative art to making technical documentation easier to use, there's still plenty of mystery around it. What goes inside an AI-powered tool like ChatGPT? How does Notion's AI feature know how to summarize an article for me on the fly? Or how are a bunch of sites suddenly popping up that can aggregate news and auto-publish a slew of "new" articles from it?

It all can seem like a black box of mysterious, arcane technology that requires an advanced computer science degree to understand. What I want to show you, though, is how we can peek inside that box and see how everything is wired up.

Specifically, this article is about large language models (LLMs) and how they "imbue" AI-powered tools with intelligence for answering queries in diverse contexts. I have previously written tutorials on how to use an LLM to transcribe and evaluate the expressed sentiment of audio files. But I want to take a step back and look at another way around it that better demonstrates — and visualizes — how data flows through an AI-powered tool.

We will discuss LLM use cases, look at several new tools that abstract the process of modeling AI with LLM with visual workflows, and get our hands on one of them to see how it all works.

## Large Language Models Overview

Forgoing technical terms, LLMs are vast sets of text data. When we integrate an LLM into an AI system, we enable the system to leverage the language knowledge and capabilities developed by the LLM through its own training. You might think of it as dumping a lifetime of knowledge into an empty brain, assigning that brain to a job, and putting it to work.

"Knowledge" is a convoluted term as it can be subjective and qualitative. We sometimes describe people as "book smart" or "street smart," and they are both types of knowledge that are useful in different contexts. This is what artificial "intelligence" is created upon. AI is fed with data, and that is what it uses to frame its understanding of the world, whether it is text data for "speaking" back to us or visual data for generating "art" on demand.

### Use Cases

As you may imagine (or have already experienced), the use cases of LLMs in AI are many and along a wide spectrum. And we're only in the early days of figuring out what to make with LLMs and how to use them in our work. A few of the most common use cases include the following.

- **Chatbot**: LLMs play a crucial role in building chatbots for customer support, troubleshooting, and interactions, thereby ensuring smooth communications with users and delivering valuable assistance. Salesforce is a good example of a company offering this sort of service.
- **Sentiment Analysis**: LLMs can analyze text for emotions. Organizations use this to collect data, summarize feedback, and quickly identify areas for improvement. Grammarly's "tone detector" is one such example, where AI is used to evaluate sentiment conveyed in content.
- **Content Moderation**: Content moderation is an important aspect of social media platforms, and LLMs come in handy. They can spot and remove offensive content, including hate speech, harassment, or inappropriate photos and videos, which is exactly what Hubspot's AI-powered content moderation feature does.
- **Translation**: Thanks to impressive advancements in language models, translation has become highly accurate. One noteworthy example is Meta AI's latest model, SeamlessM4T, which represents a big step forward in speech-to-speech and speech-to-text technology.
- **Email Filters**: LLMs can be used to automatically detect and block unwanted spam messages, keeping your inbox clean. When trained on large datasets of known spam emails, the models learn to identify suspicious links, phrases, and sender details. This allows them to distinguish legitimate messages from those trying to scam users or market illegal or fraudulent goods and services. Google has offered AI-based spam protection since 2019.
- **Writing Assistance**: Grammarly is the ultimate example of an AI-powered service that uses LLM to "learn" how you write in order to make writing suggestions. But this extends to other services as well, including Gmail's "Smart Reply" feature. The same thing is true of Notion's AI feature, which is capable of summarizing a page of content or meeting notes. Hemmingway's app recently shipped a beta AI integration that corrects writing on the spot.
- **Code and Development**: This is the one that has many developers worried about AI coming after their jobs. It hit the commercial mainstream with GitHub Copilot, a service that performs automatic code completion. Same with Amazon's CodeWhisperer. Then again, AI can be used to help sharpen development skills, which is the case of MDN's AI Help feature.

Again, these are still the early days of LLM. We're already beginning to see language models integrated into our lives, whether it's in our writing, email, or customer service, among many other services that seem to pop up every week. This is an evolving space.

## Types Of Models

There are all kinds of AI models tailored for different applications. You can scroll through Sapling's large list of the most prominent commercial and open-source LLMs to get an idea of all the diverse models that are available and what they are used for. Each model is the context in which AI views the world.

Let's look at some real-world examples of how LLMs are used for different use cases.

**Natural Conversation**: Chatbots need to master the art of conversation. Models like Anthropic's Claude are trained on massive collections of conversational data to chat naturally on any topic. As a developer, you can tap into Claude's conversational skills through an API to create interactive assistants.

**Emotions**: Developers can leverage powerful pre-trained models like Falcon for sentiment analysis. By fine-tuning Falcon on datasets with emotional labels, it can learn to accurately detect the sentiment in any text provided.

**Translation**: Meta AI released SeamlessM4T, an LLM trained on huge translated speech and text datasets. This multilingual model is groundbreaking because it translates speech from one language into another without an intermediary step between input and output. In other words, SeamlessM4T enables real-time voice conversations across languages.

**Content Moderation**: As a developer, you can integrate powerful moderation capabilities using OpenAI's API, which includes a LLM trained thoroughly on flagging toxic content for the purpose of community moderation.

**Spam Filtering**: Some LLMs are used to develop AI programs capable of text classification tasks, such as spotting spam emails. As an email user, the simple act of flagging certain messages as spam further informs AI about what constitutes an unwanted email. After seeing plenty of examples, AI is capable of establishing patterns that allow it to block spam before it hits the inbox.

## Not All Language Models Are Large

While we're on the topic, it's worth mentioning that not all language models are "large." There are plenty of models with smaller sets of data that may not go as deep as ChatGPT 4 or 5 but are well-suited for personal or niche applications.

> **Note:** The following article is reproduced verbatim from  
> Smashing Magazine Team, *Smashing Magazine* (2025):  
> [How To Use Artificial Intelligence And Machine Learning To Summarize Chat Conversations](https://www.smashingmagazine.com/2023/07/artificial-intelligence-machine-learning-summarize-chat-conversations/)  
> for internal educational use only (non-profit).

# How To Use Artificial Intelligence And Machine Learning To Summarize Chat Conversations

As developers, we often deal with large volumes of text, and making sense of it can be a challenge. In many cases, we might only be interested in a summary of the text or a quick overview of its main points. This is where text summarization comes in.

Text summarization is the process of automatically creating a shorter version of a text that preserves its key information. It has many applications in natural language processing (NLP), from summarizing news articles to generating abstracts for scientific papers. Even products, including Notion, are integrating AI features that will summarize a block of text on command.

![A screenshot of the Notion interface showing the option of using AI intelligence to write text, pages, and so on.](https://res.cloudinary.com/indysigner/image/fetch/f_auto,q_80/w_400/https://files.smashing.media/articles/artificial-intelligence-machine-learning-summarize-chat-conversations/notion-ai-technology.png)
<sub>Source: *Smashing Magazine*, Smashing Magazine Team (2025).</sub>

One interesting use case is summarizing chat conversations, where the goal is to distill the main topics and ideas discussed during the conversation. That's what we are going to explore in this article. Whether you're an experienced developer or just getting started with natural language processing, this article will provide a practical guide to building a chat summarizer from scratch. By the end, you'll have a working chat summarizer that you can use to extract the main ideas from your own chat conversations — or any other text data that you might encounter in your projects.

The best part about all of this is that accessing and integrating these sorts of AI and NLP capabilities is easier than ever. Where something like this may have required workarounds and lots of dependencies in the not-so-distant past, there are APIs and existing models readily available that we can leverage. I think you may even be surprised by how few steps there are to pull off this demo of a tool that summarizes chat conversations.

## Cohere: Chat Summarization Made Easy

Cohere is a cloud-based natural language processing platform that enables developers to build sophisticated language models without requiring deep expertise in machine learning. It offers a range of powerful tools for text classification, entity extraction, sentiment analysis, and more. One of its most popular features is chat summarization, which can automatically generate a summary of a conversation.

Using Cohere API for chat summarization is a simple and effective way to summarize chat conversations. It requires only a few lines of code to be implemented and can be used to summarize any chat conversation in real-time.

The chat summarization function of Cohere works by using natural language processing algorithms to analyze the text of the conversation. These algorithms identify important sentences and phrases, along with contextual information like speaker identity, timestamps, and sentiment. The output is a brief summary of the conversation that includes essential information and main points.

## Using The Cohere API For Chat Summarization

Now that we have a basic understanding of Cohere API and its capabilities, let's dive into how we can use it to generate chat summaries. In this section, we will discuss the step-by-step process of generating chat summaries using Cohere API.

To get started with the Cohere API, first, you'll need to sign up for an API key on the Cohere website. Once you have an API key, you can install the Cohere Python package using pip:

```
pip install cohere
```

Next, you'll need to initialize the cohere client by providing the API key:

```
import cohere

# initialize Cohere client
co = cohere.Client("YOUR_API_KEY")
```

Once the client is initialized, we can provide input for the summary. In the case of chat summarization, we need to provide the conversation as input. Here's how you can provide input for the summary:

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [How AI Models Are Trained](https://www.nngroup.com/articles/ai-model-training/)  
> for internal educational use only (non-profit).

# How AI Models Are Trained

By this point, you've undoubtedly heard that the large language model (LLM) behind your favorite AI tool has been "trained on the whole internet." To some extent, that's true, but after training hundreds of UX professionals on how to use AI in their work, it's clear that many don't understand how the AI is trained. This is crucial for forming an accurate mental model of how these LLMs work, their limitations, and their capabilities.

This article discusses four basic types of training, when these are performed within an LLM, and how they impact the role of AI in user experience.

## In This Article:

- 1. The Pretraining Phase: Unsupervised Learning
- 2. The Finetuning Phase: Supervised Learning
- 3. Advanced Finetuning: Reinforcement Learning with Human Feedback (RLHF)
- LLMs Differ from Search Engines
- Environmental and Labor Costs

## 1. The Pretraining Phase: Unsupervised Learning

When you've heard that large language models have been "trained on the whole internet," people are typically talking about the pretraining phase, which involves unsupervised learning.

During this phase, the model is fed enormous amounts of text and data scraped from the internet, digitized books, code repositories, and more. One example of such datasets is Common Crawl, which has terabytes of data from billions of webpages. Cleaning these massive datasets before use requires significant effort.

The sheer volume of data makes it impossible for humans to label or explain it all. Instead, the model learns patterns on its own by trying to predict the next word (or "token") in a sequence.

From exposure to word combinations across billions of examples, the model learns grammar, facts, reasoning abilities (of a sort), and even the biases present in the data.

During the pretraining phase, the AI model is not learning specific tasks or 'meaning' in the human sense. It's pretty much all statistical relationships: which words are most likely to follow other words in different contexts.

### Unsupervised Learning Is Like a Toddler

Think of unsupervised learning like a toddler immersed in language for the first two years of life. They hear countless conversations. They aren't explicitly taught every grammatical rule, but they start absorbing patterns.

Eventually, they begin stringing words together in ways that mimic what they've heard, sometimes surprising you with sentences they weren't directly taught and whose meaning they don't fully grasp.

### Unsupervised Learning for AI-Based Design

While models differ in the types of training data they use, the patterns they learn, and how they rely on these patterns while generating outputs, the principle is the same.

Take Figma AI as an example: for its generative AI features, it needs a large amount of training data as a solid foundation. However, rather than starting from scratch and pretraining a brand new model by feeding it many designs created in Figma (which would have major privacy issues and be extremely costly), they used "third-party, out-of-the-box AI models." These third-party models call APIs from more robust AI companies, like perhaps OpenAI.

So, how can Figma AI generate user interfaces if it relies on models pretrained on text? Because the pretraining datasets they're using are so vast, they've processed billions of lines of code and learned the patterns in how interfaces are put together. However, this doesn't guarantee that asking it to "create a dashboard to display recent sales data" will create something useful, accessible, or reasonable. That requires finetuning.

## 2. The Finetuning Phase: Supervised Learning

If the pretraining phase teaches the model about raw patterns, the finetuning phase (which uses supervised learning) is like giving it specific lessons and examples.

Now, the toddler is older and you're sending them off to school, where a teacher will give them many examples of correct and incorrect sentences. These carefully selected examples are meant to teach the child how to use the vast vocabulary they acquired during unsupervised learning.

To train AI models, researchers create much smaller datasets containing carefully crafted examples of inputs (prompts) and desired outputs (responses).

For instance, a researcher might write a specific prompt and pair it with an ideal response they want the model to emulate. They might even create several variations of the response and rate them on criteria like helpfulness, clarity, or safety. By showing the model thousands of these carefully crafted examples, they teach it to use the patterns identified during pretraining in useful, truthful ways that are aligned with human expectations. Note that the researchers are still providing all their guidance at input. They are not paying much attention yet to the AI's outputs.

![A model response to a prompt about assistive devices, with scores for helpfulness, correctness, coherence, complexity, and verbosity.](https://media.nngroup.com/media/editor/2025/05/01/code2.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

Supervised-learning datasets are much smaller than those used for unsupervised learning but require significant human effort and cost to create. This phase is crucial for specializing the model and improving its ability to follow instructions.

However, bias can creep in here too. The data used for pretraining already contains societal biases (e.g., due to certain demographics or viewpoints being overrepresented online). During finetuning, the specific examples created and rated by humans also introduce the raters' perspectives, values, and potential biases.

Different labeling-data teams might instill slightly different "manners" in the model. This is partly why even LLMs that were trained on vast amounts of the same data can have different tones, personalities, and approaches. They've been finetuned slightly differently.

### Supervised Learning for AI-Based Design

Supervised learning isn't just for chatbots. Even Figma has finetuned some of its AI features to improve the outputs, though it mentions only features like Visual Search, Asset Search, and Add Interactions — not the text-to-UI features First Draft or AI Prototyping.

Figma's finetuning used "data from public, free community files" — that is, it relied on freely available designs rather than on high-quality ones created or vetted by Figma employees — a sensible approach that is cheap, quick, and doesn't violate any privacy restrictions. However, to return to our analogy, the toddler's "teacher" is using free examples from the internet rather than a curriculum designed by experts.

This does not mean that Figma AI will fail to produce anything useful; it simply means that the outputs are strongly biased toward the patterns available in the free, public files. Don't be surprised if the outputs feel generic — Figma itself says, "we need to train models that better understand design concepts and patterns."

Only when AI tools for creating UIs have been carefully finetuned by expert designers will we see as much nuanced power in AI-generated UI design as we currently see in AI-generated language.

## 3. Advanced Finetuning: Reinforcement Learning with Human Feedback (RLHF)

This final type of training is an advanced fine-tuning technique that relies on human judgment of the model's outputs. Now the child is doing writing exercises using the vocabulary they've absorbed (unsupervised learning) according to the rules and examples they've been taught (supervised learning). The teacher provides feedback on the child's work. Then the child adjusts their process based on the teacher's immediate feedback.

Reinforcement learning with human feedback is much the same: humans coach the AI model based on its outputs, often guided by how they feel about the outputs when the success criteria are difficult to define. For tasks with clear, reliable success criteria, such as a robotaxi safely navigating to a predetermined destination without crashing, AI models can iteratively learn on their own based on success or failure (often simply called reinforcement learning).

But what happens when success isn't easily defined by a simple rule, such as the difference between good and bad or useful and not useful?

RLHF brings humans into the loop during the reinforcement-learning process. The model might generate two or more responses to a prompt. A human labeler is then asked to rank these responses — which is better? More helpful? More harmless?

If you've ever been presented with multiple versions of a response while using an LLM, you have been involved in RLHF. Gathering this feedback from real users allows AI companies to gather feedback reflecting real user preferences at scale.

![Two ChatGPT responses with different layouts of information. At the top, it asks users to choose which they like better.](https://media.nngroup.com/media/editor/2025/05/01/screenshot-2025-03-26-at-35053-pm-1.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

> **Note:** The following article is reproduced verbatim from  
> NN/g Team, *Nielsen Norman Group* (2025):  
> [Four AI Superpowers: Where AI Improves Products](https://www.nngroup.com/articles/ai-superpowers/)  
> for internal educational use only (non-profit).

# Four AI Superpowers: Where AI Improves Products

At Pendo's Pendomonium conference recently, I asked my audience who had AI features on their product's roadmap. Almost every hand in the room went up. But, based on my experience, I'm guessing many of those AI-powered features are destined to be useless.

## In This Article:

- Start from the Problem, Not the Technology
- Is GenAI the Right Solution? The Superpowers of AI
- Appropriate use of AI

## Start from the Problem, Not the Technology

Amidst the AI hype, teams fall into a familiar trap of creating solutions in search of a problem. No one needs a half-baked chatbot that clumsily answers questions better suited for an FAQ. Users want solutions that effectively address user needs, regardless of the technology behind them.

Building effective solutions starts with choosing the right tool for the job. Generative AI (genAI) is a powerful tool, but it's not the only one.

I've advised dozens of product teams on their AI strategy. But even thinking about AI, product teams should have:

- A research-backed understanding of the existing customer problem
- A clear definition of desired outcomes and user behaviors

(I've done this so often, I created a framework to map these connections out — the user outcome connection.)

Only when the team is aligned on its goals, is it ready to decide if AI is the right option. The key question for the team to ask is:

> "Will generative AI make the user more likely to engage in the specific behavior that leads to the desired outcome and creates a positive impact for the business?"

Over the last year, I have been in many conversations where asking this question makes teams realize that genAI isn't what they need. For example, one team was in the middle of developing a chatbot to collect insights from users when a simple survey would have been much more effective at accomplishing that goal.

## Is GenAI the Right Solution? The Superpowers of AI

There is no one-size-fits-all answer to this question, especially since this technology is so new and rapidly evolving. These are tasks where genAI technology often excels and delivers value. I call them the superpowers of AI.

Currently available LLMs are particularly good at:

- **Creating content**
- **Summarizing text**
- **Basic data analysis**
- **Perspective taking**

Using genAI for such tasks is beneficial whether AI takes the shape of a chatbot or of a feature embedded in a product.

### Content Creation

Many apps that incorporate AI-enhanced features begin with content creation or manipulation, because it's such a natural fit for a large language model (LLM). If your product involves creating or modifying content, there's a strong chance that genAI can deliver a significant boost in value for your users.

#### Example: Bumble

Dating companies like Bumble want to help their users make meaningful connections with others. But many people struggle to start up new conversations once they've matched with someone. To address this pain point, Bumble created Icebreaker — a feature to autogenerate an introduction message.

![Button that says "Not sure what to say? Let's help you break the ice."](https://media.nngroup.com/media/editor/2025/03/03/bumbleaisuperpowers.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

### Summarization

In addition to producing content, generative AI can ingest vast amounts of content and summarize key points. Tech giants have led the way, launching tools to condense complex information into digestible snippets. Companies of all sizes are following suit, leveraging multimodal AI models to provide summaries of everything from documents to videos. AI can make large amounts of information easily consumable to users.

However, it's important to remember that hallucinations can occur in these summaries, especially when large amounts of text are summarized.

#### Example: Gmail

Clicking on Gmail's Summarize this email button opens a chat panel that instantly starts summarization, while allowing the user to follow up with questions. It takes entire conversations and distills them into the most important points.

![A GIF showing Google Gemini summarizing a long email into a few bullets](https://media.nngroup.com/media/editor/2025/03/03/gmail_email_summary.gif)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

### Basic Data Analysis

Most people find statistical calculations intimidating. Generative AI can guide users to the appropriate statistical method or even perform the calculations for them. While these tools are not yet flawless, they represent a powerful way to foster more confident, data-driven thinking among users.

#### Example: Mixpanel

Some of the first adopters of genAI were product-and-marketing-analytics software. These often contain data-heavy functionalities that their users struggle with. One such software, Mixpanel, has a feature (Spark AI) that allows users to ask questions in natural language. In response, the system produces graphics that users can further interrogate.

![A chart showing conversion rate over time by browser.](https://media.nngroup.com/media/editor/2025/03/03/mixpanelaisuperpowers.png)
<sub>Source: *Nielsen Norman Group*, NN/g Team (2025).</sub>

## Sources

<Card title="Reference Materials">
  <ul>
    <li><strong>Anthropic Claude Documentation:</strong> <a href="https://docs.anthropic.com/en/docs/overview">https://docs.anthropic.com/en/docs/overview</a></li>
    <li><strong>Claude API Reference:</strong> <a href="https://docs.anthropic.com/en/api">https://docs.anthropic.com/en/api</a></li>
    <li><strong>Prompt Engineering Guide:</strong> <a href="https://docs.anthropic.com/en/docs/prompt-engineering">https://docs.anthropic.com/en/docs/prompt-engineering</a></li>
    <li><strong>Tool Use Documentation:</strong> <a href="https://docs.anthropic.com/en/docs/agents-and-tools/tool-use">https://docs.anthropic.com/en/docs/agents-and-tools/tool-use</a></li>
    <li><strong>Safety and Trust:</strong> <a href="https://trust.anthropic.com/">https://trust.anthropic.com/</a></li>
  </ul>
</Card>

## Figures

<Card title="Transformer Architecture">
  <Frame>
    <img src="/assets/concepts/transformer-pipeline.webp" alt="Diagram showing the transformer architecture pipeline" />
  </Frame>
  <figcaption>Credit: synthesized from multiple sources</figcaption>
</Card>

<Card title="AI Model Scaling">
  <Frame>
    <img src="/assets/concepts/transformer-architecture.webp" alt="Visualization of transformer architecture components" />
  </Frame>
</Card>

