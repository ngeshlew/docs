---
title: "python.langchain.com-docs-concepts-evaluation"
slug: "python.langchain.com-docs-concepts-evaluation"
tags: ""
whyForDesigners: "TODO"
botApplication: "TODO"
collaborationPrompts: ""
sources:
- url: "https://python.langchain.com/docs/concepts/evaluation"
author: "''"
license: "internal-copy"
retrieved_at: "'2025-08-18'"
policy: "copy"
figures: ""
- path: "../assets/python.langchain.com/python.langchain.com-docs-concepts-evaluation/1de8e5ab2af8.webp"
caption: "Figure"
credit_name: "python.langchain.com"
credit_url: "https://python.langchain.com/assets/images/langsmith_evaluate-7d48643f3e4c50d77234e13feb95144d.png"
license: "internal-copy"
updatedAt: "'2025-08-18'"
completed: false
---

# python.langchain.com-docs-concepts-evaluation

> Synthesis: TODO

# Evaluation
Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications. It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose. This process is vital for building reliable applications.
LangSmith helps with this process in a few ways:
- It makes it easier to create and curate datasets via its tracing and annotation features
- It provides an evaluation framework that helps you define metrics and run your app against your dataset
- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code
To learn more, check out this LangSmith guide.

![Figure](../assets/python.langchain.com/python.langchain.com-docs-concepts-evaluation/1de8e5ab2af8.webp)
<figcaption>Figure 1. Credit: [python.langchain.com](https://python.langchain.com/assets/images/langsmith_evaluate-7d48643f3e4c50d77234e13feb95144d.png), License: internal-copy</figcaption>
