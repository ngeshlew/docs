> **Note:** The following article is reproduced verbatim from  
> Codecademy Team, *Codecademy* (2025):  
> [Detecting Hallucinations in Generative AI](https://www.codecademy.com/article/detecting-hallucinations-in-generative-ai)  
> for internal educational use only (non-profit).

# Detecting Hallucinations in Generative AI

## What are AI hallucinations?

Generative AI is a helpful tool that can be used to assist us in a variety of tasks ranging from creating cooking recipes to debugging code and even creating an amazing Dungeons and Dragons scenario! But we need to be cognizant that Generative AI can provide incorrect data.

ChatGPT can generate a hallucination: an occurrence when a generative AI chatbot produces an output that looks real but is a falsification, incorrect, or deviates from the user's instructions. A hallucination can be incorrect data, such as ChatGPT stating a history fact that did not occur, or it could be that we told ChatGPT to do/not do something, and it does the opposite. For example, in our pair programming tutorial, we can see that it provided actual code as the navigator even when instructed otherwise. Another example is when ChatGPT provided an example of someone walking over the English Channel which is not possible.

The reality is that Generative AI is not perfect. Remember, Generative AI is not able to critically think. It is a simple input/output system. Identifying and detecting hallucinations in Generative AI is up to the user.

## How to detect hallucinations in generative AI

Hallucinations occur often enough that we'll probably spot them in most interactions we have with Generative AI. Some are going to be noticeable, such as the pair programming example in the Introduction. Other hallucinations are going to be a little more controversial, such as when ChatGPT generated a negative poem about President Trump and a positive poem about President Biden. ChatGPT was unable to provide an unbiased response per the user's instructions, favoring President Biden over President Trump.

Using only Generative AI as the only source of information can have devastating consequences. In a court case in 2023, a lawyer solely depended on ChatGPT to find court cases relating to theirs. ChatGPT responded with fake court cases that never occurred.

We can do the following to help mitigate hallucinations:

### 1. Careful Prompting

Careful and direct prompting will ensure that ChatGPT is clear on our intent. Even though we think we are being clear, we want to make sure that ChatGPT understands our prompts.

### 2. Refining Prompts

Even using our finest prompts, we must remember that ChatGPT is a text-based AI and may not understand every prompt as we think it and say it. Through careful, guided prompting and providing more accurate descriptions, ChatGPT may provide different responses, better aimed toward our intent.

### 3. Being Skeptical of AI Responses

As we use ChatGPT, we need to be careful, especially when discussing topics we may not fully understand. It is best to verify all responses of ChatGPT and not solely depend on Generative AI as a sole source of information.

### 4. Different AI Model

Using another AI model, such as Google Bard, to see if it generates a similar response. These generative AI don't all use the same models and may result in different responses. This could help detect or remove hallucinations.

### 5. Questioning Generative AI Responses

We can and should question ChatGPT responses. We can ask ChatGPT to provide sources, expound upon details, and be assertive in collecting information. You can even ask "How confident are you with this answer?".

### 6. Manual Searching

Lastly, we can cross-check information by looking it up ourselves via Google or some other sources. Doing this well help us confirm fact-based hallucinations.

Even with these corrections in our prompting, it is guaranteed that ChatGPT will continue to respond with hallucinations. It is up to us to be on the watch to discern between valid and invalid responses.

## Conclusion

Unfortunately, generative AI does not always produce valid output, i.e. hallucinations. Remember, hallucinations are when generative AI produces an output that seems real but are fake. Although sometimes difficult, we can do our best to detect them through a variety of techniques, such as careful prompting, being skeptical of generated responses, and even using multiple AI models to determine if a hallucination occurred.

Generative AI is full of wonders and can be incredibly helpful in our day-to-day. It is capable of not only providing information but taking it a step further and solving some of our problems. But we need to be sure to make sure that information is accurate. Let's make sure we are validating all information provided by Generative AI.
